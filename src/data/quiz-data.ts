// Auto-generated from OTCA quiz markdown files. Do not edit manually.
// Run: bun run local/scripts/parse-quiz.ts

export type Domain = "fundamentals" | "api-sdk" | "collector" | "debugging";
export type Difficulty = "recall" | "scenario" | "edge-case";
export type AnswerKey = "A" | "B" | "C" | "D";

export interface QuizQuestion {
  id: string;
  domain: Domain;
  domainLabel: string;
  question: string;
  options: { key: AnswerKey; text: string }[];
  correctAnswer: AnswerKey;
  explanation: string;
  difficulty: Difficulty;
}

export const DOMAIN_LABELS: Record<Domain, string> = {
  fundamentals: "Fundamentals of Observability",
  "api-sdk": "OpenTelemetry API and SDK",
  collector: "OpenTelemetry Collector",
  debugging: "Maintaining and Debugging",
};

export const EXAM_CONFIG = {
  totalQuestions: 60,
  passingScore: 0.75,
  timeLimitMinutes: 90,
  domainDistribution: {
    fundamentals: 11,
    "api-sdk": 27,
    collector: 16,
    debugging: 6,
  } as Record<Domain, number>,
} as const;

export const quizQuestions: QuizQuestion[] = [
  {
    id: "01-Q1",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `What is the primary purpose of telemetry in software systems?`,
    options: [
      { key: "A", text: `To replace manual testing with automated monitoring` },
      { key: "B", text: `To provide real-time visualizations of application code` },
      { key: "C", text: `To emit data about system behavior for later analysis` },
      { key: "D", text: `To enforce security policies across distributed services` },
    ],
    correctAnswer: "C",
    explanation: `Telemetry is data emitted by software systems about their behavior. It serves as the raw material for both monitoring (answering known questions) and observability (investigating unknown problems). The data is generated during execution and analyzed afterward to understand system health and diagnose issues.`,
    difficulty: "recall",
  },
  {
    id: "01-Q2",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `Which three telemetry signals does OpenTelemetry define as stable?`,
    options: [
      { key: "A", text: `Metrics, Events, Logs` },
      { key: "B", text: `Traces, Metrics, Logs` },
      { key: "C", text: `Traces, Metrics, Events` },
      { key: "D", text: `Logs, Profiles, Traces` },
    ],
    correctAnswer: "B",
    explanation: `OpenTelemetry defines three stable signals: Traces, Metrics, and Logs. Profiles exist as a fourth signal but remain in Development status. Events are not a standalone signal in OpenTelemetry; they exist as span events within traces or as event-style log records.`,
    difficulty: "recall",
  },
  {
    id: "01-Q3",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team operating a legacy system relies entirely on log aggregation to understand request patterns across services. They have built custom correlation IDs to track requests. Which OpenTelemetry signal would most directly replace and improve this hand-rolled approach?`,
    options: [
      { key: "A", text: `Traces, because they provide standardized context propagation and request path visualization` },
      { key: "B", text: `Metrics, because they aggregate request data more efficiently than logs` },
      { key: "C", text: `Logs with trace_id attributes, because the existing log infrastructure stays the same` },
      { key: "D", text: `Profiles, because they reveal performance bottlenecks across services` },
    ],
    correctAnswer: "A",
    explanation: `Custom correlation IDs in logs are a primitive form of distributed tracing. Traces provide this as a first-class capability with standardized context propagation (W3C TraceContext), parent-child relationships, timing data, and automatic propagation across service boundaries. While logs can carry trace_id for correlation, the core improvement comes from adopting traces as the primary signal for transaction tracking.`,
    difficulty: "scenario",
  },
  {
    id: "01-Q4",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `In the OTLP data model, what is the correct nesting order of the hierarchy from outermost to innermost?`,
    options: [
      { key: "A", text: `InstrumentationScope, Resource, Signal Data, Data Units` },
      { key: "B", text: `Data Units, Resource, InstrumentationScope, Signal Data` },
      { key: "C", text: `Signal Data, InstrumentationScope, Resource, Data Units` },
      { key: "D", text: `Signal Data, Resource, InstrumentationScope, Data Units` },
    ],
    correctAnswer: "D",
    explanation: `All OTLP signals follow the same batching-friendly hierarchy: SignalData (e.g., TracesData) contains ResourceSignal[] (e.g., ResourceSpans), which contains ScopeSignal[] (e.g., ScopeSpans), which contains the individual Data Units (e.g., Span[]). This structure enables efficient batching of data from multiple resources and scopes in a single request.`,
    difficulty: "recall",
  },
  {
    id: "01-Q5",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An automotive engineer instruments a fleet of vehicles with sensors that record speed, fuel pressure, engine RPM, and intake air temperature at regular intervals. The data is plotted on graphs to detect trends and compare current readings against historical baselines. In OpenTelemetry terms, which signal type is most analogous to these vehicle sensor readings?`,
    options: [
      { key: "A", text: `Traces, because each trip is a distributed transaction` },
      { key: "B", text: `Metrics, because they represent numeric measurements sampled at intervals` },
      { key: "C", text: `Logs, because each sensor reading is a discrete event` },
      { key: "D", text: `Profiles, because they capture system resource consumption over time` },
    ],
    correctAnswer: "B",
    explanation: `Vehicle sensors like speedometers and fuel gauges produce gauge-style measurements: current values recorded at regular intervals that can be plotted over time and compared using statistical models. This maps directly to OpenTelemetry metrics, which represent measurements as typed data points (Gauge, Sum, Histogram) with aggregation semantics. Each line of instrumentation in source code is equivalent to a sensor exporting data about the application's state.`,
    difficulty: "scenario",
  },
  {
    id: "01-Q6",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `The MELT acronym (Metrics, Events, Logs, Traces) is widely used in the observability industry. How does OpenTelemetry handle the "Events" component compared to the MELT model?`,
    options: [
      { key: "A", text: `OpenTelemetry defines Events as a fourth stable signal type alongside Traces, Metrics, and Logs` },
      { key: "B", text: `OpenTelemetry does not support events in any form` },
      { key: "C", text: `Events exist as span events within traces and as event-style log records using the event_name field` },
      { key: "D", text: `Events are transmitted through a dedicated OTLP Events service endpoint` },
    ],
    correctAnswer: "C",
    explanation: `OpenTelemetry does not define Events as a standalone signal. Instead, events appear in two forms: span events are timestamped annotations within a span (e.g., exception events), and event-style logs use the event_name field on LogRecord (stable since OTLP v1.6.0) for structured events like "user.login.failed". This design decision avoids adding a fourth signal while still supporting event-driven use cases.`,
    difficulty: "edge-case",
  },
  {
    id: "01-Q7",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An SRE team investigating a production incident sees a latency spike on a metric dashboard and wants to navigate directly to the specific request that caused it. Which OpenTelemetry mechanism enables jumping from a metric data point to the corresponding trace?`,
    options: [
      { key: "A", text: `Schema URLs that version the metric and trace data together` },
      { key: "B", text: `InstrumentationScope matching between the metric and trace pipelines` },
      { key: "C", text: `Resource attributes shared between the metric and trace data` },
      { key: "D", text: `Exemplars attached to metric data points that contain trace_id and span_id` },
    ],
    correctAnswer: "D",
    explanation: `Exemplars are sample measurements within metric data points that carry trace_id and span_id fields. They enable direct navigation from an aggregated metric (like a latency histogram bucket) to the specific trace that produced that measurement. This is how metrics and traces correlate in OpenTelemetry: metrics provide the aggregated view, and exemplars link to representative individual traces for deeper investigation.`,
    difficulty: "scenario",
  },
  {
    id: "01-Q8",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `What is the key difference between structured and unstructured log bodies in the OTLP log data model?`,
    options: [
      { key: "A", text: `Structured bodies use typed key-value pairs (kvlist), enabling searchable fields, while unstructured bodies are plain strings` },
      { key: "B", text: `Structured bodies require protobuf encoding, while unstructured bodies use JSON` },
      { key: "C", text: `Structured bodies must include trace correlation fields, while unstructured bodies do not` },
      { key: "D", text: `Structured bodies are limited to predefined semantic convention keys, while unstructured bodies accept any content` },
    ],
    correctAnswer: "A",
    explanation: `In OTLP, the log record body field uses AnyValue, which supports multiple types. A structured body uses kvlist_value (key-value list) with typed values, making fields individually searchable and filterable. An unstructured body uses string_value containing a plain text message. The OTLP best practice recommends using structured attributes for searchable/filterable fields and reserving the body for human-readable messages.`,
    difficulty: "recall",
  },
  {
    id: "01-Q9",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An e-commerce platform generates terabytes of trace data daily. The team needs to track request rates, error percentages, and latency distributions per endpoint over the past year for capacity planning. Which approach is most storage-efficient for answering these known questions?`,
    options: [
      { key: "A", text: `Store all trace data at full fidelity and query spans on demand` },
      { key: "B", text: `Increase the trace sampling rate and store spans in a columnar database` },
      { key: "C", text: `Pre-calculate the answers as metrics with long-term retention` },
      { key: "D", text: `Export logs with embedded latency attributes and aggregate them at query time` },
    ],
    correctAnswer: "C",
    explanation: `Metrics are pre-calculated aggregations designed for exactly this use case: answering known questions efficiently. Storing RED metrics (requests, errors, duration) requires far less storage than retaining complete trace data, and queries are fast because the answers are already computed. Traces can be retained with shorter retention for ad-hoc investigation, while metrics cover long-term trend analysis at a fraction of the cost.`,
    difficulty: "scenario",
  },
  {
    id: "01-Q10",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `In the OTLP data model, what does the Resource describe?`,
    options: [
      { key: "A", text: `The specific operation or request being processed` },
      { key: "B", text: `The entity producing telemetry, such as a service, host, or container` },
      { key: "C", text: `The backend system that stores and analyzes the telemetry` },
      { key: "D", text: `The network transport protocol used to transmit the data` },
    ],
    correctAnswer: "B",
    explanation: `A Resource describes the entity that produces telemetry. Its attributes identify characteristics like service.name, host.name, and container runtime. Resources are shared across all telemetry from the same producer, appearing at the top of the OTLP hierarchy. This is distinct from InstrumentationScope (which identifies the library generating telemetry) and from individual data units like spans or log records (which describe specific operations).`,
    difficulty: "recall",
  },
  {
    id: "01-Q11",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An operations team needs to know when their application's database connection pool transitions between healthy, degraded, and unavailable states. Which telemetry signal is best suited for recording these lifecycle transitions?`,
    options: [
      { key: "A", text: `Logs, because they narrate application lifecycle events like dependency state changes` },
      { key: "B", text: `Traces, because each state transition is a distributed operation spanning multiple services` },
      { key: "C", text: `Metrics, because state transitions should be counted as numeric values` },
      { key: "D", text: `Any signal works equally well; the choice has no practical impact` },
    ],
    correctAnswer: "A",
    explanation: `Logs excel at recording application lifecycle events: when a connection pool was established, when a circuit breaker opened or closed, when a critical dependency became unavailable, or when the application recovered. These are events about the application itself, not about individual business transactions (traces) or pre-calculated aggregate indicators (metrics).`,
    difficulty: "scenario",
  },
  {
    id: "01-Q12",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An OTLP log record has two timestamp fields: time_unix_nano and observed_time_unix_nano. When would these two values differ significantly?`,
    options: [
      { key: "A", text: `They always contain the same value because both represent when the log was created` },
      { key: "B", text: `time_unix_nano uses millisecond precision while observed_time_unix_nano uses nanosecond precision` },
      { key: "C", text: `observed_time_unix_nano is only populated by exporters, not by the SDK or collector` },
      { key: "D", text: `When a collector ingests logs from a file or external source, the event may have occurred well before the collector observed it` },
    ],
    correctAnswer: "D",
    explanation: `time_unix_nano records when the event actually occurred, while observed_time_unix_nano records when the log was collected or observed. These differ significantly when logs are ingested from files, external sources, or buffered systems where collection happens later than the original event. The spec requires observed_time_unix_nano to always be set, while time_unix_nano is optional and used only when the actual event time is known.`,
    difficulty: "edge-case",
  },
  {
    id: "01-Q13",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `In OpenTelemetry, log records can include trace_id and span_id fields for trace correlation. If a log is emitted outside of any active span context, what should these fields contain?`,
    options: [
      { key: "A", text: `A newly generated trace_id and span_id to ensure every log is traceable` },
      { key: "B", text: `The trace_id and span_id of the most recently completed span` },
      { key: "C", text: `Placeholder values of all zeros to indicate the absence of context` },
      { key: "D", text: `The fields should be left empty, indicating no trace correlation exists` },
    ],
    correctAnswer: "D",
    explanation: `The OTLP logs data model specifies that trace_id and span_id are optional fields. When no span context is active, these fields should not be set. Fabricating trace or span IDs breaks trace integrity and creates false correlations. An empty trace_id simply means the log was not emitted within a traced operation, which is valid for many application lifecycle events.`,
    difficulty: "edge-case",
  },
  {
    id: "01-Q14",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `Which statement correctly describes how all three stable OTLP signals (traces, metrics, logs) share a common structure?`,
    options: [
      { key: "A", text: `They all use the same data point format with identical fields for every signal` },
      { key: "B", text: `They all follow a Resource, InstrumentationScope, Data Units hierarchy and use the same KeyValue attribute structure` },
      { key: "C", text: `They all require trace_id and span_id fields for identification` },
      { key: "D", text: `They all use a single shared protobuf service endpoint for export` },
    ],
    correctAnswer: "B",
    explanation: `All OTLP signals share two structural foundations: the Resource → InstrumentationScope → Data Units hierarchy (enabling efficient batching), and the KeyValue attribute system (supporting string, bool, int, double, bytes, array, and kvlist value types). Each signal has its own data unit type (Span, Metric, LogRecord), its own export endpoint (TraceService, MetricsService, LogsService), and signal-specific fields. Trace correlation fields (trace_id, span_id) are native to spans, optional on logs, and available on metrics only through exemplars.`,
    difficulty: "recall",
  },
  {
    id: "02-Q1",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `What is the primary purpose of OpenTelemetry semantic conventions?`,
    options: [
      { key: "A", text: `To provide standardized attribute names and meanings so telemetry data is consistent across services and languages` },
      { key: "B", text: `To define the wire protocol used to transmit telemetry between the SDK and the Collector` },
      { key: "C", text: `To specify programming language-specific APIs for each OpenTelemetry SDK` },
      { key: "D", text: `To establish security and access control requirements for telemetry data` },
    ],
    correctAnswer: "A",
    explanation: `Semantic conventions establish a common vocabulary of attribute names, metric names, and span names so that telemetry from different services, languages, and frameworks can be correlated and analyzed consistently. Without them, each team would invent its own naming, breaking cross-service dashboards and alerts.`,
    difficulty: "recall",
  },
  {
    id: "02-Q2",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An operations team discovers that staging and production telemetry are mixed together in their observability backend, making it impossible to filter by environment. Which resource attribute should they configure in their SDK to separate environments?`,
    options: [
      { key: "A", text: `\`service.environment\`` },
      { key: "B", text: `\`cloud.platform\`` },
      { key: "C", text: `\`deployment.environment.name\`` },
      { key: "D", text: `\`host.type\`` },
    ],
    correctAnswer: "C",
    explanation: `The semantic convention \`deployment.environment.name\` is the standard resource attribute for indicating the deployment environment (e.g., "staging", "production", "development"). The other options either do not exist in semantic conventions (\`service.environment\`) or describe unrelated infrastructure metadata.`,
    difficulty: "scenario",
  },
  {
    id: "02-Q3",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `Which rules apply to attribute names in OpenTelemetry semantic conventions?`,
    options: [
      { key: "A", text: `Use camelCase for multi-word names and slashes to delimit namespaces` },
      { key: "B", text: `Use PascalCase for multi-word names and colons to delimit namespaces` },
      { key: "C", text: `Use kebab-case for multi-word names and underscores to delimit namespaces` },
      { key: "D", text: `Use snake_case for multi-word names and dots to delimit namespaces` },
    ],
    correctAnswer: "D",
    explanation: `Semantic convention attribute names use dots to separate namespace levels (e.g., \`http.response.status_code\`) and snake_case for multi-word components within a level. Names must be valid Unicode sequences and should be lowercase.`,
    difficulty: "recall",
  },
  {
    id: "02-Q4",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A developer is instrumenting an HTTP server. Incoming requests arrive at the route \`/api/orders/{orderId}\` using GET. What should the span name be?`,
    options: [
      { key: "A", text: `\`HTTP GET\`` },
      { key: "B", text: `\`GET /api/orders/{orderId}\`` },
      { key: "C", text: `\`api.orders.get\`` },
      { key: "D", text: `\`/api/orders/12345\`` },
    ],
    correctAnswer: "B",
    explanation: `HTTP server span names follow the pattern \`{method} {route}\`. The route should use the parameterized template (with \`{orderId}\`), not a concrete value like \`12345\`, to keep cardinality low. Including just \`HTTP GET\` loses the route information, and dot-separated naming is for attributes, not span names.`,
    difficulty: "scenario",
  },
  {
    id: "02-Q5",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team names their custom metric \`acmecorp.http.server.request.duration\`. A code review flags a naming violation. What is the problem?`,
    options: [
      { key: "A", text: `Metric names must not contain the word "duration"` },
      { key: "B", text: `The metric name is too long; metrics should have at most three dot-separated components` },
      { key: "C", text: `Metric names should never include a company or service name as a prefix` },
      { key: "D", text: `Periods are not allowed in metric names; underscores should be used instead` },
    ],
    correctAnswer: "C",
    explanation: `OpenTelemetry naming guidelines explicitly state that metric names should never include a service name or company name. The correct name would be \`http.server.request.duration\`. Context about which service produced the metric is already provided by resource attributes like \`service.name\`.`,
    difficulty: "scenario",
  },
  {
    id: "02-Q6",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An engineer defines a metric called \`system.processes\` using an UpDownCounter instrument to track the number of active processes. What naming issue does this have according to semantic conventions?`,
    options: [
      { key: "A", text: `The name is missing a descriptive suffix such as \`.count\`` },
      { key: "B", text: `UpDownCounter metric names must always end with \`_total\`` },
      { key: "C", text: `The \`system\` namespace can only be used for Gauge instruments` },
      { key: "D", text: `UpDownCounter metrics require a unit suffix in the name` },
    ],
    correctAnswer: "A",
    explanation: `Semantic conventions follow a \`<namespace>.<noun>.<descriptor>\` structure for metrics. When an UpDownCounter tracks a count of things, the name should include a descriptor such as \`.count\`, giving \`system.process.count\` rather than the bare \`system.processes\`. The \`_total\` suffix should be avoided for UpDownCounters because it implies a monotonically increasing sum, which contradicts the UpDownCounter behavior.`,
    difficulty: "edge-case",
  },
  {
    id: "02-Q7",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `OpenTelemetry semantic conventions define a priority order for database span names. Which format has the highest priority?`,
    options: [
      { key: "A", text: `The database system name (e.g., \`postgresql\`)` },
      { key: "B", text: `The target table or collection name (e.g., \`users\`)` },
      { key: "C", text: `The operation combined with the target (e.g., \`SELECT users\`)` },
      { key: "D", text: `The query summary from \`db.query.summary\`` },
    ],
    correctAnswer: "D",
    explanation: `The priority order for database span names is: \`db.query.summary\` (highest) > \`{operation} {target}\` > \`{target}\` > \`{system}\` (lowest). This means a query summary like \`SELECT users WHERE active = true\` takes precedence, followed by \`SELECT users\`, then \`users\` alone, and finally just the system name as a last resort.`,
    difficulty: "recall",
  },
  {
    id: "02-Q8",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A developer instruments a message consumer that reads orders from a topic called \`topic.orders\`. What should the span name be according to messaging semantic conventions?`,
    options: [
      { key: "A", text: `\`receive topic.orders\`` },
      { key: "B", text: `\`CONSUME topic.orders\`` },
      { key: "C", text: `\`messaging.receive\`` },
      { key: "D", text: `\`topic.orders.consumer\`` },
    ],
    correctAnswer: "A",
    explanation: `Messaging span names follow the pattern \`{operation} {destination}\`. For a consumer reading messages, the operation is \`receive\` (not \`CONSUME\`, which is not a standard operation type in semantic conventions). The destination is the full topic name. The name should not use a reversed order or omit the destination.`,
    difficulty: "scenario",
  },
  {
    id: "02-Q9",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `Which of the following is a valid attribute value type in OpenTelemetry semantic conventions?`,
    options: [
      { key: "A", text: `\`map[string]string\`` },
      { key: "B", text: `\`string[]\`` },
      { key: "C", text: `\`object\`` },
      { key: "D", text: `\`float\`` },
    ],
    correctAnswer: "B",
    explanation: `OpenTelemetry supports these attribute types: \`string\`, \`int\`, \`double\`, \`boolean\`, and array variants \`string[]\` and \`int[]\`. It also supports \`template[string]\` for parameterized keys and enums for predefined values. Maps, arbitrary objects, and \`float\` (the correct name is \`double\`) are not supported.`,
    difficulty: "recall",
  },
  {
    id: "02-Q10",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team needs to capture custom HTTP request headers as span attributes. Which semantic convention pattern should they use?`,
    options: [
      { key: "A", text: `\`http.request.custom_header\`` },
      { key: "B", text: `\`http.headers.request\`` },
      { key: "C", text: `\`http.request.header.{key}\` where \`{key}\` is replaced by the header name` },
      { key: "D", text: `\`http.request.metadata.{header_name}\`` },
    ],
    correctAnswer: "C",
    explanation: `Template attributes use a parameterized key pattern where part of the attribute name is dynamic. For HTTP headers, the pattern is \`http.request.header.{key}\`, where \`{key}\` is the lowercase header name. Similar patterns exist for Kubernetes labels (\`k8s.pod.label.{key}\`) and container labels (\`container.label.{key}\`).`,
    difficulty: "scenario",
  },
  {
    id: "02-Q11",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `After upgrading their semantic conventions library, a team finds that their \`system.network.dropped\` metric is no longer being populated. What is the correct replacement name introduced in recent versions?`,
    options: [
      { key: "A", text: `\`system.network.drop.count\`` },
      { key: "B", text: `\`system.network.packet.dropped\`` },
      { key: "C", text: `\`system.net.packet.dropped\`` },
      { key: "D", text: `\`network.system.dropped\`` },
    ],
    correctAnswer: "B",
    explanation: `In semantic conventions v1.37, \`system.network.dropped\` was renamed to \`system.network.packet.dropped\` as part of a restructuring that made network metric names more explicit. Similarly, \`system.network.packets\` became \`system.network.packet.count\`. These breaking changes are why teams should pin their semantic convention versions and test upgrades carefully.`,
    difficulty: "scenario",
  },
  {
    id: "02-Q12",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `When naming attributes in OpenTelemetry semantic conventions, which guideline applies to abbreviations?`,
    options: [
      { key: "A", text: `All abbreviations must be spelled out in full` },
      { key: "B", text: `Only single-letter abbreviations are permitted` },
      { key: "C", text: `Abbreviations are prohibited in attribute names` },
      { key: "D", text: `Common, well-known abbreviations like HTTP, DB, CPU, and K8s are allowed` },
    ],
    correctAnswer: "D",
    explanation: `The naming guidelines allow commonly understood abbreviations such as IP, DB, CPU, HTTP, URL, AWS, GCP, and K8s. Domain-specific abbreviations are also acceptable when qualified with a namespace (e.g., \`container.csi.*\`, \`container.oci.*\`). However, ambiguous abbreviations that could apply to multiple concepts should be avoided.`,
    difficulty: "edge-case",
  },
  {
    id: "02-Q13",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `What does the \`.utilization\` suffix indicate in an OpenTelemetry metric name like \`system.memory.utilization\`?`,
    options: [
      { key: "A", text: `The total amount of the resource currently in use` },
      { key: "B", text: `The maximum capacity of the resource` },
      { key: "C", text: `A fraction representing usage divided by limit, in the range 0 to 1` },
      { key: "D", text: `The rate of change in resource usage per second` },
    ],
    correctAnswer: "C",
    explanation: `Semantic conventions define specific suffixes with precise meanings. \`.utilization\` represents the ratio of \`.usage\` to \`.limit\`, yielding a dimensionless value between 0 and 1. In contrast, \`.usage\` represents the absolute amount used, \`.limit\` represents the total available, and \`.time\` represents passage of time. This distinction prevents ambiguity when teams interpret metric values.`,
    difficulty: "edge-case",
  },
  {
    id: "02-Q14",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An SRE team needs to determine which version of their checkout service is currently deployed by examining telemetry. Which resource attribute provides this information?`,
    options: [
      { key: "A", text: `\`deployment.version\`` },
      { key: "B", text: `\`service.version\`` },
      { key: "C", text: `\`process.runtime.version\`` },
      { key: "D", text: `\`code.version\`` },
    ],
    correctAnswer: "B",
    explanation: `The \`service.version\` resource attribute records the version of the service itself (e.g., "2.3.1"), as defined in the \`service.*\` namespace of semantic conventions. \`process.runtime.version\` describes the language runtime version (e.g., Go 1.22), not the application version. \`deployment.version\` and \`code.version\` are not standard semantic convention attributes.`,
    difficulty: "recall",
  },
  {
    id: "03-Q1",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A vendor claims their eBPF-based instrumentation tool adds "zero overhead" to applications because it operates at the kernel level. Why is this claim misleading?`,
    options: [
      { key: "A", text: `eBPF instrumentation does not support all programming languages` },
      { key: "B", text: `Even when the application process is not directly burdened, the operating system still consumes resources to collect the data` },
      { key: "C", text: `eBPF tools cannot produce traces, only metrics` },
      { key: "D", text: `Kernel-level instrumentation requires a specific Linux distribution` },
    ],
    correctAnswer: "B",
    explanation: `Claims of zero overhead instrumentation are misleading regardless of the approach. Even when instrumentation operates outside the application process (such as eBPF at the kernel level), the underlying operating system or kernel still consumes CPU and memory resources to collect telemetry data. There is no way to observe a system without some cost.`,
    difficulty: "edge-case",
  },
  {
    id: "03-Q2",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team deploys a third-party identity management system on Kubernetes and needs observability without modifying its source code. Which instrumentation approach is most appropriate?`,
    options: [
      { key: "A", text: `Manual instrumentation using the OpenTelemetry API` },
      { key: "B", text: `Instrumentation libraries wrapping the application's internal frameworks` },
      { key: "C", text: `Native instrumentation built into the application by its maintainers` },
      { key: "D", text: `Automatic instrumentation using a language agent` },
    ],
    correctAnswer: "D",
    explanation: `Automatic instrumentation is most useful when you do not own the application's source code, cannot or do not want to redeploy with code changes, or need quick visibility without significant engineering effort. Language agents (such as the Java agent) hook into runtime mechanisms to produce telemetry without any source code modification.`,
    difficulty: "scenario",
  },
  {
    id: "03-Q3",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `What distinguishes instrumentation libraries from manual instrumentation?`,
    options: [
      { key: "A", text: `Instrumentation libraries wrap specific frameworks to create spans automatically, while manual instrumentation uses the OpenTelemetry API to create custom spans` },
      { key: "B", text: `Instrumentation libraries require the SDK as a dependency, while manual instrumentation does not` },
      { key: "C", text: `Instrumentation libraries produce only metrics, while manual instrumentation produces only traces` },
      { key: "D", text: `Instrumentation libraries are maintained by the application team, while manual instrumentation is maintained by the OpenTelemetry project` },
    ],
    correctAnswer: "A",
    explanation: `Instrumentation libraries (such as otelhttp for Go or opentelemetry-instrumentation for Python) wrap well-known frameworks and libraries to automatically create spans at boundaries like HTTP handlers and database drivers. Manual instrumentation uses the OpenTelemetry API directly (tracer.Start, span.End) to create custom spans for business-specific operations. Both approaches use the same underlying API.`,
    difficulty: "recall",
  },
  {
    id: "03-Q4",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A developer reviews a trace for a single admin console operation and finds it contains over 1,400 spans, most from internal ORM and JDBC operations. The trace is difficult to interpret. What should the developer do first?`,
    options: [
      { key: "A", text: `Increase the head-based sampling rate to reduce the number of stored traces` },
      { key: "B", text: `Switch from automatic to manual instrumentation for all services` },
      { key: "C", text: `Suppress unnecessary auto-instrumentation integrations in the agent configuration and keep only relevant ones` },
      { key: "D", text: `Add more span attributes to make the existing spans easier to filter` },
    ],
    correctAnswer: "C",
    explanation: `Auto-instrumentation agents like the Java agent support dozens of framework integrations enabled by default. Many produce telemetry that has no current or future debugging value. The agent configuration allows suppressing specific integrations, keeping only those that matter. This is more practical than rewriting all instrumentation from scratch.`,
    difficulty: "scenario",
  },
  {
    id: "03-Q5",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `Before adding manual spans to a code path, what should a developer check first?`,
    options: [
      { key: "A", text: `Whether the operation takes longer than 100 milliseconds` },
      { key: "B", text: `Whether an existing instrumentation library already covers the operation` },
      { key: "C", text: `Whether the SDK has been configured with a sampler` },
      { key: "D", text: `Whether the span will exceed the maximum attribute count` },
    ],
    correctAnswer: "B",
    explanation: `The recommended detection strategy is: first check for existing instrumentation libraries, then look for middleware or interceptor patterns, then verify auto-instrumentation availability, and only create manual spans if none of these exist. Duplicating instrumentation that a library already provides leads to redundant spans and confusion.`,
    difficulty: "recall",
  },
  {
    id: "03-Q6",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A microservice has been running reliably in production for several months. Its internal spans for data structure operations consistently show sub-millisecond latency with no anomalies. What is the recommended next step for this instrumentation?`,
    options: [
      { key: "A", text: `Convert the internal spans to span events on the boundary span` },
      { key: "B", text: `Remove all internal instrumentation immediately and rely on logs` },
      { key: "C", text: `Increase the span detail by adding more attributes to capture edge cases` },
      { key: "D", text: `Switch from manual to automatic instrumentation for consistency` },
    ],
    correctAnswer: "A",
    explanation: `Instrumentation cycles describe how telemetry should evolve with application maturity. Once internal operations have proven reliable, converting detailed internal spans into span events on the parent boundary span reduces cost (many backends charge per span) while preserving the information. The trade-off is losing independent per-operation duration data, which by this stage has already served its purpose.`,
    difficulty: "scenario",
  },
  {
    id: "03-Q7",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `What does "native instrumentation" mean in the context of OpenTelemetry?`,
    options: [
      { key: "A", text: `Instrumentation that uses eBPF to hook into kernel-level system calls` },
      { key: "B", text: `Instrumentation added manually by the application developer using the tracer API` },
      { key: "C", text: `Instrumentation provided by OpenTelemetry auto-instrumentation agents at runtime` },
      { key: "D", text: `Instrumentation built directly into a library or framework by its own maintainers` },
    ],
    correctAnswer: "D",
    explanation: `Native instrumentation occurs when the authors of a library or framework integrate OpenTelemetry support into their code. This is distinct from instrumentation libraries (maintained by the OpenTelemetry community to wrap third-party frameworks) and from auto-instrumentation (which uses runtime hooks or bytecode manipulation). Native instrumentation provides the tightest integration because the library authors understand their own internal semantics best.`,
    difficulty: "recall",
  },
  {
    id: "03-Q8",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An engineering team instruments a payment service with spans for JSON parsing, input validation, data formatting, and the actual payment gateway HTTP call. During a production incident, the trace view is cluttered with low-value spans. Which operations should retain their own spans?`,
    options: [
      { key: "A", text: `JSON parsing and input validation, because they run before the critical path` },
      { key: "B", text: `Only the payment gateway call, because it crosses a network boundary` },
      { key: "C", text: `All four operations, because any of them could cause the incident` },
      { key: "D", text: `Input validation and the payment gateway call, because they handle external data` },
    ],
    correctAnswer: "B",
    explanation: `JSON parsing, input validation, and data formatting are internal processing steps that do not cross application boundaries. The payment gateway call is an outbound HTTP request to an external system, making it a proper boundary for a span. Internal operations can be captured as span attributes or span events on the boundary span if needed.`,
    difficulty: "scenario",
  },
  {
    id: "03-Q9",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A Go developer uses eBPF-based auto-instrumentation and observes that distributed traces show disconnected spans rather than a connected trace tree. What is the most likely cause?`,
    options: [
      { key: "A", text: `The Go application was not compiled with CGO enabled` },
      { key: "B", text: `The OpenTelemetry Collector is dropping trace context headers` },
      { key: "C", text: `eBPF programs cannot write to user-space memory on newer Linux kernels, preventing context injection into outgoing requests` },
      { key: "D", text: `The developer did not configure a composite propagator in the SDK` },
    ],
    correctAnswer: "C",
    explanation: `Go eBPF auto-instrumentation can observe incoming and outgoing calls, but a Linux kernel security restriction prevents eBPF programs from writing to user-space memory. This means the eBPF probes cannot inject trace context (such as traceparent headers) into outgoing HTTP requests, resulting in disconnected spans instead of a properly linked trace tree. This is a known limitation that affects newer kernel versions.`,
    difficulty: "edge-case",
  },
  {
    id: "03-Q10",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `In OpenTelemetry's architecture, which component should application business logic depend on?`,
    options: [
      { key: "A", text: `The OpenTelemetry API only, not the SDK` },
      { key: "B", text: `The OpenTelemetry SDK only, not the API` },
      { key: "C", text: `Both the API and the SDK together` },
      { key: "D", text: `Neither; business logic should use instrumentation libraries exclusively` },
    ],
    correctAnswer: "A",
    explanation: `The OpenTelemetry API defines the stable interface for creating telemetry (spans, metrics, logs). The SDK provides the implementation for processing and exporting that telemetry. Business logic should depend only on the API, which has a no-op default when no SDK is configured. The SDK is configured separately at application startup. This separation allows instrumentation to exist in libraries and shared code without forcing a dependency on any specific SDK implementation.`,
    difficulty: "recall",
  },
  {
    id: "03-Q11",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team benchmarks their Java application and finds that tracing adds approximately 20% latency overhead at the 50th percentile with 10% sampling. A manager asks for the overhead to be reduced. Which approach is most effective?`,
    options: [
      { key: "A", text: `Remove all tracing instrumentation and rely on structured logs` },
      { key: "B", text: `Switch from the Java agent to manual instrumentation for all endpoints` },
      { key: "C", text: `Increase the batch processor queue size to buffer more spans` },
      { key: "D", text: `Lower the head-based sampling rate so fewer traces are created` },
    ],
    correctAnswer: "D",
    explanation: `Instrumentation imposes a roughly fixed cost per operation. Reducing the sampling rate decreases the number of traces created and exported, directly reducing overhead. Switching to manual instrumentation would not necessarily reduce the per-span cost. Increasing queue size affects export batching, not trace creation overhead. Removing all instrumentation sacrifices production debugging capability.`,
    difficulty: "scenario",
  },
  {
    id: "03-Q12",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A developer creates a span that covers the entire lifetime of a long-running service, from startup to shutdown, intending to measure total uptime. What is wrong with this approach?`,
    options: [
      { key: "A", text: `The span should be closed before the application blocks waiting for shutdown signals, because an always-open span provides no debugging value` },
      { key: "B", text: `The OTLP specification defines a maximum span duration that the span will exceed` },
      { key: "C", text: `Long-running spans consume exponentially increasing memory in the SDK's span buffer` },
      { key: "D", text: `The collector will silently drop spans that exceed its configurable maximum duration` },
    ],
    correctAnswer: "A",
    explanation: `OpenTelemetry instrumentation guidelines state that application startup spans must be closed before the application blocks waiting for shutdown signals. A span that remains open for the entire application lifetime has a duration equal to the uptime, which provides no useful information for debugging. The correct pattern is to create a span for initialization, close it, then create a new span for graceful shutdown when that phase begins.`,
    difficulty: "edge-case",
  },
  {
    id: "03-Q13",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team is starting their observability journey with a legacy Java application that has no built-in telemetry. They want to understand request flow and identify slow database queries. After deploying automatic instrumentation via the Java agent, what should be their next step?`,
    options: [
      { key: "A", text: `Replace all auto-instrumentation with hand-written manual spans immediately` },
      { key: "B", text: `Add manual spans inside every service method to capture internal timing` },
      { key: "C", text: `Add instrumentation libraries for their database driver and HTTP framework, then add manual spans for business-critical operations` },
      { key: "D", text: `Remove the Java agent and write manual spans only for database queries` },
    ],
    correctAnswer: "C",
    explanation: `The recommended progression combines instrumentation approaches. Auto-instrumentation provides immediate broad visibility. Instrumentation libraries offer more targeted telemetry for specific frameworks (HTTP, database). Manual instrumentation adds business-specific context that neither auto nor libraries can provide. These three approaches can coexist in the same application, with each layer adding value.`,
    difficulty: "scenario",
  },
  {
    id: "03-Q14",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `Which of the following operations should be instrumented even when it completes successfully?`,
    options: [
      { key: "A", text: `An in-memory cache lookup that returns a cache hit` },
      { key: "B", text: `A JSON deserialization step that parses input correctly` },
      { key: "C", text: `A health check endpoint that returns 200 OK` },
      { key: "D", text: `A payment capture that processes a customer's transaction` },
    ],
    correctAnswer: "D",
    explanation: `Critical business milestones such as payment captures, order placements, user account creations, and subscription renewals should always be instrumented, even when successful, because they have business and audit value and need positive confirmation. In contrast, cache hits, routine parsing, and health check endpoints are expected operations that add telemetry noise without debugging value.`,
    difficulty: "recall",
  },
  {
    id: "04-Q1",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `How do exemplars bridge the gap between metrics and traces in OpenTelemetry?`,
    options: [
      { key: "A", text: `By converting all metrics into trace spans automatically` },
      { key: "B", text: `By embedding trace queries inside metric alert definitions` },
      { key: "C", text: `By attaching trace IDs and span IDs to specific metric data points, linking aggregated data to individual request traces` },
      { key: "D", text: `By storing metrics and traces in the same database table` },
    ],
    correctAnswer: "C",
    explanation: `Exemplars allow metrics data points to carry references (trace_id, span_id) to specific traces that contributed to that measurement. This enables engineers to jump from an anomalous metric value directly to a representative trace for investigation.`,
    difficulty: "recall",
  },
  {
    id: "04-Q2",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An SRE team defines an availability SLO of "99.9% of requests return successfully." Their system receives a mix of HTTP 4xx and 5xx responses. When calculating the availability SLI, how should HTTP 4xx responses be treated?`,
    options: [
      { key: "A", text: `As successful requests, because client errors reflect caller mistakes and are not service availability failures` },
      { key: "B", text: `As failures that consume the error budget equally with 5xx errors` },
      { key: "C", text: `As partial failures weighted at 50% compared to server errors` },
      { key: "D", text: `They should be excluded from both the numerator and denominator` },
    ],
    correctAnswer: "A",
    explanation: `For availability SLIs, only server errors (HTTP 5xx) count as failures. Client errors (4xx) represent problems on the caller side (bad input, unauthorized access) and do not indicate that the service itself is unavailable. The SLI formula uses HTTP status codes < 500 as the success criterion.`,
    difficulty: "scenario",
  },
  {
    id: "04-Q3",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team observes high latency in their checkout service. Distributed traces show the service calls a payment gateway and an inventory service. The payment gateway spans consistently take 2 seconds. What is the most appropriate next step in a root cause analysis workflow?`,
    options: [
      { key: "A", text: `Add more spans to every internal function in the checkout service` },
      { key: "B", text: `Immediately scale up the checkout service replicas` },
      { key: "C", text: `Enable debug logging on all services to capture more data` },
      { key: "D", text: `Examine the payment gateway span attributes for error codes, retry counts, and query durations to pinpoint the bottleneck` },
    ],
    correctAnswer: "D",
    explanation: `Root cause analysis with distributed traces follows a drill-down approach: start from the trace waterfall, identify the slow span, then inspect its attributes and events for details about the failure or delay. Adding more spans or debug logging at this stage would introduce noise without narrowing the cause. Scaling does not address the root issue.`,
    difficulty: "scenario",
  },
  {
    id: "04-Q4",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `In OpenTelemetry, how are logs correlated with distributed traces?`,
    options: [
      { key: "A", text: `By matching log timestamps to span start times` },
      { key: "B", text: `By including the trace_id and span_id fields in log records, linking each log entry to the span that was active when the log was emitted` },
      { key: "C", text: `By routing logs and traces through the same Collector pipeline` },
      { key: "D", text: `By using the same exporter endpoint for both signals` },
    ],
    correctAnswer: "B",
    explanation: `OpenTelemetry's log data model includes trace_id and span_id fields. When a log is emitted within the context of an active span, the SDK automatically populates these fields, creating a direct link between the log entry and the specific trace and span. This enables jumping from a trace to its associated logs and vice versa.`,
    difficulty: "recall",
  },
  {
    id: "04-Q5",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A platform team already has distributed tracing in production but lacks metrics instrumentation. They need RED (Request, Error, Duration) metrics for their HTTP endpoints. What is the most efficient approach using OpenTelemetry?`,
    options: [
      { key: "A", text: `Use the Span Metrics Connector in the Collector to derive RED metrics from existing trace data without additional application instrumentation` },
      { key: "B", text: `Add metrics instrumentation to every service's application code alongside the existing tracing` },
      { key: "C", text: `Use the transform processor to convert trace spans into log-format metrics` },
      { key: "D", text: `Deploy a separate Prometheus exporter in each service` },
    ],
    correctAnswer: "A",
    explanation: `The Span Metrics Connector is a Collector component that consumes traces from one pipeline and produces metrics in another. It generates request count (calls_total) and duration histogram metrics from spans, providing RED metrics without requiring any changes to application code. This is more efficient than duplicating instrumentation effort across all services.`,
    difficulty: "scenario",
  },
  {
    id: "04-Q6",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `Which statement correctly describes the relationship between SLOs, SLIs, and telemetry data?`,
    options: [
      { key: "A", text: `SLIs are raw telemetry signals; SLOs are the dashboards that display them` },
      { key: "B", text: `SLOs define the measurements; SLIs set the reliability targets` },
      { key: "C", text: `SLIs and SLOs are defined independently of telemetry data` },
      { key: "D", text: `SLOs set reliability targets, while SLIs are quantitative measures derived from telemetry that indicate whether those targets are met` },
    ],
    correctAnswer: "D",
    explanation: `An SLO (Service Level Objective) defines a target, such as "99.9% availability" or "P95 latency under 200ms." An SLI (Service Level Indicator) is the actual measurement, calculated from telemetry data (metrics, traces), that tracks real performance against the objective. Without properly instrumented telemetry feeding the SLIs, SLOs cannot be meaningfully monitored.`,
    difficulty: "recall",
  },
  {
    id: "04-Q7",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An e-commerce team instruments their application and produces over 400 spans per checkout request by creating spans for every internal function. During a production incident, the on-call engineer struggles to identify the root cause in the trace viewer. What is the most likely problem?`,
    options: [
      { key: "A", text: `The tracing backend cannot handle that many spans per trace` },
      { key: "B", text: `The trace viewer has a rendering limitation` },
      { key: "C", text: `Over-instrumentation creates noise that obscures the actual bottleneck, increasing mean time to resolution` },
      { key: "D", text: `The sampling rate is too low to capture the failing requests` },
    ],
    correctAnswer: "C",
    explanation: `Excessive instrumentation of internal functions generates large volumes of short-duration spans that add no debugging value. During an incident, this noise makes the trace waterfall difficult to read and slows down root cause identification. Purposeful instrumentation focuses spans at application boundaries (HTTP, database, messaging) where meaningful latency and errors occur.`,
    difficulty: "scenario",
  },
  {
    id: "04-Q8",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A metric has an exemplar attached with a trace_id. However, the referenced trace was dropped by tail sampling in the Collector. What happens when an engineer clicks the exemplar link?`,
    options: [
      { key: "A", text: `The system automatically reconstructs the trace from the exemplar data` },
      { key: "B", text: `The trace is not found, because exemplars store only a reference to the trace and do not guarantee that the linked trace was retained` },
      { key: "C", text: `The metric data point is also deleted to maintain consistency between signals` },
      { key: "D", text: `The exemplar automatically redirects to the nearest available trace in the same time window` },
    ],
    correctAnswer: "B",
    explanation: `Exemplars contain trace and span IDs as references, but they have no mechanism to ensure the referenced trace survives sampling or retention policies. When tail sampling drops a trace, the exemplar link becomes a dead reference. This is a known trade-off when combining exemplars with aggressive sampling strategies.`,
    difficulty: "edge-case",
  },
  {
    id: "04-Q9",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `According to OpenTelemetry best practices, which of the following should be recorded as a span event rather than a separate child span?`,
    options: [
      { key: "A", text: `An outgoing HTTP call to an external API` },
      { key: "B", text: `A database query that takes 50ms` },
      { key: "C", text: `A message published to a queue` },
      { key: "D", text: `A retry attempt within an ongoing operation` },
    ],
    correctAnswer: "D",
    explanation: `Span events are for recording noteworthy occurrences within an existing span's lifetime, such as retry attempts, cache misses, or fallback activations. Outgoing HTTP calls, database queries, and message publishing are application boundary operations that warrant their own spans with appropriate span kinds (CLIENT, PRODUCER).`,
    difficulty: "recall",
  },
  {
    id: "04-Q10",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `An observability team sets a latency SLO for their API: "P95 latency under 200ms." They measure latency using a histogram at the HTTP handler boundary. Why is the HTTP handler the correct measurement point?`,
    options: [
      { key: "A", text: `Because it captures the full request duration as experienced by the caller, including all downstream processing` },
      { key: "B", text: `Because the HTTP handler is the only component where the SDK can create spans` },
      { key: "C", text: `Because measuring at individual downstream services provides more accurate latency data` },
      { key: "D", text: `Because internal function measurements produce lower latency values that look better in dashboards` },
    ],
    correctAnswer: "A",
    explanation: `The HTTP handler represents the application boundary where the user's experience begins and ends. Measuring latency here captures the total time including all downstream calls, serialization, and processing. Measuring at individual downstream components would only show partial latency and miss the end-to-end picture that the SLO targets.`,
    difficulty: "scenario",
  },
  {
    id: "04-Q11",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `A team uses an UpDownCounter to track active HTTP requests. They increment the counter at request start with attributes \`{method="GET", path="/api/users"}\` and decrement at request end with attributes \`{method="GET", path="/api/users", status="200"}\`. What problem does this cause?`,
    options: [
      { key: "A", text: `UpDownCounter does not support string attribute values` },
      { key: "B", text: `The counter value becomes negative after the first completed request` },
      { key: "C", text: `The increment and decrement produce different time series because the attribute sets differ, so the counter never actually decreases` },
      { key: "D", text: `The status attribute causes a cardinality explosion across all counter measurements` },
    ],
    correctAnswer: "C",
    explanation: `UpDownCounter increments and decrements must use the same attribute values to affect the same time series. Adding the status attribute at decrement time creates a new time series (\`method=GET, path=/api/users, status=200\`) that is separate from the increment time series (\`method=GET, path=/api/users\`). The original series increases but never decreases. Only attributes available at both increment and decrement time should be used.`,
    difficulty: "edge-case",
  },
  {
    id: "04-Q12",
    domain: "fundamentals",
    domainLabel: "Fundamentals of Observability",
    question: `When investigating a production issue, an engineer has access to traces, metrics, and logs from their OpenTelemetry-instrumented services. In which situation would starting with trace-based analysis be more effective than starting with logs?`,
    options: [
      { key: "A", text: `When the issue involves a startup failure before any requests are served` },
      { key: "B", text: `When the issue involves elevated latency spread across multiple services in a request path` },
      { key: "C", text: `When the issue is a configuration error that prevents the application from booting` },
      { key: "D", text: `When the issue is a memory leak in a long-running background process` },
    ],
    correctAnswer: "B",
    explanation: `Traces excel at revealing latency distribution and request flow across service boundaries, making them ideal for diagnosing issues that span multiple services. Logs are better suited for application lifecycle events (startup, shutdown, configuration errors) and detailed diagnostic context within a single process. Starting with the right signal type accelerates root cause identification.`,
    difficulty: "recall",
  },
  {
    id: "05-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the correct nesting hierarchy in the OTLP data model for all signals?`,
    options: [
      { key: "A", text: `InstrumentationScope wraps Resource, which wraps data units` },
      { key: "B", text: `Data units are at the top level, with Resource as a nested field` },
      { key: "C", text: `Resource wraps InstrumentationScope, which wraps the signal data units` },
      { key: "D", text: `Signal data units wrap InstrumentationScope and Resource as child fields` },
    ],
    correctAnswer: "C",
    explanation: `All OTLP signals follow the same hierarchy: the root message (e.g., TracesData) contains ResourceSpans/ResourceMetrics/ResourceLogs, each of which contains ScopeSpans/ScopeMetrics/ScopeLogs, which finally contain the data units (Span, Metric, LogRecord). This batching-friendly structure groups telemetry by its producing entity and instrumentation library.`,
    difficulty: "recall",
  },
  {
    id: "05-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What does InstrumentationScope identify in the OTLP data model?`,
    options: [
      { key: "A", text: `The library or SDK component generating the telemetry` },
      { key: "B", text: `The service or host producing the telemetry data` },
      { key: "C", text: `The backend system receiving the telemetry` },
      { key: "D", text: `The network transport protocol used for export` },
    ],
    correctAnswer: "A",
    explanation: `InstrumentationScope identifies the instrumentation library or SDK component that generated the telemetry. It includes a name (typically the package import path), an optional version, and optional attributes. This allows backends to distinguish telemetry produced by different instrumentation libraries within the same service.`,
    difficulty: "recall",
  },
  {
    id: "05-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team needs to record a list of retry attempts as an attribute on a span, where each entry is a string describing the attempt. Which AnyValue type is appropriate?`,
    options: [
      { key: "A", text: `A string_value with comma-separated entries` },
      { key: "B", text: `A bytes_value containing serialized data` },
      { key: "C", text: `A kvlist_value with numeric keys` },
      { key: "D", text: `An array_value containing string values` },
    ],
    correctAnswer: "D",
    explanation: `The OTLP AnyValue type supports array_value, which holds a list of AnyValue elements. For a list of strings, using array_value preserves the structured nature of the data without requiring custom parsing. Since OTLP 1.9.0, all value types including arrays are valid in any attribute context.`,
    difficulty: "scenario",
  },
  {
    id: "05-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What are the sizes of trace_id and span_id in the OTLP trace data model?`,
    options: [
      { key: "A", text: `8 bytes and 4 bytes respectively` },
      { key: "B", text: `16 bytes and 8 bytes respectively` },
      { key: "C", text: `32 bytes and 16 bytes respectively` },
      { key: "D", text: `Both are 16 bytes` },
    ],
    correctAnswer: "B",
    explanation: `In the OTLP trace data model, trace_id is a 16-byte (128-bit) identifier that uniquely identifies a distributed trace, while span_id is an 8-byte (64-bit) identifier unique within that trace. These sizes are defined in the protobuf schema and align with the W3C Trace Context specification.`,
    difficulty: "recall",
  },
  {
    id: "05-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A service publishes order events to a message queue for asynchronous processing by downstream consumers. Which SpanKind should the publishing span use?`,
    options: [
      { key: "A", text: `PRODUCER` },
      { key: "B", text: `CLIENT` },
      { key: "C", text: `SERVER` },
      { key: "D", text: `INTERNAL` },
    ],
    correctAnswer: "A",
    explanation: `PRODUCER (value 4) is the correct SpanKind for operations that enqueue messages for asynchronous processing. CLIENT is for synchronous outbound requests (HTTP, gRPC, database calls), while PRODUCER specifically represents message queue publishing where the response is not immediately expected. The matching CONSUMER kind is used by the downstream service processing the message.`,
    difficulty: "scenario",
  },
  {
    id: "05-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Under what condition should the message field of a span's Status be populated?`,
    options: [
      { key: "A", text: `Whenever the span completes, to describe the outcome` },
      { key: "B", text: `Only when the status code is OK, to confirm success` },
      { key: "C", text: `Only when the status code is ERROR` },
      { key: "D", text: `Whenever the span has recorded events` },
    ],
    correctAnswer: "C",
    explanation: `The OTLP specification states that the Status message field should only be set when the status code is ERROR. The message provides a human-readable description of the error. Setting a message with OK or UNSET status violates the data model contract and may cause unexpected behavior in backends.`,
    difficulty: "edge-case",
  },
  {
    id: "05-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A batch processing service collects items from multiple independent traces and processes them together in a single operation. How should this service relate to the original trace contexts?`,
    options: [
      { key: "A", text: `Set parent_span_id to one of the originating spans and discard the others` },
      { key: "B", text: `Create span links to each originating span` },
      { key: "C", text: `Start a new trace with no references to the original contexts` },
      { key: "D", text: `Copy all originating trace_ids into span attributes` },
    ],
    correctAnswer: "B",
    explanation: `Span links represent causal relationships between spans that are not parent-child. Batch processing is the canonical use case: the processing span links to each input span across different traces, preserving the causal connection without forcing all items into a single trace hierarchy. Links carry their own trace_id, span_id, and optional attributes.`,
    difficulty: "scenario",
  },
  {
    id: "05-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which of the following is NOT a metric data type defined in the OTLP metrics data model?`,
    options: [
      { key: "A", text: `Gauge` },
      { key: "B", text: `ExponentialHistogram` },
      { key: "C", text: `Sum` },
      { key: "D", text: `Timer` },
    ],
    correctAnswer: "D",
    explanation: `The OTLP metrics data model defines five types: Gauge (point-in-time values), Sum (accumulated values with temporality), Histogram (explicit bucket distributions), ExponentialHistogram (exponential bucket distributions), and Summary (pre-calculated quantiles). There is no Timer type; duration measurements are typically represented using Histograms.`,
    difficulty: "recall",
  },
  {
    id: "05-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A development team needs to track the total number of HTTP requests received by their service. How should this metric be represented in the OTLP data model?`,
    options: [
      { key: "A", text: `A Sum with is_monotonic set to true` },
      { key: "B", text: `A Gauge with a continuously increasing value` },
      { key: "C", text: `A Histogram with a single bucket` },
      { key: "D", text: `A Summary with only the count field populated` },
    ],
    correctAnswer: "A",
    explanation: `A request counter is a monotonic sum: it only increases over time. In OTLP, this is represented as a Sum metric type with is_monotonic set to true and an appropriate aggregation_temporality (DELTA or CUMULATIVE). A Gauge would be incorrect because gauges have no aggregation semantics and represent point-in-time values, not accumulated totals.`,
    difficulty: "recall",
  },
  {
    id: "05-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What does AGGREGATION_TEMPORALITY_CUMULATIVE mean for a Sum metric data point?`,
    options: [
      { key: "A", text: `The value represents the change since the last report` },
      { key: "B", text: `The value is an instantaneous point-in-time measurement` },
      { key: "C", text: `The value represents the total accumulated since the process started` },
      { key: "D", text: `The value is an average calculated over the collection interval` },
    ],
    correctAnswer: "C",
    explanation: `Cumulative temporality means each data point reports the total accumulated value since the process start (indicated by start_time_unix_nano). To calculate a rate, consumers compute (current - previous) / time_interval. In contrast, DELTA temporality reports only the change since the last report. Omitting start_time_unix_nano for cumulative metrics breaks rate calculation after restarts.`,
    difficulty: "recall",
  },
  {
    id: "05-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A histogram metric has explicit_bounds set to [10, 25, 50, 100]. How many entries will the bucket_counts array contain?`,
    options: [
      { key: "A", text: `4` },
      { key: "B", text: `5` },
      { key: "C", text: `3` },
      { key: "D", text: `8` },
    ],
    correctAnswer: "B",
    explanation: `The bucket_counts array always has one more entry than explicit_bounds. With N boundaries, there are N+1 buckets: one for each range between boundaries, plus underflow and overflow buckets. For bounds [10, 25, 50, 100], the five buckets are: (-inf, 10], (10, 25], (25, 50], (50, 100], (100, +inf).`,
    difficulty: "edge-case",
  },
  {
    id: "05-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An SRE team wants to investigate which specific requests caused a spike in their latency histogram. Which OTLP metric feature enables correlating metric data points with individual traces?`,
    options: [
      { key: "A", text: `Data point attributes with a trace_id dimension` },
      { key: "B", text: `Schema URLs linking metrics to trace data` },
      { key: "C", text: `Metric metadata fields` },
      { key: "D", text: `Exemplars containing trace_id and span_id` },
    ],
    correctAnswer: "D",
    explanation: `Exemplars are sample measurements attached to metric data points that carry trace_id and span_id fields, enabling direct correlation between a metric observation and the trace that produced it. They also include a timestamp and optional filtered_attributes. Best practice is to include exemplars for latency metrics and sample representative values such as high-latency or error cases.`,
    difficulty: "scenario",
  },
  {
    id: "05-Q13",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `How many severity levels does the OTLP log data model define?`,
    options: [
      { key: "A", text: `24 levels, organized as 6 base severities with 4 sub-levels each` },
      { key: "B", text: `8 levels matching standard syslog severity` },
      { key: "C", text: `6 levels: TRACE, DEBUG, INFO, WARN, ERROR, FATAL` },
      { key: "D", text: `12 levels with 2 sub-levels per base severity` },
    ],
    correctAnswer: "A",
    explanation: `The OTLP severity scale has 24 levels (severity numbers 1-24), organized as six base severities (TRACE, DEBUG, INFO, WARN, ERROR, FATAL) with four sub-levels each (e.g., DEBUG, DEBUG2, DEBUG3, DEBUG4). This fine-grained scale enables precise mapping from source logging frameworks like Java's FINEST/FINER/FINE/CONFIG levels. Severity number 0 is UNSPECIFIED, meaning unknown.`,
    difficulty: "recall",
  },
  {
    id: "05-Q14",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A log collection agent reads application log files. A log entry was originally written at 14:00:00, but the agent reads it five seconds later at 14:00:05. How should the LogRecord timestamps be set?`,
    options: [
      { key: "A", text: `Set only time_unix_nano to 14:00:05` },
      { key: "B", text: `Set only observed_time_unix_nano to 14:00:00` },
      { key: "C", text: `Set time_unix_nano to 14:00:00 and observed_time_unix_nano to 14:00:05` },
      { key: "D", text: `Set both timestamps to 14:00:00` },
    ],
    correctAnswer: "C",
    explanation: `The OTLP log data model distinguishes between when an event occurred (time_unix_nano) and when it was collected or observed (observed_time_unix_nano). The specification requires that observed_time_unix_nano is always set, while time_unix_nano should be set when the actual event time is known. In file-based log collection, the parsed timestamp from the log line is the event time, and the collection time is the observed time.`,
    difficulty: "scenario",
  },
  {
    id: "05-Q15",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What does the OTLP specification require about attribute keys within a single attribute collection?`,
    options: [
      { key: "A", text: `Keys can be duplicated if values differ in type` },
      { key: "B", text: `Keys must be unique within a single collection` },
      { key: "C", text: `Keys are case-insensitive and deduplicated automatically` },
      { key: "D", text: `Keys must follow a dot-separated hierarchical naming pattern` },
    ],
    correctAnswer: "B",
    explanation: `The OTLP specification requires that attribute keys be unique within a single collection (e.g., within one span's attributes or one data point's attributes). Duplicate keys violate the specification. While dot-separated naming is a convention used by semantic conventions, it is not a requirement of the attribute data model itself.`,
    difficulty: "edge-case",
  },
  {
    id: "05-Q16",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An application emits structured event data containing a message, a user identifier, and an action type. What is the recommended way to represent this in an OTLP LogRecord?`,
    options: [
      { key: "A", text: `Concatenate all fields into a single string body` },
      { key: "B", text: `Use bytes_value body with JSON-encoded content` },
      { key: "C", text: `Store everything as resource attributes` },
      { key: "D", text: `Use structured attributes for searchable fields and a human-readable string body for the message` },
    ],
    correctAnswer: "D",
    explanation: `The OTLP log data model recommends using attributes for searchable and filterable fields (like user ID and action type) and the body for the human-readable message. While kvlist_value bodies are supported, placing structured data in attributes enables backends to index and query individual fields efficiently. The body should contain the human-readable log message.`,
    difficulty: "scenario",
  },
  {
    id: "05-Q17",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the recommendation regarding the Summary metric type for new OpenTelemetry instrumentation?`,
    options: [
      { key: "A", text: `It is not recommended; use Histogram or ExponentialHistogram instead` },
      { key: "B", text: `It is the preferred type for latency distribution measurements` },
      { key: "C", text: `It should only be used with DELTA temporality` },
      { key: "D", text: `It is required when exporting to Prometheus-compatible backends` },
    ],
    correctAnswer: "A",
    explanation: `The OTLP specification explicitly states that Summary is not recommended for new instrumentation. Summary exists primarily for Prometheus compatibility with pre-calculated quantiles. For new metrics, Histogram (with explicit bucket boundaries) or ExponentialHistogram (with automatic exponential bucket scaling) provide more flexible and accurate distribution data that backends can re-aggregate.`,
    difficulty: "edge-case",
  },
  {
    id: "05-Q18",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A Collector sends a batch of 1,000 spans to a backend via OTLP. The response contains a PartialSuccess with rejected_spans set to 50 and an error_message explaining the reason. What does this mean?`,
    options: [
      { key: "A", text: `All 1,000 spans failed and none were persisted` },
      { key: "B", text: `The request should be retried in its entirety` },
      { key: "C", text: `950 spans were accepted successfully and 50 were rejected` },
      { key: "D", text: `The 50 rejected spans will be automatically retried by the protocol` },
    ],
    correctAnswer: "C",
    explanation: `The OTLP partial success pattern allows receivers to accept some data while rejecting the rest. The rejected_spans (or rejected_data_points, rejected_log_records) count indicates how many items were not accepted, while the error_message provides diagnostic information. The remaining items (1,000 - 50 = 950) were successfully accepted. The sender must decide how to handle the rejected items; the protocol does not automatically retry them.`,
    difficulty: "scenario",
  },
  {
    id: "06-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the primary reason the OpenTelemetry specification separates the API from the SDK?`,
    options: [
      { key: "A", text: `So that library authors can instrument code without forcing SDK dependencies on end users` },
      { key: "B", text: `To allow the API to be written in one language and the SDK in another` },
      { key: "C", text: `To ensure the API runs on the client side and the SDK runs on the server side` },
      { key: "D", text: `To enable the API to support traces while the SDK focuses on metrics` },
    ],
    correctAnswer: "A",
    explanation: `The API/SDK separation allows library authors to depend only on the lightweight API for instrumentation, while application owners choose and configure the SDK. This prevents transitive dependency conflicts and allows different SDK implementations to be swapped in.`,
    difficulty: "recall",
  },
  {
    id: "06-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What happens when application code calls OpenTelemetry API methods but no SDK has been installed?`,
    options: [
      { key: "A", text: `The API throws a runtime exception` },
      { key: "B", text: `The API operates as a no-op, producing no telemetry and incurring no overhead` },
      { key: "C", text: `The API queues telemetry data until an SDK is registered` },
      { key: "D", text: `The API writes telemetry to standard output as a fallback` },
    ],
    correctAnswer: "B",
    explanation: `The OpenTelemetry API is no-op by default. Without an SDK, all API calls (creating spans, recording metrics) do nothing, ensuring zero runtime overhead and no side effects. This is a deliberate design choice that makes it safe for library authors to add instrumentation without affecting applications that do not use OpenTelemetry.`,
    difficulty: "recall",
  },
  {
    id: "06-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team maintains an open-source HTTP client library used by hundreds of applications. They want to add OpenTelemetry tracing support. What should they depend on?`,
    options: [
      { key: "A", text: `The full OpenTelemetry SDK to ensure spans are exported` },
      { key: "B", text: `Both the API and SDK, configuring a default OTLP exporter` },
      { key: "C", text: `Only the OpenTelemetry API, leaving SDK choice to application owners` },
      { key: "D", text: `A vendor-specific SDK that provides a built-in exporter` },
    ],
    correctAnswer: "C",
    explanation: `Library authors should depend only on the API. This ensures the library does not force a particular SDK, exporter, or sampling strategy on the applications that consume it. The application owner configures the SDK and exporters at initialization time. If no SDK is present, the API calls simply do nothing.`,
    difficulty: "scenario",
  },
  {
    id: "06-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `All OpenTelemetry signals follow the same provider pattern. What is the correct sequence of this pattern?`,
    options: [
      { key: "A", text: `Component creates Provider, which produces Data` },
      { key: "B", text: `Data is ingested by Provider, which routes to Component` },
      { key: "C", text: `Provider and Component both produce Data independently` },
      { key: "D", text: `Provider creates Component, which produces Data` },
    ],
    correctAnswer: "D",
    explanation: `The provider pattern is: Provider (entry point, e.g., TracerProvider) creates a Component (e.g., Tracer), which then produces Data (e.g., Spans). This consistent pattern applies to all three signals: traces (TracerProvider, Tracer, Span), metrics (MeterProvider, Meter, Instrument), and logs (LoggerProvider, Logger, LogRecord).`,
    difficulty: "recall",
  },
  {
    id: "06-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A developer builds a custom Collector processor that adds attributes to spans. During testing, spans from other parallel consumers in a fan-out pipeline are also unexpectedly modified. What is the most likely cause?`,
    options: [
      { key: "A", text: `The processor declared MutatesData as false but still modifies the data it receives` },
      { key: "B", text: `The processor is forwarding data to the wrong consumer` },
      { key: "C", text: `The Collector's fan-out implementation has a concurrency bug` },
      { key: "D", text: `The processor is using the wrong signal-specific consumer interface` },
    ],
    correctAnswer: "A",
    explanation: `When MutatesData is false, the Collector may share the same data instance across multiple consumers for efficiency. If a processor modifies data despite declaring MutatesData as false, those changes are visible to all consumers receiving the same data. The processor should declare MutatesData as true so the Collector clones the data before passing it.`,
    difficulty: "scenario",
  },
  {
    id: "06-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In the OpenTelemetry Collector, what does the factory pattern provide when creating a custom component?`,
    options: [
      { key: "A", text: `A runtime plugin loader that discovers components on the filesystem` },
      { key: "B", text: `A standard way to create component instances and supply default configuration` },
      { key: "C", text: `A dependency injection framework for wiring components together` },
      { key: "D", text: `A code generation tool that scaffolds the component boilerplate` },
    ],
    correctAnswer: "B",
    explanation: `The factory pattern in the Collector provides a standardized way to instantiate components and supply their default configuration. Each factory specifies the component type, the default config function, and which signal types it supports along with their stability levels. This allows the Collector framework to create and manage all component types consistently.`,
    difficulty: "recall",
  },
  {
    id: "06-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team writes a custom Collector exporter that sends telemetry over HTTP. When the backend returns HTTP 400 Bad Request, the exporter returns a generic error. What problem does this cause?`,
    options: [
      { key: "A", text: `The Collector immediately shuts down the entire pipeline` },
      { key: "B", text: `The error is silently dropped and the data is lost` },
      { key: "C", text: `The exporter helper's retry mechanism keeps retrying a request that will never succeed` },
      { key: "D", text: `The batch processor splits the batch in half and retries each part separately` },
    ],
    correctAnswer: "C",
    explanation: `Without marking the error as permanent using consumererror.NewPermanent, the exporter helper's retry mechanism treats it as a retryable error. HTTP 400 errors are client errors that will not succeed on retry, wasting resources and queue capacity. Proper error classification distinguishes permanent errors (4xx) from retryable errors (5xx, network failures).`,
    difficulty: "scenario",
  },
  {
    id: "06-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In the Collector's consumer interface, what happens to data ownership after a consumer's Consume method returns successfully?`,
    options: [
      { key: "A", text: `The caller retains ownership and can continue modifying the data` },
      { key: "B", text: `Ownership is shared between the caller and consumer until the pipeline completes` },
      { key: "C", text: `The data is immediately serialized and cannot be accessed by either party` },
      { key: "D", text: `The consumer owns the data, and the caller must not access it again` },
    ],
    correctAnswer: "D",
    explanation: `After a successful Consume call (ConsumeTraces, ConsumeMetrics, or ConsumeLogs), ownership transfers to the consumer. The caller must not read or modify the data after the call returns, because the consumer may have already forwarded, modified, or freed it. This ownership model enables efficient zero-copy data flow through the pipeline.`,
    difficulty: "edge-case",
  },
  {
    id: "06-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which built-in SpanProcessor implementation buffers spans and sends them in bulk to reduce export overhead?`,
    options: [
      { key: "A", text: `BatchSpanProcessor` },
      { key: "B", text: `SimpleSpanProcessor` },
      { key: "C", text: `BufferedSpanProcessor` },
      { key: "D", text: `AsyncSpanProcessor` },
    ],
    correctAnswer: "A",
    explanation: `BatchSpanProcessor collects finished spans and exports them in batches based on configured thresholds (queue size, batch size, and timeout). SimpleSpanProcessor exports each span immediately as it ends, which is simpler but less efficient for production workloads. There are no standard processors named BufferedSpanProcessor or AsyncSpanProcessor in the OpenTelemetry specification.`,
    difficulty: "recall",
  },
  {
    id: "06-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An SRE team needs to collect metrics from the SDK on demand during integration tests, rather than waiting for a periodic export interval. Which MetricReader should they use?`,
    options: [
      { key: "A", text: `PeriodicExportingMetricReader with the interval set to zero` },
      { key: "B", text: `ManualMetricReader` },
      { key: "C", text: `OnDemandMetricReader` },
      { key: "D", text: `PullMetricReader` },
    ],
    correctAnswer: "B",
    explanation: `ManualMetricReader allows collecting metrics on demand by calling its collect method explicitly, which is ideal for testing and integration scenarios. PeriodicExportingMetricReader exports at fixed intervals and does not support deterministic on-demand collection. OnDemandMetricReader and PullMetricReader are not standard SDK components defined in the specification.`,
    difficulty: "scenario",
  },
  {
    id: "06-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An operations team maintains a custom Collector exporter that sometimes fails to send part of a batch. They want the retry mechanism to only resend the failed items rather than the entire batch. How should they report the failure?`,
    options: [
      { key: "A", text: `Return a generic error wrapping the count of failed items` },
      { key: "B", text: `Return consumererror.NewPermanent with the failed items attached` },
      { key: "C", text: `Return a signal-specific error using consumererror.NewTraces (or NewMetrics/NewLogs) containing only the failed data` },
      { key: "D", text: `Split the batch manually within the push function and retry each item individually` },
    ],
    correctAnswer: "C",
    explanation: `The consumererror package provides signal-specific error constructors (NewTraces, NewMetrics, NewLogs) that carry the subset of data that failed. The exporter helper's retry mechanism uses this subset for retries rather than resending the entire original batch. This enables efficient partial failure handling without custom retry logic.`,
    difficulty: "scenario",
  },
  {
    id: "06-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `When implementing a custom Collector component, the Shutdown method is called during collector shutdown. Which requirement must the Shutdown method satisfy?`,
    options: [
      { key: "A", text: `It must return an error if called more than once` },
      { key: "B", text: `It must block indefinitely until all background operations complete` },
      { key: "C", text: `It must panic if called before Start` },
      { key: "D", text: `It must be idempotent, meaning it is safe to call multiple times` },
    ],
    correctAnswer: "D",
    explanation: `The OpenTelemetry Collector component interface requires Shutdown to be idempotent. It may be called multiple times during error recovery or shutdown sequences, and each call must succeed without side effects. The implementation should also flush any buffered data and release acquired resources.`,
    difficulty: "edge-case",
  },
  {
    id: "06-Q13",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the role of the Capabilities struct (containing MutatesData) in the Collector's consumer interface?`,
    options: [
      { key: "A", text: `It declares whether the component modifies the telemetry data it receives, allowing the Collector to optimize data sharing` },
      { key: "B", text: `It lists which signal types the component supports` },
      { key: "C", text: `It specifies the maximum throughput the component can handle` },
      { key: "D", text: `It indicates whether the component supports graceful shutdown` },
    ],
    correctAnswer: "A",
    explanation: `The Capabilities struct with its MutatesData field tells the Collector whether a component will modify the data passed to it. When MutatesData is false, the Collector can safely share the same data instance with multiple consumers. When true, the Collector clones the data before passing it to prevent unintended side effects across parallel consumers.`,
    difficulty: "recall",
  },
  {
    id: "06-Q14",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In a large enterprise Java environment, one team uses the official OpenTelemetry SDK while another team uses an alternative processing engine that implements the OpenTelemetry Tracing API. Both teams share instrumented libraries. Is this a valid use of OpenTelemetry?`,
    options: [
      { key: "A", text: `No, the specification requires all teams to use the same SDK implementation` },
      { key: "B", text: `Yes, the API is designed to be implemented by any compatible SDK` },
      { key: "C", text: `Only if both SDKs export data in OTLP format` },
      { key: "D", text: `Only if the alternative SDK has been certified by the CNCF` },
    ],
    correctAnswer: "B",
    explanation: `The API/SDK separation explicitly supports this use case. The API defines a contract for instrumentation, and any SDK that correctly implements the API's provider interfaces is a valid choice. This interchangeability is a core design goal of OpenTelemetry that prevents lock-in at the SDK level, allowing different teams or frameworks to choose different SDK implementations while sharing the same instrumented libraries.`,
    difficulty: "scenario",
  },
  {
    id: "06-Q15",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A Collector component's stability level is listed as "Unmaintained." What does this mean according to the OpenTelemetry project?`,
    options: [
      { key: "A", text: `The component has been removed from all distributions` },
      { key: "B", text: `The component has a known security vulnerability and should not be used` },
      { key: "C", text: `The project is actively seeking new contributors, and the component will be deprecated after 3 months if none volunteer` },
      { key: "D", text: `The component works but will never receive any bug fixes or updates` },
    ],
    correctAnswer: "C",
    explanation: `The Unmaintained stability level signals that the original maintainers have stepped away and the project is actively seeking replacements. If no new contributors volunteer within 3 months, the component moves to Deprecated status and is eventually removed. This lifecycle ensures components do not remain in limbo indefinitely without active ownership.`,
    difficulty: "edge-case",
  },
  {
    id: "06-Q16",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A platform team wants to add a custom processor and a proprietary exporter to their Collector deployment without modifying the upstream Collector source code. What is the recommended approach?`,
    options: [
      { key: "A", text: `Fork the opentelemetry-collector-contrib repository and add the components directly` },
      { key: "B", text: `Use environment variables to dynamically load plugin modules at runtime` },
      { key: "C", text: `Submit the components to the contrib repository through the standard contribution process` },
      { key: "D", text: `Use the OpenTelemetry Collector Builder to create a custom distribution that includes their components` },
    ],
    correctAnswer: "D",
    explanation: `The OpenTelemetry Collector Builder (ocb) creates custom Collector distributions by combining selected components from core, contrib, and external Go modules into a single binary. This approach does not require modifying or forking upstream repositories, supports version pinning, and produces a smaller, more secure binary than the full contrib distribution.`,
    difficulty: "scenario",
  },
  {
    id: "07-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the purpose of the \`OTEL_SERVICE_NAME\` environment variable?`,
    options: [
      { key: "A", text: `It sets the gRPC service name used for load balancing` },
      { key: "B", text: `It defines the \`service.name\` resource attribute for the SDK` },
      { key: "C", text: `It configures the service name for the Collector's health check endpoint` },
      { key: "D", text: `It sets the span name prefix for all spans created by the SDK` },
    ],
    correctAnswer: "B",
    explanation: `\`OTEL_SERVICE_NAME\` is a convenience environment variable that sets the \`service.name\` resource attribute on the SDK. This is the single most important resource attribute, as backends use it to group and identify telemetry from a service. It takes precedence over \`service.name\` set via \`OTEL_RESOURCE_ATTRIBUTES\`.`,
    difficulty: "recall",
  },
  {
    id: "07-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which environment variable configures the sampling strategy for traces?`,
    options: [
      { key: "A", text: `OTEL_SAMPLING_STRATEGY` },
      { key: "B", text: `OTEL_TRACE_SAMPLER` },
      { key: "C", text: `OTEL_SPAN_SAMPLER` },
      { key: "D", text: `OTEL_TRACES_SAMPLER` },
    ],
    correctAnswer: "D",
    explanation: `\`OTEL_TRACES_SAMPLER\` sets the sampler used by the SDK's TracerProvider. Valid values include \`always_on\`, \`always_off\`, \`traceidratio\`, \`parentbased_always_on\`, \`parentbased_always_off\`, and \`parentbased_traceidratio\`. The companion variable \`OTEL_TRACES_SAMPLER_ARG\` provides the sampler's parameter (e.g., the ratio).`,
    difficulty: "recall",
  },
  {
    id: "07-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A development team needs to send traces to two OTLP endpoints with different authentication headers: a local Collector for debugging and a cloud backend for long-term storage. They currently use environment variables for SDK configuration. Why might they switch to file-based SDK configuration?`,
    options: [
      { key: "A", text: `File-based configuration supports multiple exporter instances with independent settings per signal` },
      { key: "B", text: `Environment variables cannot configure OTLP endpoints` },
      { key: "C", text: `File-based configuration is required for the batch processor to work` },
      { key: "D", text: `Environment variables are deprecated in the latest OpenTelemetry specification` },
    ],
    correctAnswer: "A",
    explanation: `While environment variables can configure a single OTLP exporter, they cannot express multiple exporter instances of the same type with different endpoints and headers. File-based YAML configuration supports declaring multiple exporters per signal, each with independent configuration (endpoints, headers, TLS settings). This is one of the key motivations for the file-based configuration specification.`,
    difficulty: "scenario",
  },
  {
    id: "07-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the correct precedence order for OpenTelemetry SDK configuration sources, from highest to lowest priority?`,
    options: [
      { key: "A", text: `File-based configuration > environment variables > programmatic` },
      { key: "B", text: `Environment variables > file-based configuration > programmatic` },
      { key: "C", text: `Programmatic > environment variables > file-based configuration` },
      { key: "D", text: `Environment variables > programmatic > file-based configuration` },
    ],
    correctAnswer: "C",
    explanation: `The OpenTelemetry specification defines a clear precedence: programmatic (code-level) configuration has the highest priority, followed by environment variables, and then file-based (declarative) configuration. This allows developers to override default file configurations with environment variables at deployment time, and override everything with explicit code when needed.`,
    difficulty: "recall",
  },
  {
    id: "07-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team sets \`OTEL_EXPORTER_OTLP_ENDPOINT=http://collector.internal:4317\` and \`OTEL_EXPORTER_OTLP_PROTOCOL=http/protobuf\`. The Collector's OTLP receiver listens on gRPC (port 4317) and HTTP (port 4318). What happens?`,
    options: [
      { key: "A", text: `The export fails because the SDK sends HTTP requests to the gRPC-only port` },
      { key: "B", text: `Traces are exported successfully because OTLP auto-negotiates the protocol` },
      { key: "C", text: `The export succeeds but with degraded performance` },
      { key: "D", text: `The Collector automatically redirects the request to port 4318` },
    ],
    correctAnswer: "A",
    explanation: `The OTLP receiver listens on separate ports for gRPC (default 4317) and HTTP (default 4318). When the SDK is configured to use \`http/protobuf\` but the endpoint points to port 4317, it sends HTTP requests to a gRPC listener, which cannot parse them. The fix is to change the endpoint to port 4318 or switch the protocol to \`grpc\`.`,
    difficulty: "scenario",
  },
  {
    id: "07-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What does \`OTEL_EXPORTER_OTLP_PROTOCOL\` control?`,
    options: [
      { key: "A", text: `The encryption algorithm used for OTLP connections` },
      { key: "B", text: `Whether the exporter uses synchronous or asynchronous sending` },
      { key: "C", text: `The version of the OTLP specification to use` },
      { key: "D", text: `The transport protocol used by the OTLP exporter (e.g., grpc or http/protobuf)` },
    ],
    correctAnswer: "D",
    explanation: `\`OTEL_EXPORTER_OTLP_PROTOCOL\` specifies the transport protocol for OTLP exports. The supported values are \`grpc\`, \`http/protobuf\`, and \`http/json\`. The default varies by language SDK (e.g., \`grpc\` in Go, \`http/protobuf\` in Java). Signal-specific variants like \`OTEL_EXPORTER_OTLP_TRACES_PROTOCOL\` can override this per signal.`,
    difficulty: "recall",
  },
  {
    id: "07-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A developer sets both \`OTEL_RESOURCE_ATTRIBUTES=service.name=api-gateway\` and \`OTEL_SERVICE_NAME=payment-service\`. Which value does the SDK use for \`service.name\`?`,
    options: [
      { key: "A", text: `api-gateway, because OTEL_RESOURCE_ATTRIBUTES is more specific` },
      { key: "B", text: `payment-service, because OTEL_SERVICE_NAME takes precedence` },
      { key: "C", text: `The SDK raises an error due to conflicting values` },
      { key: "D", text: `Both values are stored, creating a multi-valued attribute` },
    ],
    correctAnswer: "B",
    explanation: `The OpenTelemetry specification states that \`OTEL_SERVICE_NAME\` takes precedence over \`service.name\` set via \`OTEL_RESOURCE_ATTRIBUTES\`. This is by design: \`OTEL_SERVICE_NAME\` is a convenience shorthand specifically for the most critical resource attribute, and it always wins when both are specified.`,
    difficulty: "edge-case",
  },
  {
    id: "07-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A platform team maintains a shared SDK initialization library used by 30 microservices. Each service has a unique name and occasionally needs different sampling rates. Which configuration approach best supports this?`,
    options: [
      { key: "A", text: `Hardcode the service name and sampling rate in the shared library` },
      { key: "B", text: `Require each service to write its own SDK initialization code` },
      { key: "C", text: `Use file-based SDK configuration with environment variable substitution for service-specific values` },
      { key: "D", text: `Use a single configuration file deployed identically to all services` },
    ],
    correctAnswer: "C",
    explanation: `File-based SDK configuration supports environment variable substitution (e.g., \`\${SERVICE_NAME}\`, \`\${SAMPLING_RATE}\`), allowing a shared YAML template where each deployment injects its own values. This combines the consistency of a shared library with per-service customization, without requiring code changes or separate configuration files for each service.`,
    difficulty: "scenario",
  },
  {
    id: "07-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An e-commerce platform deploys services across development, staging, and production environments. They want the same application binary to export traces to different backends per environment without recompiling. What is the recommended approach?`,
    options: [
      { key: "A", text: `Use a YAML configuration file with environment variable substitution for endpoint values` },
      { key: "B", text: `Maintain separate source code branches for each environment` },
      { key: "C", text: `Use conditional compilation flags to switch endpoints at build time` },
      { key: "D", text: `Embed all three endpoints and select at startup via a command-line flag` },
    ],
    correctAnswer: "A",
    explanation: `File-based SDK configuration with environment variable substitution (e.g., \`endpoint: \${OTEL_EXPORTER_ENDPOINT}\`) allows the same binary and configuration template to be used across environments. Each environment sets the appropriate environment variables at deployment time. This approach, recommended by the OpenTelemetry project, avoids recompilation and supports the same configuration syntax across Go, Java, and JavaScript SDKs.`,
    difficulty: "scenario",
  },
  {
    id: "07-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What happens when an application initializes the OpenTelemetry SDK with a batch span processor but does NOT call the shutdown function before the process exits?`,
    options: [
      { key: "A", text: `The SDK automatically detects the process exit and flushes all data` },
      { key: "B", text: `The backend receives all data but with corrupted timestamps` },
      { key: "C", text: `The operating system sends a SIGTERM that triggers an automatic flush` },
      { key: "D", text: `Buffered telemetry data in the batch processor may be lost` },
    ],
    correctAnswer: "D",
    explanation: `The batch span processor buffers spans in memory and exports them periodically or when the batch reaches a configured size. If the application exits without calling \`Shutdown()\`, any spans still in the buffer are lost. Graceful shutdown is essential: it triggers a final flush of all buffered data before the process terminates. This is why SDK setup guides always emphasize deferring the shutdown call.`,
    difficulty: "edge-case",
  },
  {
    id: "07-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An operations team wants to reduce the sampling rate of their production application from 100% to 10% without redeploying or changing code. They use file-based SDK configuration. What is the most practical approach?`,
    options: [
      { key: "A", text: `Modify the YAML configuration file and restart the application` },
      { key: "B", text: `The SDK watches the configuration file for changes and applies them automatically` },
      { key: "C", text: `Set \`OTEL_TRACES_SAMPLER=traceidratio\` and \`OTEL_TRACES_SAMPLER_ARG=0.1\` as environment variables and restart` },
      { key: "D", text: `Use the OpAMP protocol to push the new sampling rate without a restart` },
    ],
    correctAnswer: "C",
    explanation: `Currently, the OpenTelemetry SDK specification does not support configuration hot-reload. Changing a file-based configuration requires an application restart. However, environment variables take precedence over file-based configuration, so the team can override the sampling rate by setting \`OTEL_TRACES_SAMPLER\` and \`OTEL_TRACES_SAMPLER_ARG\` as environment variables and restarting. This avoids modifying the base configuration file. Hot-reload via OpAMP is expected in the future but is not yet part of the SDK specification.`,
    difficulty: "scenario",
  },
  {
    id: "07-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the default value of \`OTEL_TRACES_SAMPLER\` if it is not explicitly set?`,
    options: [
      { key: "A", text: `always_on` },
      { key: "B", text: `always_off` },
      { key: "C", text: `traceidratio` },
      { key: "D", text: `parentbased_always_on` },
    ],
    correctAnswer: "D",
    explanation: `The OpenTelemetry specification defines \`parentbased_always_on\` as the default sampler. This means root spans are always sampled, and child spans follow their parent's sampling decision. This default ensures complete traces while allowing upstream services to control sampling decisions through context propagation.`,
    difficulty: "recall",
  },
  {
    id: "07-Q13",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In the Collector's YAML configuration, a team writes \`endpoint: \${BACKEND_ENDPOINT}\` but forgets to set the \`BACKEND_ENDPOINT\` environment variable. What happens when the Collector starts?`,
    options: [
      { key: "A", text: `The Collector refuses to start with a validation error` },
      { key: "B", text: `The variable resolves to an empty string, and the Collector logs a warning` },
      { key: "C", text: `The Collector uses a built-in default endpoint` },
      { key: "D", text: `The \`\${VAR:?error}\` syntax automatically catches the missing variable` },
    ],
    correctAnswer: "B",
    explanation: `When an environment variable referenced in the Collector configuration is unset and has no default value (via the \`\${VAR:-default}\` syntax), the env provider returns an empty string and logs a warning: "Configuration references unset environment variable." The \`\${VAR:?error}\` syntax for required variables is NOT supported by the Collector's env provider. To enforce required variables, teams should validate externally before starting the Collector.`,
    difficulty: "edge-case",
  },
  {
    id: "07-Q14",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A fintech company discovers that their services report traces without a \`service.name\` resource attribute, making backend filtering impossible. The SDK is configured programmatically, but the developer omitted resource attributes. Which environment variable provides the quickest fix without a code change?`,
    options: [
      { key: "A", text: `OTEL_PROPAGATORS` },
      { key: "B", text: `OTEL_EXPORTER_OTLP_ENDPOINT` },
      { key: "C", text: `OTEL_SERVICE_NAME` },
      { key: "D", text: `OTEL_TRACES_EXPORTER` },
    ],
    correctAnswer: "C",
    explanation: `\`OTEL_SERVICE_NAME\` sets the \`service.name\` resource attribute without requiring any code changes. It is the fastest way to fix a missing service name in production. Because environment variables take precedence over file-based configuration, and \`OTEL_SERVICE_NAME\` specifically overrides \`service.name\` from \`OTEL_RESOURCE_ATTRIBUTES\`, deploying the variable is an immediate, non-code fix.`,
    difficulty: "scenario",
  },
  {
    id: "07-Q15",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which environment variable allows setting multiple resource attributes in a single value?`,
    options: [
      { key: "A", text: `OTEL_RESOURCE_ATTRIBUTES` },
      { key: "B", text: `OTEL_SERVICE_ATTRIBUTES` },
      { key: "C", text: `OTEL_RESOURCE_LABELS` },
      { key: "D", text: `OTEL_SPAN_ATTRIBUTES` },
    ],
    correctAnswer: "A",
    explanation: `\`OTEL_RESOURCE_ATTRIBUTES\` accepts a comma-separated list of key=value pairs (e.g., \`service.version=1.2.0,deployment.environment.name=production\`). This is the standard way to set multiple resource attributes via environment variables. Only \`OTEL_SERVICE_NAME\` and \`OTEL_RESOURCE_ATTRIBUTES\` are specified for resource configuration; the other options are not valid OpenTelemetry environment variables.`,
    difficulty: "recall",
  },
  {
    id: "08-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An API gateway receives HTTP requests from external clients and routes them to internal services. What SpanKind should the gateway use for its incoming request span?`,
    options: [
      { key: "A", text: `SERVER` },
      { key: "B", text: `CLIENT` },
      { key: "C", text: `PRODUCER` },
      { key: "D", text: `INTERNAL` },
    ],
    correctAnswer: "A",
    explanation: `SERVER is the correct SpanKind for spans representing inbound request/response operations, such as an HTTP handler processing incoming requests. CLIENT is for outgoing requests, PRODUCER for asynchronous fire-and-forget outgoing messages, and INTERNAL for in-process operations with no remote relationship.`,
    difficulty: "recall",
  },
  {
    id: "08-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An application receives a batch of 50 messages from a queue and processes them in a single operation. The team wants to relate the processing span to each original message's trace. What should they use?`,
    options: [
      { key: "A", text: `Create 50 child spans, one per message` },
      { key: "B", text: `Set parent_span_id to the first message's span ID` },
      { key: "C", text: `Add span links from the processing span to each message's span context` },
      { key: "D", text: `Add span events for each message with the trace ID as an attribute` },
    ],
    correctAnswer: "C",
    explanation: `Span links represent causal relationships that are not parent-child. Batch processing is a classic use case: a single processing span can link to multiple spans from different traces that contributed to the batch. Creating 50 child spans would create unnecessary overhead, and a span can only have one parent via parent_span_id.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which set of span status codes is defined in OpenTelemetry?`,
    options: [
      { key: "A", text: `Success, Failure, Unknown` },
      { key: "B", text: `Ok, Warning, Error, Critical` },
      { key: "C", text: `None, Success, Error, Timeout` },
      { key: "D", text: `Unset, Ok, Error` },
    ],
    correctAnswer: "D",
    explanation: `OpenTelemetry defines exactly three status codes: Unset (the default, letting analysis tools decide), Ok (explicitly marking success), and Error (indicating the operation failed). There is no Warning, Critical, or Timeout status code in the specification.`,
    difficulty: "recall",
  },
  {
    id: "08-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A developer calls \`span.SetStatus(codes.Error, "timeout")\` early in a span's lifecycle. Later, after a successful retry, they call \`span.SetStatus(codes.Ok, "")\`. What is the final span status?`,
    options: [
      { key: "A", text: `Error, because it was set first and cannot be changed` },
      { key: "B", text: `Ok, because Ok is allowed to override Error and becomes immutable once set` },
      { key: "C", text: `Unset, because conflicting statuses reset to the default` },
      { key: "D", text: `The SDK raises an exception for conflicting status updates` },
    ],
    correctAnswer: "B",
    explanation: `In OpenTelemetry, a span status can transition from Error to Ok. Once Ok is set, the status becomes immutable and cannot be changed further. This is why Ok is described as "explicitly mark success (suppresses errors)." The allowed transitions are: Unset to Error, Unset to Ok, and Error to Ok.`,
    difficulty: "edge-case",
  },
  {
    id: "08-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A service publishes order events to a message queue for asynchronous processing by downstream consumers. Which SpanKind pair correctly represents the publishing and consuming operations?`,
    options: [
      { key: "A", text: `CLIENT for publishing and SERVER for consuming` },
      { key: "B", text: `INTERNAL for both publishing and consuming` },
      { key: "C", text: `CLIENT for publishing and CONSUMER for consuming` },
      { key: "D", text: `PRODUCER for publishing and CONSUMER for consuming` },
    ],
    correctAnswer: "D",
    explanation: `PRODUCER is for outgoing asynchronous (fire-and-forget) operations like publishing to a message queue. CONSUMER is for incoming asynchronous operations like processing queue messages. CLIENT/SERVER are reserved for synchronous request/response patterns like HTTP and gRPC calls.`,
    difficulty: "recall",
  },
  {
    id: "08-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A developer writes a shared library using the OpenTelemetry API to create spans. The library is used by two applications: one has an SDK configured, the other does not. What happens in the application without an SDK?`,
    options: [
      { key: "A", text: `The library throws an error at startup because no SDK is registered` },
      { key: "B", text: `Span creation succeeds but calling SetAttributes causes a panic` },
      { key: "C", text: `The API returns non-recording spans that silently accept all operations` },
      { key: "D", text: `Spans are buffered in memory until an SDK is eventually registered` },
    ],
    correctAnswer: "C",
    explanation: `Without an SDK installed, the OpenTelemetry API provides a no-op implementation that returns non-recording spans. All operations (SetAttributes, AddEvent, SetStatus, End) silently succeed with no performance overhead. This design enables library authors to add instrumentation without forcing users to install an SDK. Context propagation (SpanContext) still works even without an SDK.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the primary purpose of span events in OpenTelemetry?`,
    options: [
      { key: "A", text: `To record timestamped annotations within a span's lifetime` },
      { key: "B", text: `To create child spans without starting a new trace operation` },
      { key: "C", text: `To propagate context information across service boundaries` },
      { key: "D", text: `To aggregate span data into metrics for monitoring dashboards` },
    ],
    correctAnswer: "A",
    explanation: `Span events are timestamped annotations that record notable occurrences during a span's lifetime. Common use cases include recording exceptions (via RecordError, which creates an event named "exception"), state changes, cache misses, and business milestones. Each event carries a name, a timestamp, and optional attributes.`,
    difficulty: "recall",
  },
  {
    id: "08-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team wants to reduce trace storage costs but must ensure all traces containing errors are captured. Which sampling approach best meets this requirement?`,
    options: [
      { key: "A", text: `Head-based AlwaysOn sampling with a storage quota at the backend` },
      { key: "B", text: `Head-based TraceIdRatio sampling at 10%` },
      { key: "C", text: `ParentBased sampling with AlwaysOff as the root sampler` },
      { key: "D", text: `Tail-based sampling in the Collector with error-aware policies` },
    ],
    correctAnswer: "D",
    explanation: `Tail-based sampling makes decisions after collecting all spans for a trace, enabling content-aware policies that always sample error traces while dropping successful ones at a configured rate. Head-based sampling (AlwaysOn, TraceIdRatio, ParentBased) makes decisions at trace creation time, before errors occur, so it cannot guarantee error capture.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A span's SpanContext has \`IsRemote()\` returning true. What does this indicate?`,
    options: [
      { key: "A", text: `The span has been exported to a remote observability backend` },
      { key: "B", text: `The SpanContext was received from a different process via context propagation` },
      { key: "C", text: `The span was created using a remote procedure call such as gRPC` },
      { key: "D", text: `The span's trace data is replicated across multiple Collector instances` },
    ],
    correctAnswer: "B",
    explanation: `IsRemote() returns true when the SpanContext was extracted from an incoming request, typically from HTTP headers or gRPC metadata propagated by another service. It distinguishes locally-created spans from those whose context was received from a remote process. This information is used by samplers like ParentBased to apply different sampling rules for remote versus local parents.`,
    difficulty: "edge-case",
  },
  {
    id: "08-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An organization wants to sample 25% of traces using a deterministic approach that ensures the same trace is either fully sampled or fully dropped across all services. Which sampler should they configure in each service's SDK?`,
    options: [
      { key: "A", text: `TraceIdRatio with a ratio of 0.25` },
      { key: "B", text: `AlwaysOn with a post-processing filter that drops 75%` },
      { key: "C", text: `A custom random sampler that independently drops 75% of spans` },
      { key: "D", text: `ParentBased with AlwaysOff as the root sampler and no remote parent configuration` },
    ],
    correctAnswer: "A",
    explanation: `TraceIdRatio makes deterministic sampling decisions based on a hash of the trace ID. Because all services participating in the same trace see the same trace ID, they independently arrive at the same sampling decision. This ensures complete traces are either fully captured or fully dropped, preventing partial traces.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `When a developer creates a span using \`tracer.Start(ctx, "process order")\` without specifying a SpanKind option, which kind is assigned by default?`,
    options: [
      { key: "A", text: `SERVER` },
      { key: "B", text: `UNSPECIFIED` },
      { key: "C", text: `INTERNAL` },
      { key: "D", text: `No SpanKind is assigned until explicitly set later` },
    ],
    correctAnswer: "C",
    explanation: `When no SpanKind is explicitly specified via WithSpanKind, the default is INTERNAL. This represents in-process operations with no incoming or outgoing remote relationship. Developers should explicitly set the appropriate kind (SERVER, CLIENT, PRODUCER, CONSUMER) when the span involves remote communication.`,
    difficulty: "recall",
  },
  {
    id: "08-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A function creates a span and passes the resulting context to a helper function, which also creates a span using that context. What is the relationship between these two spans?`,
    options: [
      { key: "A", text: `They are sibling spans sharing the same parent` },
      { key: "B", text: `The helper's span is automatically a child of the outer function's span` },
      { key: "C", text: `They share the same span ID but have different parent IDs` },
      { key: "D", text: `The helper's span replaces the outer function's span in the context` },
    ],
    correctAnswer: "B",
    explanation: `When tracer.Start() receives a context containing an active span, the new span automatically becomes a child of that active span. The parent span's trace_id is inherited and its span_id becomes the child's parent_span_id, establishing the parent-child hierarchy through context propagation.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q13",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What does \`span.RecordError(err)\` do in OpenTelemetry?`,
    options: [
      { key: "A", text: `Sets the span status to Error and ends the span immediately` },
      { key: "B", text: `Propagates the error to the parent span via the context` },
      { key: "C", text: `Logs the error to the application's standard logging framework` },
      { key: "D", text: `Creates a span event named "exception" with exception.type, exception.message, and exception.stacktrace attributes` },
    ],
    correctAnswer: "D",
    explanation: `RecordError is a convenience method that creates a span event with the name "exception" and standard exception semantic convention attributes. It does NOT automatically set the span status to Error or end the span. Developers must separately call span.SetStatus(codes.Error, msg) to mark the span as failed. This separation allows recording errors while still potentially marking the span as successful if the error is handled.`,
    difficulty: "edge-case",
  },
  {
    id: "08-Q14",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A service uses ParentBased sampling and receives a request with a trace context where the sampled flag is not set. What happens when the service creates a new span for this request?`,
    options: [
      { key: "A", text: `The service samples the span using its local TraceIdRatio configuration` },
      { key: "B", text: `The service starts a new independent trace, ignoring the unsampled parent` },
      { key: "C", text: `The service creates a non-recording span, honoring the parent's unsampled decision` },
      { key: "D", text: `The service records the span but marks it with a "deferred" sampling flag` },
    ],
    correctAnswer: "C",
    explanation: `ParentBased sampling inherits the parent's sampling decision. When the incoming context has the sampled flag unset, the service creates a non-recording span that accepts all API calls (SetAttributes, AddEvent) as no-ops. This ensures consistent sampling decisions across the distributed trace, preventing fragmented traces.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q15",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What does the AlwaysOn sampler do?`,
    options: [
      { key: "A", text: `Records all spans and marks every one for export` },
      { key: "B", text: `Samples every span but defers the export decision to the Collector` },
      { key: "C", text: `Samples only root spans and propagates the decision to child spans` },
      { key: "D", text: `Records spans in a ring buffer and exports only when the buffer is full` },
    ],
    correctAnswer: "A",
    explanation: `The AlwaysOn sampler creates recording spans for every operation, meaning all spans are captured and exported. It is commonly used during development or in low-traffic services. In production high-traffic environments, it is typically replaced by TraceIdRatio or ParentBased samplers to manage costs.`,
    difficulty: "recall",
  },
  {
    id: "08-Q16",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team at an e-commerce platform discovers their service creates over 30 spans per request by instrumenting every internal step ("validate order", "check inventory", "calculate tax", "apply discount"), each taking under 1ms. Following instrumentation best practices, what should they do?`,
    options: [
      { key: "A", text: `Increase the sampling rate so that fewer complete traces are stored` },
      { key: "B", text: `Convert the internal step spans into span events on the parent boundary span` },
      { key: "C", text: `Set all internal spans to SpanKind INTERNAL so backends can auto-filter them` },
      { key: "D", text: `Move the internal span creation into a separate TracerProvider with its own exporter` },
    ],
    correctAnswer: "B",
    explanation: `When operations are quick, in-process, and sequential within a single service, span events on a parent boundary span are preferred over individual child spans. Events preserve timestamps and relevant metadata while significantly reducing trace complexity, SDK overhead, and storage costs. This pattern, converting excessive internal spans to events, is a core instrumentation best practice.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q17",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What are the two primary identifiers contained in a SpanContext?`,
    options: [
      { key: "A", text: `ServiceName and OperationName` },
      { key: "B", text: `SpanID and ParentSpanID` },
      { key: "C", text: `TraceID and SpanID` },
      { key: "D", text: `TraceID and TraceFlags` },
    ],
    correctAnswer: "C",
    explanation: `A SpanContext carries a TraceID (16 bytes, represented as 32 hex characters) identifying the entire distributed trace and a SpanID (8 bytes, represented as 16 hex characters) identifying the specific span. Together they form a globally unique reference. While TraceFlags and TraceState are also part of the SpanContext, the primary identifiers for correlation and assembly are TraceID and SpanID.`,
    difficulty: "recall",
  },
  {
    id: "08-Q18",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `When should a developer explicitly set span status to Ok instead of leaving it as the default Unset?`,
    options: [
      { key: "A", text: `After every successful database query to confirm correctness` },
      { key: "B", text: `Before ending every span as a standard best practice` },
      { key: "C", text: `Whenever the span represents an HTTP 200 response` },
      { key: "D", text: `Only when they need to override a previously set Error status or explicitly suppress potential errors` },
    ],
    correctAnswer: "D",
    explanation: `Unset is the default span status and is preferred for successful operations, allowing backend analysis tools to make their own determination. Ok should only be used when the developer wants to explicitly mark success and prevent any previously-set or future Error status from taking effect. Once set to Ok, the status becomes immutable. Routinely setting Ok on every span is unnecessary and defeats the purpose of the Unset default.`,
    difficulty: "edge-case",
  },
  {
    id: "08-Q19",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A service processes an HTTP request and within the handler needs to start an independent trace for an asynchronous background task. The developer wants to preserve request context values (such as deadlines and auth info) but not the trace parent. Which approach is correct?`,
    options: [
      { key: "A", text: `Use tracer.Start() with the WithNewRoot() option on the existing context` },
      { key: "B", text: `Create a new SpanContext with a manually-generated TraceID and inject it into a fresh context` },
      { key: "C", text: `Pass context.Background() to tracer.Start() to avoid inheriting the parent` },
      { key: "D", text: `Set the new span's parent_span_id to an empty byte array after span creation` },
    ],
    correctAnswer: "A",
    explanation: `WithNewRoot() creates a new root span (starting a new trace) while preserving all non-trace values in the existing context, such as deadlines, cancellation signals, and authentication data. Using context.Background() would also start a new trace but would lose all other context values. WithNewRoot() is the idiomatic and explicit approach for this use case.`,
    difficulty: "scenario",
  },
  {
    id: "08-Q20",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A Go helper function retrieves a span from the context using \`trace.SpanFromContext(ctx)\`, then calls \`span.RecordError(err)\` and \`span.SetStatus(codes.Error, err.Error())\` before returning the error. The calling function, which created the span, also calls RecordError and SetStatus when it receives the error. What is wrong with this pattern?`,
    options: [
      { key: "A", text: `RecordError can only be called once per span lifetime` },
      { key: "B", text: `The helper should not record errors or set status on a span it did not create` },
      { key: "C", text: `SetStatus must always be called before RecordError, not after` },
      { key: "D", text: `SpanFromContext returns a copy of the span, so the helper's changes have no effect` },
    ],
    correctAnswer: "B",
    explanation: `The error handling golden rule in OpenTelemetry is: the span creator records errors. If a function retrieves a span from context (it did not create the span), it should simply return the error and let the span owner handle error recording. When both the helper and the creator record the same error, it results in duplicate "exception" events and redundant status changes on the span.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which OpenTelemetry metric instrument is designed for values that can both increase and decrease synchronously, such as the number of items in a cache?`,
    options: [
      { key: "A", text: `Counter` },
      { key: "B", text: `Histogram` },
      { key: "C", text: `UpDownCounter` },
      { key: "D", text: `ObservableGauge` },
    ],
    correctAnswer: "C",
    explanation: `The UpDownCounter is a synchronous, additive instrument for values that can increase or decrease. Unlike Counter (which is monotonic and only increases) or ObservableGauge (which is asynchronous and non-additive), UpDownCounter is the correct choice for tracking quantities like queue depth, cache size, or active connections where the value changes in both directions during normal operation.`,
    difficulty: "recall",
  },
  {
    id: "09-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An SRE team monitors active HTTP connections to their API gateway. The count goes up when new connections open and down when they close. The team records changes inline in the connection handler code. Which instrument should they use?`,
    options: [
      { key: "A", text: `ObservableCounter` },
      { key: "B", text: `UpDownCounter` },
      { key: "C", text: `Histogram` },
      { key: "D", text: `Counter` },
    ],
    correctAnswer: "B",
    explanation: `Active connections are a synchronous, non-monotonic, additive value. The UpDownCounter is the right choice because the application adds +1 when a connection opens and -1 when it closes. A Counter cannot decrease. An ObservableCounter is both monotonic and asynchronous. A Histogram records distributions, not current counts.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What distinguishes asynchronous (observable) metric instruments from synchronous ones in OpenTelemetry?`,
    options: [
      { key: "A", text: `Asynchronous instruments use callbacks invoked once per collection cycle, rather than being called inline in business logic` },
      { key: "B", text: `Asynchronous instruments can only record integer values, while synchronous instruments support both integers and floats` },
      { key: "C", text: `Asynchronous instruments are automatically thread-safe, while synchronous instruments require manual locking` },
      { key: "D", text: `Asynchronous instruments send data immediately to the backend, while synchronous ones batch data` },
    ],
    correctAnswer: "A",
    explanation: `Asynchronous (observable) instruments register a callback function that the SDK invokes once per collection interval. This is ideal for reading values from external sources (OS stats, runtime metrics) where the measurement is "pulled" rather than "pushed." Synchronous instruments are called directly in application code when a measurement occurs. Both types support integer and float values, and both are thread-safe by design.`,
    difficulty: "recall",
  },
  {
    id: "09-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A platform team needs to report total CPU time consumed by a long-running process. The value is read from the operating system and only increases over the lifetime of the process. Which instrument is most appropriate?`,
    options: [
      { key: "A", text: `Gauge` },
      { key: "B", text: `Counter` },
      { key: "C", text: `UpDownCounter` },
      { key: "D", text: `ObservableCounter` },
    ],
    correctAnswer: "D",
    explanation: `CPU time is a monotonically increasing value read from an external source (the operating system), not recorded inline in business logic. The ObservableCounter is designed for exactly this pattern: it uses a callback to observe monotonic values from external sources at each collection cycle. A regular Counter would require the application to compute deltas and add them synchronously, which is less natural for externally-tracked cumulative values.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A histogram metric has explicit_bounds set to [10, 25, 50, 100]. How many elements does the bucket_counts array contain?`,
    options: [
      { key: "A", text: `3` },
      { key: "B", text: `4` },
      { key: "C", text: `5` },
      { key: "D", text: `8` },
    ],
    correctAnswer: "C",
    explanation: `A histogram with N boundary values always has N+1 bucket counts. The boundaries [10, 25, 50, 100] define 5 buckets: (-inf, 10], (10, 25], (25, 50], (50, 100], and (100, +inf). The extra bucket catches all values above the highest boundary. This is a common source of off-by-one errors when working with histogram data.`,
    difficulty: "edge-case",
  },
  {
    id: "09-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the key difference between delta and cumulative aggregation temporality for metrics?`,
    options: [
      { key: "A", text: `Delta reports only the change since the last collection, while cumulative reports the total value since the process started` },
      { key: "B", text: `Delta is used for counters and cumulative is used for gauges` },
      { key: "C", text: `Delta requires more memory on the client side because it must store historical values` },
      { key: "D", text: `Cumulative temporality is not supported by the OTLP protocol` },
    ],
    correctAnswer: "A",
    explanation: `Delta temporality transmits only the difference since the previous report, making payloads smaller but requiring synchronization between client and server. Cumulative temporality transmits the running total since process start, making it resilient to missed collection cycles (as used by Prometheus). Both temporalities are fully supported by OTLP. The choice depends on the backend's expectations and the desired trade-offs around statefulness.`,
    difficulty: "recall",
  },
  {
    id: "09-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An operations team adds three metric attributes to their \`http.server.request.count\` counter: \`http.method\` (5 values), \`http.route\` (20 routes), and \`user.id\` (100,000 unique users). Approximately how many time series can this metric produce?`,
    options: [
      { key: "A", text: `100,025` },
      { key: "B", text: `10,000,000` },
      { key: "C", text: `100,000` },
      { key: "D", text: `25` },
    ],
    correctAnswer: "B",
    explanation: `Each unique combination of attribute values creates a separate time series. The total is the product of all attribute cardinalities: 5 x 20 x 100,000 = 10,000,000 time series. This is a cardinality explosion caused by including user.id as a metric dimension. High-cardinality attributes like user.id belong on spans, where they are handled per-request rather than creating persistent time series. Removing user.id reduces the count to just 5 x 20 = 100 time series.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `According to OpenTelemetry naming conventions, which of the following is the correct metric name for tracking HTTP server request duration?`,
    options: [
      { key: "A", text: `myservice.http.server.request.duration` },
      { key: "B", text: `HTTP_SERVER_REQUEST_DURATION` },
      { key: "C", text: `http.server.request.duration_seconds` },
      { key: "D", text: `http.server.request.duration` },
    ],
    correctAnswer: "D",
    explanation: `OpenTelemetry metric names follow the pattern {domain}.{component}.{property} and should never include service names, company names, or unit suffixes. Option A incorrectly embeds a service name. Option B uses uppercase and underscores, violating the lowercase dot-separated convention. Option C embeds the unit in the name; units belong in the dedicated unit field. Option D correctly uses the hierarchical dot-separated pattern without extraneous information.`,
    difficulty: "recall",
  },
  {
    id: "09-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team needs to track the current depth of a message processing queue. The value changes when messages arrive and are consumed. They want to observe it through a callback that reads the queue length from an internal data structure once per collection cycle. Which instrument should they use?`,
    options: [
      { key: "A", text: `ObservableUpDownCounter` },
      { key: "B", text: `ObservableGauge` },
      { key: "C", text: `UpDownCounter` },
      { key: "D", text: `ObservableCounter` },
    ],
    correctAnswer: "A",
    explanation: `Queue depth is a non-monotonic, additive value: summing queue depths across instances gives total queue depth, which is meaningful. Because the team wants to read it via a callback, an asynchronous instrument is needed. ObservableUpDownCounter fits because the value is additive. ObservableGauge would be incorrect because gauge values are non-additive, meaning summing them across instances has no clear interpretation. UpDownCounter is synchronous, and ObservableCounter is monotonic.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In the OTLP data model, a monotonic Counter metric is represented as which data type?`,
    options: [
      { key: "A", text: `Gauge` },
      { key: "B", text: `Histogram` },
      { key: "C", text: `Sum with is_monotonic set to true` },
      { key: "D", text: `ExponentialHistogram` },
    ],
    correctAnswer: "C",
    explanation: `In the OTLP wire format, there is no "Counter" type. Counter values are represented as a Sum data type with the is_monotonic field set to true and an aggregation_temporality of either DELTA or CUMULATIVE. The Sum type serves double duty: monotonic sums represent Counters, while non-monotonic sums represent UpDownCounters.`,
    difficulty: "recall",
  },
  {
    id: "09-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team needs to report heap allocation, garbage collection count, and goroutine count from the Go runtime. Reading runtime statistics is expensive. What is the recommended approach in OpenTelemetry?`,
    options: [
      { key: "A", text: `Create three separate ObservableGauges, each with its own callback that reads runtime stats independently` },
      { key: "B", text: `Use RegisterCallback to create a single callback that observes all three instruments from one runtime stats read` },
      { key: "C", text: `Create three synchronous Gauges and record values in a background goroutine on a timer` },
      { key: "D", text: `Use a single ObservableGauge with different attribute values for each measurement` },
    ],
    correctAnswer: "B",
    explanation: `When multiple metrics share an expensive data source, RegisterCallback allows registering a single callback that observes multiple instruments at once. This avoids calling the expensive operation (like reading runtime memory stats) multiple times per collection cycle. Creating separate callbacks would result in redundant expensive calls. Using a single instrument with attributes conflates unrelated measurements into one metric.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What happens when an application creates and uses metric instruments without installing an OpenTelemetry SDK?`,
    options: [
      { key: "A", text: `The application throws an exception when attempting to record measurements` },
      { key: "B", text: `Instrument creation fails with an error, but the application continues running` },
      { key: "C", text: `Measurements are buffered in memory until an SDK is installed` },
      { key: "D", text: `Instrument creation succeeds, recording operations silently succeed, and no data is collected` },
    ],
    correctAnswer: "D",
    explanation: `Without an SDK, the API returns no-op instrument implementations. Creating instruments succeeds, recording measurements silently does nothing, and asynchronous callbacks are never invoked. This design ensures libraries instrumented with OpenTelemetry can be used in applications that have not adopted OpenTelemetry without causing errors or performance overhead.`,
    difficulty: "edge-case",
  },
  {
    id: "09-Q13",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `According to OpenTelemetry semantic conventions, which metric name correctly follows the naming rules for an UpDownCounter tracking active process count?`,
    options: [
      { key: "A", text: `system.process.count` },
      { key: "B", text: `system.processes` },
      { key: "C", text: `system.process.count_total` },
      { key: "D", text: `system.active_processes` },
    ],
    correctAnswer: "A",
    explanation: `Semantic conventions follow a \`<namespace>.<noun>.<descriptor>\` structure. When an UpDownCounter tracks a count of things, the name should include a descriptor such as \`.count\`, giving \`system.process.count\` rather than the bare \`system.processes\`. The \`_total\` suffix should be avoided because it implies a monotonic sum, which is misleading for an UpDownCounter. Option D lacks the hierarchical dot-separated structure and the required descriptor.`,
    difficulty: "edge-case",
  },
  {
    id: "09-Q14",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A backend team exports cumulative metrics from their service. After a service restart, the monitoring dashboard shows a sudden spike in request rates that did not actually occur. What is the most likely cause?`,
    options: [
      { key: "A", text: `The backend is using the wrong query language for cumulative metrics` },
      { key: "B", text: `The exporter switched from cumulative to delta temporality after restart` },
      { key: "C", text: `The StartTimeUnixNano was not properly reset after the service restarted, causing the backend to misinterpret the new cumulative series` },
      { key: "D", text: `The batch processor in the Collector is duplicating data points` },
    ],
    correctAnswer: "C",
    explanation: `For cumulative metrics, StartTimeUnixNano indicates when the cumulative sum began. After a service restart, a new cumulative series starts from zero. If the start time is not properly set (or reuses the old start time), the backend calculates the rate using the wrong interval, producing a false spike. The OTLP specification requires including correct StartTimeUnixNano for cumulative metrics to enable accurate rate calculation.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q15",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What advantage does ExponentialHistogram offer over a standard Histogram with explicit bucket boundaries?`,
    options: [
      { key: "A", text: `ExponentialHistogram supports negative values while standard Histogram does not` },
      { key: "B", text: `ExponentialHistogram automatically adjusts bucket boundaries for wide value ranges, requiring no manual boundary configuration` },
      { key: "C", text: `ExponentialHistogram uses less network bandwidth because it omits bucket counts` },
      { key: "D", text: `ExponentialHistogram is the only histogram type that supports exemplars` },
    ],
    correctAnswer: "B",
    explanation: `ExponentialHistogram uses a mathematical formula (base^i to base^(i+1), where base = 2^(2^-scale)) to define bucket boundaries. This means buckets automatically cover a wide range of values with appropriate resolution, eliminating the need to manually choose explicit boundaries. Standard histograms require the user to specify boundaries, and poor choices can lead to loss of precision. Both histogram types support exemplars.`,
    difficulty: "recall",
  },
  {
    id: "09-Q16",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A fintech company notices their metrics storage costs have tripled in one month. Investigation reveals a recently deployed service records a \`payment.amount\` histogram with attributes for \`customer.org.id\` (500 organizations), \`payment.method\` (5 types), \`currency\` (3 currencies), and \`merchant.id\` (10,000 merchants). What is the most effective first step to reduce cardinality?`,
    options: [
      { key: "A", text: `Switch from Histogram to Counter to reduce storage per time series` },
      { key: "B", text: `Remove all attributes except payment.method` },
      { key: "C", text: `Increase the collection interval to reduce the frequency of data points` },
      { key: "D", text: `Move merchant.id to span attributes and remove it from the metric, since it is the highest-cardinality dimension` },
    ],
    correctAnswer: "D",
    explanation: `The total time series count is approximately 500 x 5 x 3 x 10,000 = 75,000,000. The merchant.id attribute contributes the most to the cardinality explosion (10,000 unique values). Removing it reduces the count to 500 x 5 x 3 = 7,500 series, a 10,000x reduction. High-cardinality attributes like merchant.id belong on spans, where they are handled per-request rather than creating persistent time series.`,
    difficulty: "scenario",
  },
  {
    id: "09-Q17",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `When creating a Histogram instrument for HTTP request duration, what does the WithExplicitBucketBoundaries option configure?`,
    options: [
      { key: "A", text: `The boundary values that define the histogram's bucket ranges for categorizing recorded measurements` },
      { key: "B", text: `The maximum number of data points the histogram can store before aggregation` },
      { key: "C", text: `The time interval between histogram collections` },
      { key: "D", text: `The minimum and maximum values the histogram will accept` },
    ],
    correctAnswer: "A",
    explanation: `WithExplicitBucketBoundaries is an advisory parameter that specifies the boundary values for histogram buckets. For example, boundaries [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10] create 12 buckets suitable for HTTP latency measurement in seconds. The SDK uses these boundaries to categorize recorded values into the appropriate bucket. Without this option, the SDK uses default bucket boundaries that may not be optimal for the specific use case.`,
    difficulty: "recall",
  },
  {
    id: "09-Q18",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `The Summary metric type appears in the OTLP data model with pre-calculated quantiles. Under what circumstances should new instrumentation use Summary?`,
    options: [
      { key: "A", text: `When exact percentiles are required rather than approximate ones from histograms` },
      { key: "B", text: `When the backend does not support histogram queries` },
      { key: "C", text: `Summary is not recommended for new instrumentation; Histogram or ExponentialHistogram should be used instead` },
      { key: "D", text: `When measuring non-numeric properties that cannot be bucketed` },
    ],
    correctAnswer: "C",
    explanation: `The OTLP specification explicitly states that Summary is not recommended for new instrumentation. It exists primarily for backward compatibility with Prometheus client libraries that generate pre-calculated quantiles. Histograms and ExponentialHistograms are preferred because they allow backends to calculate quantiles at query time, supporting re-aggregation across instances and time windows, which pre-calculated quantiles cannot do.`,
    difficulty: "edge-case",
  },
  {
    id: "10-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the primary use case for the OpenTelemetry Logs API?`,
    options: [
      { key: "A", text: `Provide a bridge to connect existing logging frameworks to OpenTelemetry` },
      { key: "B", text: `Replace all existing logging frameworks with a new logging library` },
      { key: "C", text: `Generate log files on disk for offline analysis` },
      { key: "D", text: `Send logs directly to cloud storage backends` },
    ],
    correctAnswer: "A",
    explanation: `The OpenTelemetry Logs API is designed primarily for log bridges that connect existing logging libraries (such as Log4j, slog, Zap, and logrus) to OpenTelemetry. Direct instrumentation through the API is possible but less common. The API does not replace existing frameworks; it integrates with them.`,
    difficulty: "recall",
  },
  {
    id: "10-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team wants to send logs from their Java application that uses Log4j to an OpenTelemetry backend. What is the recommended approach?`,
    options: [
      { key: "A", text: `Rewrite all log statements to use the OpenTelemetry Logs API directly` },
      { key: "B", text: `Replace Log4j with a custom OpenTelemetry-native logging library` },
      { key: "C", text: `Use the log bridge API to connect Log4j to OpenTelemetry` },
      { key: "D", text: `Configure the Collector's filelog receiver to parse Log4j text files from disk` },
    ],
    correctAnswer: "C",
    explanation: `The recommended approach is to use a log bridge that connects the existing logging framework (Log4j) to OpenTelemetry. The bridge converts native log calls into OTel LogRecords, preserving all existing logging code while adding OpenTelemetry capabilities like trace correlation and OTLP export. Rewriting log statements or replacing the framework is unnecessary and disruptive.`,
    difficulty: "scenario",
  },
  {
    id: "10-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `How many severity levels does the OpenTelemetry log data model define, and how are they organized?`,
    options: [
      { key: "A", text: `6 levels: TRACE, DEBUG, INFO, WARN, ERROR, FATAL` },
      { key: "B", text: `12 levels: 6 base severities with 2 sub-levels each` },
      { key: "C", text: `8 levels: TRACE, DEBUG, INFO, NOTICE, WARN, ERROR, CRITICAL, FATAL` },
      { key: "D", text: `24 levels: 6 base severities with 4 sub-levels each` },
    ],
    correctAnswer: "D",
    explanation: `The OTLP log data model defines a 24-level severity scale organized as 6 base severities (TRACE, DEBUG, INFO, WARN, ERROR, FATAL) with 4 sub-levels each. For example, DEBUG has DEBUG (5), DEBUG2 (6), DEBUG3 (7), and DEBUG4 (8). This granularity allows fine-grained mapping from diverse source frameworks.`,
    difficulty: "recall",
  },
  {
    id: "10-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A Python application uses the standard logging module with level CRITICAL. When this log is exported via an OpenTelemetry log bridge, what severity_number is it mapped to?`,
    options: [
      { key: "A", text: `17 (ERROR)` },
      { key: "B", text: `20 (ERROR4)` },
      { key: "C", text: `24 (FATAL4)` },
      { key: "D", text: `21 (FATAL)` },
    ],
    correctAnswer: "D",
    explanation: `Python's CRITICAL level maps to OTLP severity_number 21, which corresponds to FATAL. The mapping follows the convention table in the OTLP logs specification: Python DEBUG maps to 5 (DEBUG), INFO to 9 (INFO), WARNING to 13 (WARN), ERROR to 17 (ERROR), and CRITICAL to 21 (FATAL).`,
    difficulty: "scenario",
  },
  {
    id: "10-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In the OTLP log data model, what is the difference between \`time_unix_nano\` and \`observed_time_unix_nano\`?`,
    options: [
      { key: "A", text: `\`time_unix_nano\` is when the event occurred; \`observed_time_unix_nano\` is when the log was collected or observed` },
      { key: "B", text: `They are interchangeable aliases for the same timestamp` },
      { key: "C", text: `\`time_unix_nano\` is when the log was collected; \`observed_time_unix_nano\` is when the event occurred` },
      { key: "D", text: `Both are required fields that must contain identical values` },
    ],
    correctAnswer: "A",
    explanation: `The two timestamp fields serve distinct purposes. \`time_unix_nano\` records when the event actually occurred (and may reflect a time in the past), while \`observed_time_unix_nano\` records when the log was collected or observed by the telemetry pipeline. The specification states that \`observed_time_unix_nano\` should always be set, while \`time_unix_nano\` is optional.`,
    difficulty: "recall",
  },
  {
    id: "10-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A log record arrives with \`severity_number\` set to 0. What does this indicate in the OTLP data model?`,
    options: [
      { key: "A", text: `The log has TRACE severity, the lowest defined level` },
      { key: "B", text: `The severity is unknown or was not set by the source` },
      { key: "C", text: `The log record is malformed and should be dropped by the pipeline` },
      { key: "D", text: `The severity must be inferred from the \`severity_text\` field` },
    ],
    correctAnswer: "B",
    explanation: `In the OTLP log data model, severity_number 0 is defined as SEVERITY_NUMBER_UNSPECIFIED, meaning the severity is unknown. TRACE severity begins at 1, not 0. Logs with unspecified severity are still valid log records and should not be dropped. The severity_text field may provide additional context but does not override the numeric value.`,
    difficulty: "edge-case",
  },
  {
    id: "10-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A development team is building a Go service with traces and logs. They want log records to automatically include the trace_id and span_id of the active span. What do they need to do?`,
    options: [
      { key: "A", text: `Manually extract the span context and set trace_id and span_id on each log record` },
      { key: "B", text: `Configure the Collector to inject trace correlation after receiving the logs` },
      { key: "C", text: `Pass the context.Context containing the active span to logger.Emit(); the SDK extracts the correlation fields automatically` },
      { key: "D", text: `Set the OTEL_LOGS_TRACE_CORRELATION environment variable to true` },
    ],
    correctAnswer: "C",
    explanation: `When you call logger.Emit(ctx, record) and the context contains an active span, the OpenTelemetry SDK automatically extracts the trace_id and span_id from the span context and attaches them to the log record. No manual extraction or Collector-side processing is needed. This is the standard correlation pattern in OpenTelemetry.`,
    difficulty: "scenario",
  },
  {
    id: "10-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In the OTLP log record, the \`body\` field uses the \`AnyValue\` type. Which of the following is NOT a valid body value type?`,
    options: [
      { key: "A", text: `A string containing the log message` },
      { key: "B", text: `A key-value list (map) representing structured data` },
      { key: "C", text: `An array of values` },
      { key: "D", text: `A regular expression pattern` },
    ],
    correctAnswer: "D",
    explanation: `The \`body\` field accepts AnyValue, which supports string, int, float, bool, bytes, key-value list (kvlistValue), and array (arrayValue). Regular expression is not a supported AnyValue type. The most common body type is string, while structured events often use kvlist for key-value data.`,
    difficulty: "recall",
  },
  {
    id: "10-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An SRE team receives log records where all structured data is embedded in the body as a single unstructured string: "ERROR user=123 action=login status=failed ip=192.168.1.1". Why is this an anti-pattern in OpenTelemetry?`,
    options: [
      { key: "A", text: `The body field has a strict maximum length that key-value strings exceed` },
      { key: "B", text: `The OTLP specification prohibits embedding key-value pairs in string bodies` },
      { key: "C", text: `Structured data embedded in the body cannot be efficiently searched, filtered, or indexed by backends` },
      { key: "D", text: `The Collector rejects log records with key-value patterns in the body field` },
    ],
    correctAnswer: "C",
    explanation: `Placing structured data in the body as an unstructured string defeats the purpose of the structured log data model. Fields like user ID, action, and status should be log attributes (key-value pairs), which backends can index, search, and filter efficiently. The body should contain a human-readable message, while attributes carry the structured, queryable fields.`,
    difficulty: "scenario",
  },
  {
    id: "10-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `When the OpenTelemetry Logs API is used without an SDK installed, what happens when a Logger emits a log record?`,
    options: [
      { key: "A", text: `The emit call silently succeeds with no side effects and no performance overhead` },
      { key: "B", text: `The log record is buffered in memory until an SDK is eventually configured` },
      { key: "C", text: `The emit call throws an exception to alert the developer` },
      { key: "D", text: `The log record is written to standard output as a fallback` },
    ],
    correctAnswer: "A",
    explanation: `Without an SDK, the Logs API operates in no-op mode. Logger creation succeeds (returning a no-op logger), Emit operations silently succeed without doing anything, and the Enabled method returns false. This design ensures that libraries instrumented with the Logs API do not break applications that have not installed an SDK.`,
    difficulty: "edge-case",
  },
  {
    id: "10-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A Java application uses java.util.logging with level SEVERE. When this log is exported through the OpenTelemetry log bridge, the severity_number maps to 17 (ERROR). What should the severity_text field contain?`,
    options: [
      { key: "A", text: `"ERROR" to match the OTLP base severity name` },
      { key: "B", text: `"SEVERE" to preserve the original logging framework's level name` },
      { key: "C", text: `"17" as a string representation of the severity_number` },
      { key: "D", text: `An empty string, since severity_number is the authoritative field` },
    ],
    correctAnswer: "B",
    explanation: `The severity_text field should preserve the original severity name from the source logging framework. For java.util.logging, this means "SEVERE" rather than the OTLP name "ERROR". This dual representation allows routing and filtering on the normalized severity_number while retaining source-specific terminology for debugging and context.`,
    difficulty: "scenario",
  },
  {
    id: "10-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What mechanism does the OpenTelemetry Logs API provide to avoid performing expensive operations for log records that will not be recorded?`,
    options: [
      { key: "A", text: `A severity threshold configured globally at the LoggerProvider level` },
      { key: "B", text: `An automatic circuit breaker that disables logging under high CPU load` },
      { key: "C", text: `The Logger.Enabled method, which checks whether logging is enabled for given parameters before emitting` },
      { key: "D", text: `A compile-time flag that strips log statements from production builds` },
    ],
    correctAnswer: "C",
    explanation: `The Logger.Enabled API (stable since v1.44.0) allows callers to check whether a log at a given severity (and optionally event name) would actually be recorded. If Enabled returns false, the caller can skip expensive operations like gathering detailed context or formatting complex messages. The return value is not static and can change over time, so it should be checked before each emission.`,
    difficulty: "recall",
  },
  {
    id: "10-Q13",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team configures a Collector pipeline where the routing connector routes logs based on severity_number. The condition is \`severity_number >= SEVERITY_NUMBER_WARN\` (which evaluates to 13). Some log records arrive with severity_number 0 (UNSPECIFIED). Where do these logs route?`,
    options: [
      { key: "A", text: `To the vendor pipeline, because severity 0 is treated as the highest priority` },
      { key: "B", text: `To the default pipeline, because 0 does not satisfy the condition >= 13` },
      { key: "C", text: `The logs are dropped because severity 0 is considered invalid` },
      { key: "D", text: `The routing connector raises an evaluation error and stops processing the batch` },
    ],
    correctAnswer: "B",
    explanation: `The OTTL condition \`severity_number >= SEVERITY_NUMBER_WARN\` evaluates to \`severity_number >= 13\`. Since 0 is less than 13, logs with SEVERITY_NUMBER_UNSPECIFIED do not match the condition and route to the default pipeline. This can be a subtle issue: if unspecified severity logs should be treated as potentially important, the condition needs an explicit check like \`severity_number >= SEVERITY_NUMBER_WARN or severity_number == 0\`.`,
    difficulty: "edge-case",
  },
  {
    id: "10-Q14",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team integrates the Zap logging library in their Go application with an OpenTelemetry log bridge. They notice that logs in the observability backend include attributes like service.name, service.version, and the instrumentation scope name, even though their code only calls \`logger.Info("message")\`. What explains this?`,
    options: [
      { key: "A", text: `The Collector enriches every log with default resource attributes from its own configuration` },
      { key: "B", text: `Zap natively supports OpenTelemetry attributes without requiring any bridge` },
      { key: "C", text: `The observability backend infers service metadata from the log body content` },
      { key: "D", text: `The log bridge and SDK automatically attach resource and scope information to each log record` },
    ],
    correctAnswer: "D",
    explanation: `When a log bridge forwards logs to the OpenTelemetry SDK, the SDK attaches resource attributes (such as service.name and service.version from the SDK configuration) and scope information (the Logger's instrumentation scope name) to every log record. This enrichment happens transparently in the SDK pipeline, so application code only needs to emit the log message and any custom attributes.`,
    difficulty: "scenario",
  },
  {
    id: "11-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the role of a SpanProcessor in the OpenTelemetry SDK?`,
    options: [
      { key: "A", text: `It validates span names against semantic conventions` },
      { key: "B", text: `It receives completed spans and forwards them to a SpanExporter` },
      { key: "C", text: `It creates new spans when the tracer.Start() method is called` },
      { key: "D", text: `It assigns trace IDs and span IDs to new spans` },
    ],
    correctAnswer: "B",
    explanation: `A SpanProcessor sits between the TracerProvider and the SpanExporter. When a span ends, the processor receives it and decides how and when to forward it to the exporter. SimpleSpanProcessor forwards immediately; BatchSpanProcessor buffers spans before forwarding in batches. The processor does not create spans or assign IDs, as those are responsibilities of the Tracer and ID generator respectively.`,
    difficulty: "recall",
  },
  {
    id: "11-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A developer is debugging a service locally and wants spans printed to the terminal immediately as each span completes, without any buffering delay. Which combination should they use?`,
    options: [
      { key: "A", text: `BatchSpanProcessor with a console exporter` },
      { key: "B", text: `PeriodicExportingMetricReader with a stdout exporter` },
      { key: "C", text: `SimpleSpanProcessor with a console exporter` },
      { key: "D", text: `ManualMetricReader with an in-memory exporter` },
    ],
    correctAnswer: "C",
    explanation: `SimpleSpanProcessor exports each span synchronously as it completes, with no batching or delay. Paired with a console (stdout) exporter, this prints each span to the terminal the moment it finishes. BatchSpanProcessor would introduce a buffering delay before export. The metric-related options (PeriodicExportingMetricReader, ManualMetricReader) are for metrics, not traces.`,
    difficulty: "scenario",
  },
  {
    id: "11-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which three parameters are typically configurable on a BatchSpanProcessor?`,
    options: [
      { key: "A", text: `Maximum queue size, scheduled delay, and maximum export batch size` },
      { key: "B", text: `Sampling rate, compression algorithm, and retry count` },
      { key: "C", text: `Thread pool size, connection timeout, and TLS certificate path` },
      { key: "D", text: `Flush interval, memory limit, and endpoint URL` },
    ],
    correctAnswer: "A",
    explanation: `The BatchSpanProcessor specification defines four configurable parameters: maxQueueSize (default 2048), scheduledDelayMillis (default 5000ms), maxExportBatchSize (default 512), and exportTimeoutMillis (default 30000ms). The three most commonly tuned are queue size, scheduled delay, and batch size. Sampling, TLS, and endpoint configuration belong to other SDK components (samplers and exporters).`,
    difficulty: "recall",
  },
  {
    id: "11-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team configures their MeterProvider with a PeriodicExportingMetricReader set to a 60-second interval and an OTLP exporter. When does telemetry data actually get exported?`,
    options: [
      { key: "A", text: `Each time a metric instrument records a value` },
      { key: "B", text: `When the application calls Flush() on the MeterProvider` },
      { key: "C", text: `When the exporter's internal queue reaches a configured batch size` },
      { key: "D", text: `Every 60 seconds, when the reader collects and exports the accumulated metric data` },
    ],
    correctAnswer: "D",
    explanation: `PeriodicExportingMetricReader collects metric data at the configured interval (60 seconds in this case) and exports it through the associated exporter. Unlike trace pipelines where each span is an event, metrics are aggregated in memory and only read at collection time. The reader does not export on each recording; it accumulates data points between collection cycles.`,
    difficulty: "scenario",
  },
  {
    id: "11-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An application using BatchSpanProcessor terminates abruptly without calling Shutdown() on the TracerProvider. What is the most likely consequence?`,
    options: [
      { key: "A", text: `Spans buffered in the batch processor's queue are lost` },
      { key: "B", text: `The SDK automatically flushes all pending spans before the process exits` },
      { key: "C", text: `The operating system ensures OTLP connections are drained before termination` },
      { key: "D", text: `The spans are persisted to disk and recovered on the next application start` },
    ],
    correctAnswer: "A",
    explanation: `BatchSpanProcessor holds spans in an in-memory queue until the next scheduled export. If the application exits without calling Shutdown(), which triggers a final flush, all queued spans are lost. The SDK does not register OS signal handlers or persist data to disk automatically. Graceful shutdown with a timeout (for example, 10 seconds) is essential to flush remaining telemetry.`,
    difficulty: "edge-case",
  },
  {
    id: "11-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `When building an SDK tracing pipeline programmatically, in what order must the components be created?`,
    options: [
      { key: "A", text: `TracerProvider, then BatchSpanProcessor, then SpanExporter` },
      { key: "B", text: `BatchSpanProcessor, then SpanExporter, then TracerProvider` },
      { key: "C", text: `TracerProvider, then SpanExporter, then BatchSpanProcessor` },
      { key: "D", text: `SpanExporter, then BatchSpanProcessor, then TracerProvider` },
    ],
    correctAnswer: "D",
    explanation: `Pipeline components are assembled inside-out. First, create the SpanExporter (the destination). Then create the BatchSpanProcessor, passing it the exporter. Finally, create the TracerProvider, registering the processor. This is because each component depends on the next one in the pipeline: the processor needs to know the exporter, and the provider needs to know the processor.`,
    difficulty: "recall",
  },
  {
    id: "11-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An engineer is writing integration tests and needs to assert that specific span attributes are set correctly by their instrumentation code. Which exporter type is most appropriate?`,
    options: [
      { key: "A", text: `OTLP exporter pointing to a test Collector instance` },
      { key: "B", text: `Console exporter with output captured in a buffer` },
      { key: "C", text: `In-memory exporter that stores spans for programmatic inspection` },
      { key: "D", text: `Debug exporter with verbose logging enabled` },
    ],
    correctAnswer: "C",
    explanation: `The in-memory exporter stores exported spans in a data structure that test code can inspect programmatically. This allows assertions on span names, attributes, status, and other fields without parsing text output or running external infrastructure. While a console exporter could work with output capturing, it requires fragile text parsing. An OTLP exporter with a test Collector adds unnecessary complexity for unit-level assertions.`,
    difficulty: "scenario",
  },
  {
    id: "11-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which transport protocols does the OTLP exporter specification define?`,
    options: [
      { key: "A", text: `gRPC and UDP` },
      { key: "B", text: `gRPC, HTTP/protobuf, and HTTP/JSON` },
      { key: "C", text: `HTTP/protobuf and WebSocket` },
      { key: "D", text: `gRPC and TCP` },
    ],
    correctAnswer: "B",
    explanation: `The OTLP specification defines three transport bindings: gRPC (using protocol buffers over HTTP/2), HTTP with protobuf encoding (http/protobuf), and HTTP with JSON encoding (http/json). The YAML SDK configuration file selects the transport via the \`protocol\` field. In practice, gRPC and http/protobuf are the most commonly used, but http/json is also a valid option defined by the specification. There is no WebSocket, UDP, or plain TCP transport in OTLP.`,
    difficulty: "recall",
  },
  {
    id: "11-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team wants to send trace data to both a production observability backend via OTLP and a console output for local development visibility. How should they configure the SDK?`,
    options: [
      { key: "A", text: `Register two SpanProcessors on the TracerProvider, each with its own exporter` },
      { key: "B", text: `Configure a single exporter that fans out to multiple destinations` },
      { key: "C", text: `Create two separate TracerProviders and set both as global` },
      { key: "D", text: `Use a SpanProcessor that clones spans before forwarding` },
    ],
    correctAnswer: "A",
    explanation: `The TracerProvider supports registering multiple SpanProcessors. Each processor operates independently with its own exporter. For this use case, one BatchSpanProcessor with an OTLP exporter handles production export, while a second SimpleSpanProcessor (or BatchSpanProcessor) with a console exporter handles local output. Only one TracerProvider can be set as the global provider; creating two would lose data from one of them.`,
    difficulty: "scenario",
  },
  {
    id: "11-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What distinguishes ManualMetricReader from PeriodicExportingMetricReader?`,
    options: [
      { key: "A", text: `ManualMetricReader does not require an exporter` },
      { key: "B", text: `ManualMetricReader only exports when the application explicitly triggers a collection` },
      { key: "C", text: `ManualMetricReader supports only delta temporality while PeriodicExportingMetricReader supports both` },
      { key: "D", text: `ManualMetricReader is the only reader compatible with asynchronous instruments` },
    ],
    correctAnswer: "B",
    explanation: `ManualMetricReader does not run on a timer. It collects and exports metric data only when the application code explicitly calls its Collect or ForceFlush method. This makes it ideal for testing scenarios where deterministic control over export timing is needed. PeriodicExportingMetricReader, by contrast, runs a background timer that collects and exports at regular intervals. Both readers work with all instrument types and support the same temporality options.`,
    difficulty: "edge-case",
  },
  {
    id: "11-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An SRE observes that when their observability backend becomes briefly unavailable, spans are permanently lost even though outages last only a few seconds. The SDK is configured with a SimpleSpanProcessor. What change would most improve resilience?`,
    options: [
      { key: "A", text: `Increase the OTLP exporter's connection timeout` },
      { key: "B", text: `Add a retry policy to the OTLP exporter configuration` },
      { key: "C", text: `Configure the SDK with a persistent queue` },
      { key: "D", text: `Replace SimpleSpanProcessor with BatchSpanProcessor` },
    ],
    correctAnswer: "D",
    explanation: `SimpleSpanProcessor exports each span synchronously and immediately. If the export fails (for example, because the backend is unreachable), that span is lost. BatchSpanProcessor buffers spans in an in-memory queue and exports them in batches, providing a natural buffer against brief outages. Spans queued during a short outage are exported once the backend recovers, as long as the queue does not fill up. Increasing timeouts or adding retries to the exporter helps, but the fundamental issue is the lack of buffering.`,
    difficulty: "scenario",
  },
  {
    id: "11-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `In the OpenTelemetry SDK's YAML configuration file, under which section are log record processors and their exporters defined?`,
    options: [
      { key: "A", text: `\`tracer_provider.processors\`` },
      { key: "B", text: `\`meter_provider.readers\`` },
      { key: "C", text: `\`logger_provider.processors\`` },
      { key: "D", text: `\`log_provider.exporters\`` },
    ],
    correctAnswer: "C",
    explanation: `The YAML configuration file uses separate top-level sections for each signal: \`tracer_provider\` for traces, \`meter_provider\` for metrics, and \`logger_provider\` for logs. Log record processors (such as batch processors with OTLP exporters) are defined under \`logger_provider.processors\`. There is no \`log_provider\` section; the correct key is \`logger_provider\`, consistent with the SDK's LoggerProvider naming.`,
    difficulty: "recall",
  },
  {
    id: "11-Q13",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team uses YAML-based SDK configuration with a single periodic metric reader exporting via OTLP. They want to add a second reader that exports to the console for local debugging. What is the correct approach?`,
    options: [
      { key: "A", text: `Add a second entry under \`meter_provider.readers\` with a console exporter` },
      { key: "B", text: `Add a second \`meter_provider\` section to the configuration file` },
      { key: "C", text: `Configure the existing periodic reader with two exporters` },
      { key: "D", text: `Create a separate YAML configuration file for the console reader` },
    ],
    correctAnswer: "A",
    explanation: `The \`meter_provider.readers\` section accepts a list of readers. Each reader has its own exporter, so adding a second \`periodic\` entry with a \`console\` exporter creates two independent collection and export pipelines. A single reader can only have one exporter, and YAML keys cannot be duplicated (so a second \`meter_provider\` section would override the first, not merge with it).`,
    difficulty: "scenario",
  },
  {
    id: "11-Q14",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Why is SimpleSpanProcessor generally unsuitable for production workloads?`,
    options: [
      { key: "A", text: `It does not support the OTLP export protocol` },
      { key: "B", text: `It limits the TracerProvider to a single registered processor` },
      { key: "C", text: `It exports each span synchronously, blocking the application thread until the export completes` },
      { key: "D", text: `It does not attach resource attributes to exported spans` },
    ],
    correctAnswer: "C",
    explanation: `SimpleSpanProcessor calls the exporter's Export method synchronously on each Span.End() call. This means the application thread is blocked while the span is being transmitted over the network. In production, where export targets may have variable latency, this adds unpredictable delays to request processing. BatchSpanProcessor solves this by decoupling span completion from export via an asynchronous queue. SimpleSpanProcessor supports all exporters (including OTLP), all resource attributes, and allows multiple processors on the same provider.`,
    difficulty: "edge-case",
  },
  {
    id: "12-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What does the OpenTelemetry Context carry?`,
    options: [
      { key: "A", text: `Only the current span's trace ID and span ID` },
      { key: "B", text: `Cross-cutting concerns including the current span and baggage` },
      { key: "C", text: `The full span data including attributes, events, and links` },
      { key: "D", text: `Only baggage key-value pairs for cross-service communication` },
    ],
    correctAnswer: "B",
    explanation: `Context is an immutable container that carries execution-scoped values, including the current Span (for trace correlation), Baggage (application-defined key-value pairs), and other cross-cutting concerns. It does not carry full span data or only trace IDs.`,
    difficulty: "recall",
  },
  {
    id: "12-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team instruments a service that publishes order events to a message queue. Downstream consumers process these events but their spans appear as separate traces instead of being connected to the producer's trace. What is the most likely cause?`,
    options: [
      { key: "A", text: `The producer is not calling inject to serialize the trace context into the message headers` },
      { key: "B", text: `The message queue does not support the OTLP protocol` },
      { key: "C", text: `The consumer is using a different SDK language than the producer` },
      { key: "D", text: `The spans are being sampled independently by each service` },
    ],
    correctAnswer: "A",
    explanation: `For context propagation through message queues, the producer must explicitly call inject on the propagator to serialize the span context into the message headers. The consumer then calls extract to reconstruct the context. Without inject on the producer side, the consumer has no context to extract, and its spans start a new trace. Message queues do not need to support OTLP; they only need to support metadata or headers where the serialized context can be placed.`,
    difficulty: "scenario",
  },
  {
    id: "12-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which of the following correctly describes the W3C \`traceparent\` header format?`,
    options: [
      { key: "A", text: `\`{trace-id}-{span-id}-{flags}\` with 16-byte trace ID and 8-byte span ID` },
      { key: "B", text: `\`{version}-{trace-id}-{flags}-{span-id}\` with 32 hex character trace ID` },
      { key: "C", text: `\`{version}-{trace-id}-{span-id}-{flags}\` with 32 hex character trace ID and 16 hex character span ID` },
      { key: "D", text: `\`{trace-id}:{span-id}:{parent-id}:{flags}\` with 16 hex character trace ID` },
    ],
    correctAnswer: "C",
    explanation: `The W3C traceparent header uses the format \`{version}-{trace-id}-{span-id}-{flags}\`, where version is 2 hex characters (currently "00"), trace-id is 32 hex characters (16 bytes), span-id is 16 hex characters (8 bytes), and flags is 2 hex characters (containing the sampled flag). Example: \`00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01\`.`,
    difficulty: "recall",
  },
  {
    id: "12-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team deploys a new service instrumented with OpenTelemetry. The service receives HTTP requests and creates spans, but the spans always appear as root spans with no parent, even though the calling service correctly sends trace context headers. What should the team check first?`,
    options: [
      { key: "A", text: `Whether the OTLP exporter is configured correctly` },
      { key: "B", text: `Whether the span kind is set to SERVER` },
      { key: "C", text: `Whether the service is using BatchSpanProcessor instead of SimpleSpanProcessor` },
      { key: "D", text: `Whether a global TextMapPropagator has been configured at startup` },
    ],
    correctAnswer: "D",
    explanation: `If no global propagator is set, the SDK has no mechanism to extract incoming trace context from HTTP headers. The default propagator is a no-op, meaning extract returns an empty context and every span becomes a root span. The fix is to set a global propagator at startup, typically using \`otel.SetTextMapPropagator()\` with at least \`propagation.TraceContext{}\`. The exporter, span kind, and processor type do not affect context extraction.`,
    difficulty: "scenario",
  },
  {
    id: "12-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `Which methods must a custom carrier implement to satisfy the TextMapCarrier interface?`,
    options: [
      { key: "A", text: `Get, Set, and Keys` },
      { key: "B", text: `Inject, Extract, and Fields` },
      { key: "C", text: `Read, Write, and List` },
      { key: "D", text: `Encode, Decode, and Headers` },
    ],
    correctAnswer: "A",
    explanation: `The TextMapCarrier interface requires three methods: Get(key) to retrieve a value by key, Set(key, value) to store a key-value pair, and Keys() to list all available keys. Inject, Extract, and Fields are methods on the TextMapPropagator interface, not the carrier.`,
    difficulty: "recall",
  },
  {
    id: "12-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An organization is migrating from a Zipkin-based tracing system to OpenTelemetry. During the transition, some services use Zipkin's B3 headers while new services use W3C Trace Context. How should the team configure propagation to maintain end-to-end tracing across both old and new services?`,
    options: [
      { key: "A", text: `Configure all services to use B3 propagation until the migration is complete` },
      { key: "B", text: `Use a composite propagator that includes both TraceContext and B3 propagators` },
      { key: "C", text: `Deploy a dedicated Collector between old and new services to translate headers` },
      { key: "D", text: `Set the W3C TraceContext propagator and let the SDK automatically fall back to B3` },
    ],
    correctAnswer: "B",
    explanation: `A composite propagator combines multiple propagation formats. When injecting, all configured propagators write their respective headers, so outgoing requests carry both W3C and B3 headers. When extracting, each propagator is tried in order, and the first match wins. This allows old Zipkin services to continue reading B3 headers while new services read W3C headers, maintaining trace continuity during migration. There is no automatic fallback in the SDK; the composite propagator must be explicitly configured.`,
    difficulty: "scenario",
  },
  {
    id: "12-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `How does a composite propagator behave differently during inject versus extract operations?`,
    options: [
      { key: "A", text: `During inject, only the first propagator writes headers; during extract, all propagators attempt extraction` },
      { key: "B", text: `During inject and extract, all propagators are called and their results are merged` },
      { key: "C", text: `During inject, all propagators write their headers; during extract, propagators are tried in order and the first match wins` },
      { key: "D", text: `During inject, only the primary propagator writes; during extract, a fallback chain is used based on header presence` },
    ],
    correctAnswer: "C",
    explanation: `The composite propagator has asymmetric behavior. On inject, every configured propagator writes its format to the carrier, so headers for all formats are present. On extract, propagators are tried in sequence and the first one that successfully extracts context provides the result. This design ensures maximum compatibility on the sending side while avoiding conflicts from multiple formats on the receiving side.`,
    difficulty: "edge-case",
  },
  {
    id: "12-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team needs to pass a tenant identifier from the API gateway to all downstream services so that each service can route database queries to the correct tenant shard. The identifier must be available in services three or four hops downstream. What is the appropriate OpenTelemetry mechanism?`,
    options: [
      { key: "A", text: `Add the tenant ID as a resource attribute on all services` },
      { key: "B", text: `Store the tenant ID in a span event at the gateway` },
      { key: "C", text: `Pass the tenant ID as a span attribute and rely on span links` },
      { key: "D", text: `Use OpenTelemetry Baggage to propagate the tenant ID across service boundaries` },
    ],
    correctAnswer: "D",
    explanation: `Baggage is designed for cross-cutting context that must travel across service boundaries throughout a distributed transaction. Span attributes are local to a single span and do not propagate. Resource attributes are static per-service metadata, not per-request data. Span events and links do not carry data forward to downstream services. Baggage entries are automatically injected into and extracted from carriers alongside trace context when the Baggage propagator is configured.`,
    difficulty: "scenario",
  },
  {
    id: "12-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What are the recommended size limits for W3C Baggage?`,
    options: [
      { key: "A", text: `Maximum 8192 bytes total, 4096 bytes per member, and 180 members` },
      { key: "B", text: `Maximum 4096 bytes total, 2048 bytes per member, and 64 members` },
      { key: "C", text: `Maximum 16384 bytes total, 8192 bytes per member, and 256 members` },
      { key: "D", text: `No size limits are specified; implementations decide their own limits` },
    ],
    correctAnswer: "A",
    explanation: `The W3C Baggage specification recommends a maximum of 8192 bytes for all baggage combined, 4096 bytes per individual member, and a maximum of 180 members. These limits exist to prevent excessive header sizes that could cause network overhead, exceed HTTP header size limits on intermediary servers, or degrade performance.`,
    difficulty: "recall",
  },
  {
    id: "12-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A developer stores an API key in OpenTelemetry Baggage so that downstream services can authenticate with a shared external API. What is the security concern with this approach?`,
    options: [
      { key: "A", text: `Baggage values are hashed during propagation, making the key unreadable` },
      { key: "B", text: `Baggage is transmitted in cleartext HTTP headers, exposing the API key to any intermediary` },
      { key: "C", text: `Baggage is encrypted by default but uses weak encryption algorithms` },
      { key: "D", text: `Baggage values are only visible to OpenTelemetry-instrumented services, so there is no risk` },
    ],
    correctAnswer: "B",
    explanation: `Baggage is propagated as plain text in HTTP headers (the \`baggage\` header in W3C format). Any intermediary that can inspect HTTP headers, such as load balancers, proxies, or logging systems, can read baggage values. Sensitive data like passwords, API keys, and credit card numbers should never be placed in baggage. Instead, use opaque identifiers (like user IDs or session IDs) that can be resolved securely by each service.`,
    difficulty: "scenario",
  },
  {
    id: "12-Q11",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An integration platform uses a legacy messaging protocol that does not support message headers or metadata. The team wants distributed traces to span across services connected by this protocol. What is the correct assessment?`,
    options: [
      { key: "A", text: `The team should encode the trace context in the message body as a JSON field` },
      { key: "B", text: `The team should use span links instead of parent-child relationships` },
      { key: "C", text: `Context propagation is not possible when the transport has no mechanism to carry metadata` },
      { key: "D", text: `The OpenTelemetry SDK automatically detects and uses alternative propagation channels` },
    ],
    correctAnswer: "C",
    explanation: `Context propagation requires a carrier mechanism (headers, metadata, or similar) in the transport protocol to serialize and deserialize the span context. When the protocol lacks any such mechanism and the message payload cannot be modified, distributed tracing cannot cross that boundary. The spans on each side would appear as independent traces. Encoding context in the message body is not a standard practice and would require custom serialization logic outside the propagator API.`,
    difficulty: "edge-case",
  },
  {
    id: "12-Q12",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team configures the global propagator with only \`propagation.TraceContext{}\` but does not include \`propagation.Baggage{}\`. What happens to baggage values set in the context when making outgoing HTTP requests?`,
    options: [
      { key: "A", text: `Baggage values are serialized into a custom \`x-otel-baggage\` header` },
      { key: "B", text: `Baggage values are included in the \`tracestate\` header alongside trace context` },
      { key: "C", text: `Baggage values are added as span attributes instead of being propagated` },
      { key: "D", text: `Baggage values are silently dropped and not included in outgoing headers` },
    ],
    correctAnswer: "D",
    explanation: `Each propagator is responsible for its own format. The TraceContext propagator only handles \`traceparent\` and \`tracestate\` headers. Without the Baggage propagator in the composite, the baggage is not serialized into the \`baggage\` header during inject. The baggage values remain in the local context but are not propagated to downstream services. The Baggage propagator must be explicitly included in the composite propagator for cross-service baggage propagation to work.`,
    difficulty: "recall",
  },
  {
    id: "13-Q1",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `What is the primary benefit of zero-code instrumentation in OpenTelemetry?`,
    options: [
      { key: "A", text: `It generates more detailed spans than manual instrumentation` },
      { key: "B", text: `It provides higher-quality semantic conventions than manual approaches` },
      { key: "C", text: `It adds observability to applications without requiring source code changes` },
      { key: "D", text: `It eliminates the need for an OpenTelemetry Collector` },
    ],
    correctAnswer: "C",
    explanation: `Zero-code (automatic) instrumentation allows teams to add observability to applications without modifying application source code. It uses language-specific mechanisms (Java agents, environment variables, eBPF) to inject instrumentation at runtime. While convenient, it typically provides less granular observability than manual instrumentation and still requires a Collector or backend to receive the data.`,
    difficulty: "recall",
  },
  {
    id: "13-Q2",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A team has manually instrumented their Go service using the OpenTelemetry API but wants the Operator to manage configuration such as exporter endpoint, sampler settings, and resource attributes. Which annotation should they use?`,
    options: [
      { key: "A", text: `\`instrumentation.opentelemetry.io/inject-sdk: "true"\`` },
      { key: "B", text: `\`instrumentation.opentelemetry.io/inject-go: "true"\`` },
      { key: "C", text: `\`instrumentation.opentelemetry.io/inject-manual: "true"\`` },
      { key: "D", text: `\`instrumentation.opentelemetry.io/inject-config: "true"\`` },
    ],
    correctAnswer: "A",
    explanation: `The \`inject-sdk\` annotation injects SDK environment variables (exporter endpoint, sampler, resource attributes) without adding auto-instrumentation libraries. This is ideal for applications that already have manual instrumentation but want the Operator to manage configuration centrally. The \`inject-go\` annotation would add Go auto-instrumentation (eBPF-based), which is experimental and different from what the team needs.`,
    difficulty: "scenario",
  },
  {
    id: "13-Q3",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An operations team annotates a single-container pod with both \`inject-java: "true"\` and \`inject-python: "true"\`. What is the expected behavior?`,
    options: [
      { key: "A", text: `Both instrumentations are applied and traces from both runtimes are collected` },
      { key: "B", text: `The Operator applies only the first annotation alphabetically` },
      { key: "C", text: `The pod fails to start because the init containers conflict` },
      { key: "D", text: `This is an anti-pattern; only one language injection is supported per container` },
    ],
    correctAnswer: "D",
    explanation: `The OpenTelemetry Operator supports only one language injection per container. Applying multiple language injections to the same container is explicitly documented as an anti-pattern. For pods with multiple containers running different languages, use the language-specific \`<language>-container-names\` annotation to target each container separately.`,
    difficulty: "edge-case",
  },
  {
    id: "13-Q4",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A platform team wants all Java services in a namespace to be automatically instrumented. They annotate the namespace with \`instrumentation.opentelemetry.io/inject-java: "true"\`. One legacy service in the namespace should not be instrumented. How does that service opt out?`,
    options: [
      { key: "A", text: `The service must be deployed in a different namespace` },
      { key: "B", text: `The service's pod template sets the annotation \`instrumentation.opentelemetry.io/inject-java: "false"\`` },
      { key: "C", text: `The service sets \`OTEL_JAVAAGENT_ENABLED=false\` as an environment variable` },
      { key: "D", text: `The Instrumentation CR must list excluded deployments in a \`spec.excludes\` field` },
    ],
    correctAnswer: "B",
    explanation: `When auto-instrumentation is enabled at the namespace level via annotations, individual pods can opt out by setting the same injection annotation to \`"false"\` on their pod template. The pod-level annotation overrides the namespace-level annotation, providing fine-grained control.`,
    difficulty: "scenario",
  },
  {
    id: "13-Q5",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `How does the OpenTelemetry Operator inject auto-instrumentation into a .NET application?`,
    options: [
      { key: "A", text: `By adding a NuGet package to the application at build time` },
      { key: "B", text: `By injecting a sidecar container that proxies all HTTP traffic` },
      { key: "C", text: `By setting \`CORECLR_ENABLE_PROFILING=1\` and related CLR profiler environment variables` },
      { key: "D", text: `By patching the .NET runtime with a custom assembly loader` },
    ],
    correctAnswer: "C",
    explanation: `.NET auto-instrumentation uses the CLR profiling API. The Operator sets \`CORECLR_ENABLE_PROFILING=1\` along with related profiler environment variables to hook into the .NET runtime at startup. This enables automatic instrumentation of supported libraries without requiring code changes or build-time modifications.`,
    difficulty: "recall",
  },
  {
    id: "13-Q6",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A development team runs a Java microservice with both the OpenTelemetry Java agent (auto-instrumentation) and manually created spans using the Tracer API. What happens to the manual spans when the agent is active?`,
    options: [
      { key: "A", text: `Manual spans appear as children of the auto-instrumented spans, creating a unified trace` },
      { key: "B", text: `Manual spans are discarded because the agent takes over all span creation` },
      { key: "C", text: `Manual spans and auto-instrumented spans create separate, unlinked traces` },
      { key: "D", text: `The application fails to start due to conflicting TracerProvider registrations` },
    ],
    correctAnswer: "A",
    explanation: `Auto-instrumentation and manual instrumentation are designed to work together. The auto-instrumentation agent provides the SDK and TracerProvider, so manual spans created via the API are nested within the automatically created spans (such as HTTP server spans). This is a key advantage of the API/SDK separation: library code uses the API, and the agent supplies the SDK implementation.`,
    difficulty: "recall",
  },
  {
    id: "13-Q7",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `How does the OpenTelemetry Operator inject auto-instrumentation libraries into an application pod at runtime?`,
    options: [
      { key: "A", text: `It rebuilds the application container image with the instrumentation libraries pre-installed` },
      { key: "B", text: `It deploys a sidecar container that proxies all outbound network traffic from the application` },
      { key: "C", text: `It uses an init container to copy instrumentation libraries into a shared volume and sets environment variables to load them` },
      { key: "D", text: `It patches the node's container runtime to intercept process startup for matching pods` },
    ],
    correctAnswer: "C",
    explanation: `The Operator uses a mutating admission webhook that fires when a matching pod is created. It injects an init container that copies the language-specific instrumentation libraries into an emptyDir volume shared with the application container. After the init container completes, the Operator also sets the appropriate environment variables (such as \`JAVA_TOOL_OPTIONS\`) so the application loads the libraries on startup. No image rebuilds or sidecars are involved.`,
    difficulty: "recall",
  },
  {
    id: "13-Q8",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A namespace contains two Instrumentation CRs: one named \`default\` configured with 10% sampling and one named \`high-fidelity\` configured with 100% sampling. A team wants a specific pod to use the \`high-fidelity\` CR. How do they select it?`,
    options: [
      { key: "A", text: `Add a label \`instrumentation-cr: high-fidelity\` to the pod spec` },
      { key: "B", text: `Set the injection annotation value to the CR name, e.g., \`instrumentation.opentelemetry.io/inject-java: "high-fidelity"\`` },
      { key: "C", text: `Set \`instrumentation.opentelemetry.io/instrumentation-name: "high-fidelity"\` on the pod` },
      { key: "D", text: `The Operator always picks the most recently created Instrumentation CR in the namespace` },
    ],
    correctAnswer: "B",
    explanation: `When the injection annotation value is \`"true"\`, the Operator selects the Instrumentation CR named \`default\` in the same namespace. To select a specific CR, set the annotation value to that CR's name instead of \`"true"\`. This allows different pods in the same namespace to use different instrumentation configurations, such as different sampling rates or exporter endpoints.`,
    difficulty: "scenario",
  },
  {
    id: "13-Q9",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `An Instrumentation CR defines \`spec.resource.resourceAttributes\` with \`service.name: "api-gateway"\`. A pod annotated with this Instrumentation also has an environment variable \`OTEL_SERVICE_NAME=custom-api\` in its container spec. Which service name takes effect?`,
    options: [
      { key: "A", text: `\`custom-api\`, because pod environment variables have the highest priority` },
      { key: "B", text: `\`api-gateway\`, because the Instrumentation CR takes priority over pod settings` },
      { key: "C", text: `The pod fails to start due to conflicting service name definitions` },
      { key: "D", text: `Both values are sent, creating duplicate \`service.name\` attributes` },
    ],
    correctAnswer: "A",
    explanation: `The OpenTelemetry Operator applies environment variables in a defined priority order, from lowest to highest: default config, exporter/sampler settings, resource attributes, pod labels, pod annotations, common env (\`spec.env\`), language-specific env, and finally pod environment variables. Pod environment variables have the highest priority and override all other sources, so \`custom-api\` wins.`,
    difficulty: "scenario",
  },
  {
    id: "13-Q10",
    domain: "api-sdk",
    domainLabel: "OpenTelemetry API and SDK",
    question: `A pod runs two containers: a Java backend named "api" and a Node.js frontend named "web". The team wants to auto-instrument both containers. How should they configure the annotations?`,
    options: [
      { key: "A", text: `Add both \`inject-java\` and \`inject-nodejs\` annotations without specifying containers, and the Operator detects the language automatically` },
      { key: "B", text: `Create two separate Instrumentation CRs, one for each language` },
      { key: "C", text: `Add both \`inject-java\` and \`inject-nodejs\` annotations along with \`java-container-names: "api"\` and \`nodejs-container-names: "web"\`` },
      { key: "D", text: `This is not possible; multi-language pods require manual instrumentation only` },
    ],
    correctAnswer: "C",
    explanation: `For pods with multiple containers running different languages, the Operator supports language-specific container targeting via \`<language>-container-names\` annotations. Each language injection annotation is paired with a corresponding container-names annotation (e.g., \`instrumentation.opentelemetry.io/java-container-names: "api"\` and \`instrumentation.opentelemetry.io/nodejs-container-names: "web"\`) to specify which container receives which instrumentation.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q1",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What are the six top-level sections in a complete OpenTelemetry Collector configuration file?`,
    options: [
      { key: "A", text: `receivers, processors, exporters, connectors, extensions, service` },
      { key: "B", text: `inputs, transforms, outputs, plugins, helpers, pipelines` },
      { key: "C", text: `sources, filters, sinks, bridges, utilities, service` },
      { key: "D", text: `receivers, processors, exporters, pipelines, extensions, telemetry` },
    ],
    correctAnswer: "A",
    explanation: `A complete Collector configuration file is structured with six top-level sections: \`receivers\` (data ingestion), \`processors\` (data transformation), \`exporters\` (data egress), \`connectors\` (pipeline bridges), \`extensions\` (supporting services), and \`service\` (pipeline definitions and settings). The \`service\` section internally contains \`pipelines\`, \`extensions\`, and \`telemetry\` subsections, but these are not top-level sections themselves.`,
    difficulty: "recall",
  },
  {
    id: "14-Q2",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team deploys a Collector to accept telemetry from both backend gRPC services and browser-based JavaScript applications. Which OTLP receiver configuration supports both use cases?`,
    options: [
      { key: "A", text: `Configure two separate receivers: one Jaeger receiver for gRPC and one OTLP receiver for HTTP` },
      { key: "B", text: `Configure the OTLP receiver with both grpc and http protocols enabled` },
      { key: "C", text: `Configure a single OTLP gRPC receiver, since the HTTP protocol is not supported by the OTLP receiver` },
      { key: "D", text: `Configure a Zipkin receiver for browser applications and an OTLP receiver for backend services` },
    ],
    correctAnswer: "B",
    explanation: `The OTLP receiver supports both gRPC and HTTP protocols simultaneously. gRPC is efficient for backend service-to-collector communication, while HTTP/JSON is better suited for browser-based telemetry. Both can be enabled in the same receiver configuration under the \`protocols\` key. Jaeger and Zipkin are separate formats, not OTLP.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q3",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What are the default ports for the OTLP receiver's gRPC and HTTP protocols?`,
    options: [
      { key: "A", text: `gRPC: 4318, HTTP: 4317` },
      { key: "B", text: `gRPC: 14250, HTTP: 14268` },
      { key: "C", text: `gRPC: 4317, HTTP: 4318` },
      { key: "D", text: `gRPC: 9090, HTTP: 8080` },
    ],
    correctAnswer: "C",
    explanation: `The OTLP receiver listens on port 4317 for gRPC and port 4318 for HTTP by default. Port 14250 and 14268 are Jaeger ports, and 9090/8080 are commonly used for Prometheus and generic HTTP services. These default ports are part of the OTLP specification.`,
    difficulty: "recall",
  },
  {
    id: "14-Q4",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An operations team manages Collector configurations across staging and production environments. They want a base configuration with environment-specific overrides without duplicating the entire file. How should they start the Collector?`,
    options: [
      { key: "A", text: `Use a single configuration file with conditional blocks that select settings based on the environment` },
      { key: "B", text: `Use the --env flag to choose between staging.yaml and production.yaml` },
      { key: "C", text: `Define two separate service sections in the same configuration file` },
      { key: "D", text: `Use multiple --config flags so that configurations are deep-merged left to right` },
    ],
    correctAnswer: "D",
    explanation: `The Collector supports multiple \`--config\` flags (e.g., \`otelcol --config base.yaml --config production.yaml\`). Configurations are deep-merged in order: keys from later sources override keys from earlier sources at each level. The Collector does not support conditional blocks or \`--env\` flags. Only one \`service\` section can exist in the final merged configuration.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q5",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An engineer configures the env provider with \`\${env:LOG_LEVEL:-info}\`. The LOG_LEVEL environment variable exists but is set to an empty string (\`export LOG_LEVEL=""\`). What value does the Collector use?`,
    options: [
      { key: "A", text: `An empty string, because the default value only applies when the variable is unset` },
      { key: "B", text: `"info", because the default value applies when the variable is empty` },
      { key: "C", text: `An error, because the env provider rejects empty environment variable values` },
      { key: "D", text: `"debug", which is the Collector's built-in default log level` },
    ],
    correctAnswer: "A",
    explanation: `The \`:-\` default value syntax in the env provider only applies to **unset** variables. If a variable exists but is set to an empty string, the empty string is used as the value, not the default. This is a common source of confusion. To use the default, the variable must be completely unset (\`unset LOG_LEVEL\`).`,
    difficulty: "edge-case",
  },
  {
    id: "14-Q6",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Which Collector extension provides an HTTP endpoint that container orchestrators like Kubernetes use for liveness and readiness probes?`,
    options: [
      { key: "A", text: `zpages` },
      { key: "B", text: `health_check` },
      { key: "C", text: `pprof` },
      { key: "D", text: `memory_ballast` },
    ],
    correctAnswer: "B",
    explanation: `The \`health_check\` extension provides an HTTP health endpoint (default \`0.0.0.0:13133\`) designed for orchestrator integration. The \`zpages\` extension provides debug pages (tracez, pipelinez) for troubleshooting. The \`pprof\` extension exposes Go profiling data. The \`memory_ballast\` extension is used to reduce garbage collection frequency.`,
    difficulty: "recall",
  },
  {
    id: "14-Q7",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A platform team needs to run two OTLP receivers on different ports, one for frontend services on port 4317 and another for internal backend services on port 4319. How should they configure this in the Collector?`,
    options: [
      { key: "A", text: `Configure two separate receiver types: otlp for one port and otlphttp for the other` },
      { key: "B", text: `Define the same receiver key (otlp) twice with different endpoint values` },
      { key: "C", text: `Use named instances such as otlp/frontend and otlp/backend with distinct endpoints` },
      { key: "D", text: `Configure a single OTLP receiver with a list of multiple endpoint values` },
    ],
    correctAnswer: "C",
    explanation: `All Collector components support named instances using the \`type/name\` pattern. For example, \`otlp/frontend\` and \`otlp/backend\` are two distinct OTLP receiver instances, each with its own configuration. YAML does not allow duplicate keys, so defining \`otlp\` twice would result in only the last definition being used.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q8",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `In the Collector's service section, which subsection configures the Collector's own observability, such as its log level and internal metrics endpoint?`,
    options: [
      { key: "A", text: `pipelines` },
      { key: "B", text: `extensions` },
      { key: "C", text: `diagnostics` },
      { key: "D", text: `telemetry` },
    ],
    correctAnswer: "D",
    explanation: `The \`service.telemetry\` subsection configures the Collector's self-observability. It includes \`logs\` (level, encoding, output paths), \`metrics\` (level, address for the Prometheus endpoint), and \`traces\` (propagators). The \`pipelines\` subsection defines data pipelines for user telemetry. There is no \`diagnostics\` subsection.`,
    difficulty: "recall",
  },
  {
    id: "14-Q9",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An SRE team wants to inject API tokens and backend endpoints into their Collector configuration without hardcoding secrets in the YAML file. What is the recommended approach?`,
    options: [
      { key: "A", text: `Use environment variable substitution with the \${env:API_TOKEN} syntax` },
      { key: "B", text: `Store the secret in a separate secrets.yaml file and reference it with an include directive` },
      { key: "C", text: `Pass the token via the --secret flag on the Collector command line` },
      { key: "D", text: `Embed the token in a base64-encoded configuration source` },
    ],
    correctAnswer: "A",
    explanation: `The Collector's env provider enables environment variable substitution using \`\${env:VAR_NAME}\` or the shorthand \`\${VAR_NAME}\` syntax. This is the recommended approach for injecting secrets such as API tokens, passwords, and endpoints. There is no \`--secret\` flag, \`include\` directive, or base64 configuration source in the Collector.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q10",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team configures the batch processor with \`send_batch_size: 10000\` and \`send_batch_max_size: 5000\`. What happens when the Collector starts?`,
    options: [
      { key: "A", text: `The Collector silently uses send_batch_max_size as the effective trigger size` },
      { key: "B", text: `The Collector fails to start with a validation error` },
      { key: "C", text: `The batch processor ignores send_batch_size and splits all batches at 5000 items` },
      { key: "D", text: `The Collector logs a warning and falls back to default batch processor values` },
    ],
    correctAnswer: "B",
    explanation: `The batch processor validates that \`send_batch_max_size\` must be greater than or equal to \`send_batch_size\` (or zero to disable the maximum). Setting \`send_batch_max_size\` lower than \`send_batch_size\` is an invalid configuration that triggers a validation error at startup, preventing the Collector from running.`,
    difficulty: "edge-case",
  },
  {
    id: "14-Q11",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Which Collector exporter outputs telemetry data to the console and is primarily intended for development and debugging?`,
    options: [
      { key: "A", text: `otlp` },
      { key: "B", text: `file` },
      { key: "C", text: `debug` },
      { key: "D", text: `stdout` },
    ],
    correctAnswer: "C",
    explanation: `The \`debug\` exporter (formerly called the \`logging\` exporter) logs telemetry data to the console. It supports a \`verbosity\` setting (basic, normal, detailed) and sampling options to control output volume. There is no \`stdout\` exporter in the Collector. The \`file\` exporter writes to files, not the console. The \`otlp\` exporter sends data to backends.`,
    difficulty: "recall",
  },
  {
    id: "14-Q12",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Which command validates a Collector configuration file for correctness without actually starting the Collector?`,
    options: [
      { key: "A", text: `otelcol --config config.yaml --test` },
      { key: "B", text: `otelcol start --validate --config config.yaml` },
      { key: "C", text: `otelcol check --config config.yaml` },
      { key: "D", text: `otelcol validate --config config.yaml` },
    ],
    correctAnswer: "D",
    explanation: `The \`otelcol validate --config config.yaml\` command checks the configuration for errors such as invalid component references, malformed YAML, and constraint violations without starting the Collector. This enables CI/CD pipelines and pre-deployment checks. The Collector does not support \`--test\`, \`start --validate\`, or \`check\` subcommands for validation.`,
    difficulty: "recall",
  },
  {
    id: "14-Q13",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What is the default timeout for the batch processor, after which it sends accumulated data regardless of batch size?`,
    options: [
      { key: "A", text: `200ms` },
      { key: "B", text: `1s` },
      { key: "C", text: `5s` },
      { key: "D", text: `10s` },
    ],
    correctAnswer: "A",
    explanation: `The batch processor's default timeout is 200 milliseconds. When this timeout expires, the processor sends whatever data has accumulated, even if the batch has not reached \`send_batch_size\` (default 8192 items). This default balances latency and batching efficiency for most scenarios.`,
    difficulty: "recall",
  },
  {
    id: "14-Q14",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An observability team configures a Collector to scrape its own internal metrics endpoint (localhost:8888) using a Prometheus receiver within the same Collector instance. Why is this considered an anti-pattern?`,
    options: [
      { key: "A", text: `The Prometheus receiver cannot scrape localhost targets` },
      { key: "B", text: `If the Collector crashes, its own metrics become unavailable, creating a monitoring blind spot` },
      { key: "C", text: `The internal metrics endpoint only supports OTLP format and cannot be scraped by a Prometheus receiver` },
      { key: "D", text: `The Collector automatically exports its internal metrics to all configured exporters` },
    ],
    correctAnswer: "B",
    explanation: `Configuring a Collector to scrape its own metrics creates a circular dependency. If the Collector crashes or becomes unreachable, the metrics needed to troubleshoot the problem are lost. The recommended approach is to expose the telemetry endpoint for external scraping by a separate monitoring system or a dedicated Collector layer.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q15",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An engineer writes the environment variable reference \`\${env:my-endpoint}\` in their Collector configuration. What happens when the Collector starts?`,
    options: [
      { key: "A", text: `The variable resolves normally because hyphens are valid in environment variable names` },
      { key: "B", text: `The variable resolves to an empty string and the Collector logs a warning` },
      { key: "C", text: `A validation error occurs because environment variable names cannot contain hyphens` },
      { key: "D", text: `The Collector treats "my" as the variable name and ignores "-endpoint"` },
    ],
    correctAnswer: "C",
    explanation: `The env provider validates that environment variable names match the regex \`^[a-zA-Z_][a-zA-Z0-9_]*\$\`. Names must start with a letter or underscore and can only contain letters, digits, and underscores. Hyphens and periods are not allowed. The correct approach is to use underscores instead, such as \`my_endpoint\`.`,
    difficulty: "edge-case",
  },
  {
    id: "14-Q16",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team wants to generate rate, error, and duration (RED) metrics from distributed traces without adding separate metric instrumentation to their application code. Which Collector component enables this?`,
    options: [
      { key: "A", text: `The transform processor with OTTL statements` },
      { key: "B", text: `The count connector` },
      { key: "C", text: `The probabilistic_sampler processor` },
      { key: "D", text: `The spanmetrics connector` },
    ],
    correctAnswer: "D",
    explanation: `The \`spanmetrics\` connector generates metrics from spans, producing request rate, error rate, and duration histograms. It acts as an exporter in a traces pipeline and a receiver in a metrics pipeline. The \`count\` connector counts items but does not generate RED metrics. The \`transform\` processor modifies existing data but does not generate metrics from traces.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q17",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Which configuration source scheme allows loading the entire Collector configuration from an environment variable?`,
    options: [
      { key: "A", text: `env:, as in otelcol --config env:CONFIG` },
      { key: "B", text: `yaml:, as in otelcol --config yaml:\$CONFIG` },
      { key: "C", text: `shell:, as in otelcol --config shell:CONFIG` },
      { key: "D", text: `inline:, as in otelcol --config inline:CONFIG` },
    ],
    correctAnswer: "A",
    explanation: `The \`env:\` scheme tells the Collector to read the configuration content from the named environment variable (e.g., \`export CONFIG='receivers: ...'\` then \`otelcol --config env:CONFIG\`). The Collector also supports \`file:\` and \`http:\`/\`https:\` schemes for loading configuration. There are no \`yaml:\`, \`shell:\`, or \`inline:\` schemes in the Collector.`,
    difficulty: "recall",
  },
  {
    id: "14-Q18",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An infrastructure team has a Collector configuration that has grown to over 500 lines. They split it into separate files: base.yaml for the service section, receivers.yaml, processors.yaml, and exporters.yaml. How does the Collector combine these files when started with multiple --config flags?`,
    options: [
      { key: "A", text: `It concatenates the files in order and parses them as a single YAML document` },
      { key: "B", text: `It performs a deep merge of configurations, where keys from later sources override earlier sources at each level` },
      { key: "C", text: `It reads only the first file and uses subsequent files as fallbacks for missing keys` },
      { key: "D", text: `It requires an explicit merge directive at the top of each additional configuration file` },
    ],
    correctAnswer: "B",
    explanation: `When the Collector receives multiple configuration sources via \`--config\` flags, it performs a deep merge in the specified order. Keys from later sources override keys from earlier sources at each level of the YAML hierarchy. This enables modular configuration where a base file defines structure and additional files provide or override specific settings.`,
    difficulty: "scenario",
  },
  {
    id: "14-Q19",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An engineer uses the env provider default value syntax \`\${env:ENDPOINT:-http://localhost:8080}\` in their Collector configuration. The default value itself contains a colon. How does the env provider parse this?`,
    options: [
      { key: "A", text: `It treats "http" as a provider scheme reference and returns a parsing error` },
      { key: "B", text: `It splits on the last colon, interpreting only "8080" as the default value` },
      { key: "C", text: `It splits only on the first :- delimiter, correctly using http://localhost:8080 as the default` },
      { key: "D", text: `It requires the default value to be quoted with double quotes to prevent ambiguity` },
    ],
    correctAnswer: "C",
    explanation: `The \`:-\` delimiter in the env provider only splits on the first occurrence. Everything after the first \`:-\` is treated as the default value, even if it contains additional colons. So \`\${env:ENDPOINT:-http://localhost:8080}\` correctly defaults to \`http://localhost:8080\` when the ENDPOINT variable is unset.`,
    difficulty: "edge-case",
  },
  {
    id: "14-Q20",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A Collector is configured with a single OTLP receiver that feeds into two separate trace pipelines: one for sampling and another for full export. When telemetry data enters the shared receiver, what happens to the data?`,
    options: [
      { key: "A", text: `Each pipeline receives a reference to the same data, so modifications in one pipeline affect the other` },
      { key: "B", text: `The first pipeline listed processes data exclusively, and the second receives the output` },
      { key: "C", text: `The Collector distributes data round-robin between the two pipelines to balance load` },
      { key: "D", text: `The data is cloned so that each pipeline receives its own independent copy` },
    ],
    correctAnswer: "D",
    explanation: `When a receiver feeds multiple pipelines, the Collector clones the data so that each pipeline gets its own independent copy. This means processors in one pipeline can modify data without affecting the other pipeline. This cloning behavior ensures pipeline isolation and prevents unintended side effects between pipelines that share a receiver.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q1",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What characterizes the gateway deployment mode of the OpenTelemetry Collector?`,
    options: [
      { key: "A", text: `It runs as a sidecar container alongside each application pod` },
      { key: "B", text: `It runs as a DaemonSet with one instance per Kubernetes node` },
      { key: "C", text: `It runs as a centralized deployment receiving telemetry from multiple agents, handling aggregation, sampling, and routing` },
      { key: "D", text: `It runs as a standalone binary on developer laptops for local testing` },
    ],
    correctAnswer: "C",
    explanation: `Gateway mode is a centralized deployment that receives data from multiple agents or applications. It handles heavier processing such as aggregation, sampling, and routing, with higher resource allocation, and connects to multiple backends. Agent mode runs per-host, and standalone mode is a single-instance deployment for small environments.`,
    difficulty: "recall",
  },
  {
    id: "15-Q2",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A platform team runs a multi-tenant Kubernetes environment where each team's telemetry costs must be tracked separately for chargeback. Which Collector deployment strategy best supports tenant isolation?`,
    options: [
      { key: "A", text: `Sidecar collectors, one per application pod` },
      { key: "B", text: `A single centralized gateway collector for all teams` },
      { key: "C", text: `A DaemonSet collector shared across all pods on each node` },
      { key: "D", text: `A StatefulSet collector with persistent volume claims` },
    ],
    correctAnswer: "A",
    explanation: `Sidecar collectors provide per-pod isolation, keeping each application's telemetry in separate processing pipelines. DaemonSet collectors mix telemetry from multiple applications in the same batch, making chargeback and tenant isolation difficult. A centralized gateway offers no isolation at all. Sidecars or per-namespace collectors are the right choice when strict separation is required.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q3",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `How does the OpenTelemetry Operator inject a sidecar Collector into an application pod?`,
    options: [
      { key: "A", text: `By modifying the application's container image at build time` },
      { key: "B", text: `By detecting an annotation on the pod such as \`sidecar.opentelemetry.io/inject: "true"\`` },
      { key: "C", text: `By replacing the pod's network namespace with a shared one` },
      { key: "D", text: `By adding a CronJob that periodically restarts pods with the Collector container` },
    ],
    correctAnswer: "B",
    explanation: `The Operator uses an admission webhook that watches for pods annotated with \`sidecar.opentelemetry.io/inject: "true"\`. When detected, it injects the sidecar Collector container into the pod spec. The annotation value can also specify a particular collector name or a namespace/name combination.`,
    difficulty: "recall",
  },
  {
    id: "15-Q4",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An SRE team implements tail sampling in the OpenTelemetry Collector. They deploy three tail-sampling instances behind a standard Kubernetes Service, but traces in the backend appear incomplete with missing spans. What is the most likely cause?`,
    options: [
      { key: "A", text: `The tail-sampling processor does not support multiple instances` },
      { key: "B", text: `The batch processor is dropping spans before they reach the sampler` },
      { key: "C", text: `The sampling decision wait time is too short for slow services` },
      { key: "D", text: `Spans from the same trace are landing on different sampling instances because there is no trace-aware load balancing` },
    ],
    correctAnswer: "D",
    explanation: `Tail sampling requires all spans from the same trace to reach the same Collector instance so it can make a complete sampling decision. Without a load-balancing exporter that routes by trace ID, a standard Kubernetes Service distributes spans randomly across instances. This causes each instance to see only a partial trace, leading to inconsistent decisions and broken traces in the backend.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q5",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `When creating an OpenTelemetryCollector custom resource without specifying a mode, what deployment mode does the Operator use by default?`,
    options: [
      { key: "A", text: `deployment` },
      { key: "B", text: `daemonset` },
      { key: "C", text: `sidecar` },
      { key: "D", text: `statefulset` },
    ],
    correctAnswer: "A",
    explanation: `The OpenTelemetryCollector CRD defaults to \`deployment\` mode when no mode is specified. This creates a Kubernetes Deployment, a Service, and a ConfigMap. Other available modes are \`daemonset\`, \`statefulset\`, and \`sidecar\`, each creating different Kubernetes resources appropriate for their use case.`,
    difficulty: "recall",
  },
  {
    id: "15-Q6",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Which Kubernetes scheduling features are NOT supported when the OpenTelemetryCollector CRD is set to sidecar mode?`,
    options: [
      { key: "A", text: `Resource requests and limits` },
      { key: "B", text: `Tolerations, pod affinity, additional containers, and priority class` },
      { key: "C", text: `Environment variables from ConfigMaps and Secrets` },
      { key: "D", text: `Volume mounts for TLS certificates` },
    ],
    correctAnswer: "B",
    explanation: `Sidecar mode injects a Collector container into an existing pod rather than creating its own pod. Because of this, pod-level scheduling features such as tolerations, affinity rules, additional containers, and priority class names are not supported in sidecar mode. These features are controlled by the host pod's spec, not the injected sidecar. Resource requests and limits, environment variables, and volume mounts are supported.`,
    difficulty: "edge-case",
  },
  {
    id: "15-Q7",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An operations team profiles their OpenTelemetry Collector and discovers excessive memory consumption from transitive dependencies they do not use. Their Collector uses the contrib distribution. What should they do to reduce the binary size and memory footprint?`,
    options: [
      { key: "A", text: `Increase the memory limit to accommodate the unused dependencies` },
      { key: "B", text: `Add a memory_limiter processor to cap memory usage at runtime` },
      { key: "C", text: `Switch to the core distribution, which contains fewer components` },
      { key: "D", text: `Build a custom distribution using OCB with only the components they need` },
    ],
    correctAnswer: "D",
    explanation: `The OpenTelemetry Collector Builder (OCB) generates custom distributions containing only specified components. A custom distribution can be as small as 28 MB compared to 350 MB for the contrib distribution. Beyond binary size, unused components consume memory through transitive dependencies and init functions. Building a tailored distribution reduces both the attack surface and the resource footprint.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q8",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What are the three steps the OpenTelemetry Collector Builder (OCB) executes during a build?`,
    options: [
      { key: "A", text: `Generate Go source files, download Go modules, compile the binary` },
      { key: "B", text: `Validate manifest, transform configuration, deploy binary` },
      { key: "C", text: `Parse YAML, optimize dependencies, link libraries` },
      { key: "D", text: `Download components, package sources, create container image` },
    ],
    correctAnswer: "A",
    explanation: `OCB executes three sequential steps: first it generates Go source files (main.go, components.go) from templates based on the manifest; then it runs \`go mod tidy\` to download module dependencies; finally it compiles everything into a single binary using \`go build\`. Each step can be skipped individually using flags like \`--skip-compilation\` or \`--skip-generate\`.`,
    difficulty: "recall",
  },
  {
    id: "15-Q9",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A growing organization with multiple departments starts experiencing configuration conflicts because all teams share a single centralized Collector cluster. Each department wants control over their own processing rules. What architecture change best addresses this?`,
    options: [
      { key: "A", text: `Add separate pipelines within the same Collector for each department` },
      { key: "B", text: `Replace all Collectors with sidecars in every pod` },
      { key: "C", text: `Move to a decentralized model where each department runs its own Collector cluster, forwarding data to a central observability cluster` },
      { key: "D", text: `Deploy one Collector per Kubernetes namespace using StatefulSets` },
    ],
    correctAnswer: "C",
    explanation: `The decentralized model gives each department ownership of their own Collector configuration while a central observability team manages the shared downstream fleet for cross-cutting concerns like sampling, routing, and backend connectivity. This separation allows departments to move independently without configuration conflicts. Organizations should adopt this model only when they have the operational capacity for it; the centralized model remains the right starting point.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q10",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What is the purpose of the StatefulSet deployment mode in the OpenTelemetryCollector CRD?`,
    options: [
      { key: "A", text: `To deploy one Collector per node in the cluster` },
      { key: "B", text: `To provide persistent storage, ordered deployment, and stable network identities` },
      { key: "C", text: `To inject a Collector as a sidecar into application pods` },
      { key: "D", text: `To enable automatic horizontal scaling of Collector instances` },
    ],
    correctAnswer: "B",
    explanation: `StatefulSet mode creates a Kubernetes StatefulSet with support for volumeClaimTemplates, which provides persistent storage that survives pod restarts. It also provides ordered deployment and stable network identities. This is useful for scenarios like persistent queues, file-based buffering, or the write-ahead log (WAL) mechanism for resilience. It creates a headless Service, ConfigMap, and PVCs.`,
    difficulty: "recall",
  },
  {
    id: "15-Q11",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A platform team experiences sudden traffic spikes during major marketing events that overwhelm their Collector's processing capacity. They want the ingestion layer to absorb bursts without requiring immediate horizontal scaling of the processing layer. What architecture pattern addresses this?`,
    options: [
      { key: "A", text: `Deploy a StatefulSet Collector with persistent volume claims for disk buffering` },
      { key: "B", text: `Replace the Collector with direct SDK-to-backend export to reduce latency` },
      { key: "C", text: `Add more replicas of the processing Collector behind a standard load balancer` },
      { key: "D", text: `Insert Apache Kafka between a publisher Collector layer and a subscriber Collector layer` },
    ],
    correctAnswer: "D",
    explanation: `Using Kafka as a buffer between two Collector layers decouples ingestion from processing. The publisher layer scales horizontally to absorb traffic spikes and pushes data into Kafka topics (one per signal type). The subscriber layer pulls from Kafka at a controlled rate, processing data steadily without needing to be sized for peak traffic. This eliminates the need for aggressive HPA scaling of processing instances.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q12",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What discovery mechanism does the load-balancing exporter use when deployed on Kubernetes with a headless Service to find backend Collector instances?`,
    options: [
      { key: "A", text: `DNS resolution of the headless Service to discover individual pod IP addresses` },
      { key: "B", text: `The Kubernetes API to watch for pod lifecycle events` },
      { key: "C", text: `A static list of IP addresses configured in the exporter` },
      { key: "D", text: `Consul service discovery with health checking` },
    ],
    correctAnswer: "A",
    explanation: `The load-balancing exporter supports multiple resolver types. When using DNS resolution with a headless Kubernetes Service, the exporter queries DNS A records to discover the IP addresses of all pods backing the Service. It periodically re-resolves DNS (every five seconds by default) to detect autoscaling events. Other available resolvers include static host lists, Kubernetes API watches, and AWS Cloud Map.`,
    difficulty: "recall",
  },
  {
    id: "15-Q13",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Why must log collection from host file systems use a DaemonSet deployment rather than a horizontally scaled Deployment?`,
    options: [
      { key: "A", text: `Log files on a node can only be read by processes running on that same node` },
      { key: "B", text: `The filelog receiver requires hostNetwork access, which only DaemonSets support` },
      { key: "C", text: `Running multiple log Collectors reading the same host files would produce duplicate log entries in the backend` },
      { key: "D", text: `Kubernetes does not allow Deployment pods to mount host paths` },
    ],
    correctAnswer: "C",
    explanation: `Log collection from host file systems follows a single-writer constraint: each log file must be read by exactly one Collector to avoid duplicates. A DaemonSet guarantees one Collector per node, ensuring each node's logs are collected once. If a Deployment with multiple replicas were used, multiple pods could be scheduled on the same node, each reading the same log files and producing duplicate entries. The same single-writer constraint applies to Prometheus-style metric scraping.`,
    difficulty: "edge-case",
  },
  {
    id: "15-Q14",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team deploys a DaemonSet Collector on Kubernetes, but they notice that pods on node A are sometimes sending telemetry to the DaemonSet Collector on node B instead of their local instance. What is most likely missing?`,
    options: [
      { key: "A", text: `The Collector needs a StatefulSet identity to bind to specific nodes` },
      { key: "B", text: `The Service fronting the DaemonSet needs \`internalTrafficPolicy: Local\` or pods need to target the local Collector directly to ensure same-node routing` },
      { key: "C", text: `The OTLP receiver must be configured with \`hostNetwork: true\`` },
      { key: "D", text: `The batch processor is routing data across node boundaries` },
    ],
    correctAnswer: "B",
    explanation: `By default, a Kubernetes Service distributes traffic across all endpoints, including DaemonSet pods on other nodes. To ensure pods communicate only with the DaemonSet Collector on their own node, operators can set \`internalTrafficPolicy: Local\` on the Service (Kubernetes 1.21+), use the Downward API to inject the node IP, or configure pods to target localhost. Without this, telemetry crosses node boundaries, adding latency and defeating the purpose of per-node collection.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q15",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What are the two upgrade strategy options available in the OpenTelemetryCollector CRD?`,
    options: [
      { key: "A", text: `rolling and bluegreen` },
      { key: "B", text: `canary and recreate` },
      { key: "C", text: `fast and careful` },
      { key: "D", text: `automatic and none` },
    ],
    correctAnswer: "D",
    explanation: `The OpenTelemetryCollector CRD supports two upgrade strategies. The \`automatic\` strategy (the default) lets the Operator manage image versions automatically, upgrading the Collector when the Operator itself is updated. The \`none\` strategy disables automatic image management, which is necessary when running custom Collector images built with OCB, since the Operator should not override the custom image.`,
    difficulty: "recall",
  },
  {
    id: "15-Q16",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `When the number of tail-sampling Collector instances changes during an autoscaling event, approximately what percentage of trace IDs get reassigned when using the load-balancing exporter's consistent hashing algorithm?`,
    options: [
      { key: "A", text: `About 30%` },
      { key: "B", text: `About 50%` },
      { key: "C", text: `About 75%` },
      { key: "D", text: `100%, since all hashes must be recalculated` },
    ],
    correctAnswer: "A",
    explanation: `The load-balancing exporter uses consistent hashing rather than a naive hash-mod-n approach. With consistent hashing, when the number of backend instances changes, only about 30% of trace IDs get reassigned to different instances. A naive modulo-based approach would reassign nearly 100% of trace IDs, causing widespread disruption to in-flight sampling decisions and potentially breaking many traces.`,
    difficulty: "edge-case",
  },
  {
    id: "15-Q17",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team runs a single Collector handling traces, metrics, and logs in one instance. A bug in the metrics pipeline causes the Collector to crash, taking down trace and log collection as well. What deployment strategy prevents this cross-signal failure?`,
    options: [
      { key: "A", text: `Adding a memory_limiter processor to each pipeline` },
      { key: "B", text: `Switching from gateway mode to agent mode` },
      { key: "C", text: `Separating Collectors by signal type so that traces, metrics, and logs each run in independent Collector instances` },
      { key: "D", text: `Using a routing connector to isolate signals within the same Collector process` },
    ],
    correctAnswer: "C",
    explanation: `Per-signal separation deploys independent Collector instances for each signal type. This provides fault isolation: a failure in the metrics Collector does not affect the trace or log Collectors. This approach also allows independent scaling strategies, since OTLP-based trace ingestion scales horizontally as stateless deployments, while Prometheus metric scraping requires target-aware allocation, and file-based log collection requires DaemonSets.`,
    difficulty: "scenario",
  },
  {
    id: "15-Q18",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What happens if an OCB manifest file does not include any configuration providers (such as fileprovider or envprovider)?`,
    options: [
      { key: "A", text: `The build fails with a missing dependency compilation error` },
      { key: "B", text: `The build succeeds but the resulting binary cannot load any configuration file` },
      { key: "C", text: `The binary falls back to a built-in default configuration` },
      { key: "D", text: `The binary automatically detects and uses the file provider` },
    ],
    correctAnswer: "B",
    explanation: `Configuration providers are compiled into the Collector binary and handle resolving configuration sources like \`file:\` and \`env:\` URI schemes. Without any providers in the manifest, OCB generates and compiles the binary successfully, but the resulting Collector has no way to load a configuration file at runtime. The OCB documentation lists this as an anti-pattern and recommends including at minimum the file and env providers.`,
    difficulty: "recall",
  },
  {
    id: "15-Q19",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Starting with operator v0.140.0, what Go runtime optimization does the OpenTelemetry Operator automatically apply to Collector pods?`,
    options: [
      { key: "A", text: `It sets GOGC to 50 to reduce garbage collection pause times` },
      { key: "B", text: `It sets GORACE to enable race condition detection in production` },
      { key: "C", text: `It sets GOMEMLIMIT and GOMAXPROCS based on the pod's resource configuration` },
      { key: "D", text: `It enables continuous CPU profiling by default via the pprof extension` },
    ],
    correctAnswer: "C",
    explanation: `As of v0.140.0, the Operator automatically sets the \`GOMEMLIMIT\` and \`GOMAXPROCS\` environment variables based on the pod's resource limits. \`GOMEMLIMIT\` optimizes Go's garbage collector to be aware of the container's memory limit, reducing OOM risk. \`GOMAXPROCS\` aligns the Go runtime's thread count with the available CPU, preventing excessive context switching. This is a Beta feature gate (\`operator.golang.flags\`) enabled by default.`,
    difficulty: "edge-case",
  },
  {
    id: "15-Q20",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team wants their gateway Collector to automatically scale between 2 and 10 replicas based on resource utilization. Which approach does the OpenTelemetryCollector CRD support for this?`,
    options: [
      { key: "A", text: `The replicas field accepts a range notation like "2-10"` },
      { key: "B", text: `A separate HorizontalPodAutoscaler resource must be created and managed manually` },
      { key: "C", text: `The resources section includes automatic scaling thresholds` },
      { key: "D", text: `The autoscaler section with minReplicas, maxReplicas, and target CPU and memory utilization percentages` },
    ],
    correctAnswer: "D",
    explanation: `The OpenTelemetryCollector CRD includes a built-in \`autoscaler\` section that configures horizontal pod autoscaling. It supports \`minReplicas\`, \`maxReplicas\`, \`targetCPUUtilization\`, and \`targetMemoryUtilization\` fields. The Operator creates and manages the HorizontalPodAutoscaler resource automatically based on these settings, so teams do not need to create or maintain a separate HPA.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q1",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What determines the signal type that a Collector pipeline handles?`,
    options: [
      { key: "A", text: `The signal type of the first receiver listed in the pipeline` },
      { key: "B", text: `The pipeline key name under the service.pipelines section (e.g., traces, metrics, or logs)` },
      { key: "C", text: `The signal type supported by the majority of exporters in the pipeline` },
      { key: "D", text: `An explicit signal_type field in the pipeline configuration` },
    ],
    correctAnswer: "B",
    explanation: `Each pipeline is declared under a key that specifies the signal type: \`traces\`, \`metrics\`, or \`logs\`. The key name determines which data types flow through the pipeline. Named pipelines use the format \`signal/name\` (e.g., \`traces/critical\`), where the part before the slash is the signal type.`,
    difficulty: "recall",
  },
  {
    id: "16-Q2",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What is the naming convention for creating multiple pipelines of the same signal type in the Collector?`,
    options: [
      { key: "A", text: `traces-1, traces-2 (hyphen-separated numeric suffixes)` },
      { key: "B", text: `traces.critical, traces.standard (dot-separated names)` },
      { key: "C", text: `traces[critical], traces[standard] (bracket notation)` },
      { key: "D", text: `traces/critical, traces/standard (slash-separated names)` },
    ],
    correctAnswer: "D",
    explanation: `Named pipelines use the \`signal/name\` format, where the signal type is followed by a slash and a descriptive name. For example, \`traces/critical\` and \`traces/standard\` create two separate trace pipelines with independent receiver, processor, and exporter configurations. This pattern mirrors the naming convention used for component instances (e.g., \`otlp/frontend\`).`,
    difficulty: "recall",
  },
  {
    id: "16-Q3",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An SRE team is migrating from multiple tracing formats to a unified backend. Their services currently emit traces in Jaeger Thrift, Zipkin, and OTLP formats. They want a single Collector to accept all three formats and export to one OTLP backend. How should they configure the pipeline?`,
    options: [
      { key: "A", text: `Define one traces pipeline with three receivers (otlp, jaeger, zipkin) and one OTLP exporter` },
      { key: "B", text: `Define three separate traces pipelines, one for each receiver format, each with its own OTLP exporter` },
      { key: "C", text: `Define one traces pipeline with a single otlp receiver configured with three protocol adapters` },
      { key: "D", text: `Use a routing connector to merge the three formats before exporting` },
    ],
    correctAnswer: "A",
    explanation: `A single pipeline supports fan-in from multiple receivers. Listing \`[otlp, jaeger, zipkin]\` in the receivers field merges all incoming data into the same processing chain. The Collector internally converts all formats to its OTLP-aligned internal representation. While separate pipelines would work, a single pipeline with multiple receivers is simpler when the same processing applies to all sources.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q4",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `In what order are processors executed within a Collector pipeline?`,
    options: [
      { key: "A", text: `In alphabetical order based on processor names` },
      { key: "B", text: `In parallel, since processors are independent of each other` },
      { key: "C", text: `In the exact order they are listed in the pipeline configuration` },
      { key: "D", text: `The Collector automatically determines optimal execution order` },
    ],
    correctAnswer: "C",
    explanation: `Processors execute sequentially in the exact order specified in the pipeline's \`processors\` list. This order matters for correctness and performance. For example, placing \`memory_limiter\` first ensures back-pressure is applied before other processors consume memory, and placing \`batch\` last ensures efficient export after data reduction steps like filtering and sampling.`,
    difficulty: "recall",
  },
  {
    id: "16-Q5",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team configures their trace pipeline with the processor list \`[batch, tail_sampling, memory_limiter]\`. What is the primary problem with this ordering?`,
    options: [
      { key: "A", text: `The tail_sampling processor is incompatible with the batch processor in the same pipeline` },
      { key: "B", text: `The memory_limiter should be first to prevent OOM, and batch should be last since sampling reduces data volume before batching` },
      { key: "C", text: `The batch processor requires at least 8192 items, which tail_sampling cannot guarantee` },
      { key: "D", text: `Nothing is wrong; the Collector automatically reorders processors to ensure correctness` },
    ],
    correctAnswer: "B",
    explanation: `The recommended processor order places \`memory_limiter\` first to protect against out-of-memory conditions, followed by data transformations and sampling, with \`batch\` last. Placing \`batch\` before \`tail_sampling\` means data is batched and then partially discarded by sampling, wasting batching effort. Placing \`memory_limiter\` last means the Collector has no protection during the processing stages that consume the most memory. The correct order would be \`[memory_limiter, tail_sampling, batch]\`.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q6",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A platform team wants to send all trace data to both a primary observability backend and a secondary archival storage system, using the same processing for both. How should they configure this in the Collector?`,
    options: [
      { key: "A", text: `Define two separate trace pipelines, each with its own copy of the receiver` },
      { key: "B", text: `Use a forward connector to duplicate data between two pipelines` },
      { key: "C", text: `Configure a routing connector to copy traces to both destinations` },
      { key: "D", text: `List both exporters in a single pipeline's exporters field` },
    ],
    correctAnswer: "D",
    explanation: `Fan-out is achieved by listing multiple exporters in a single pipeline (e.g., \`exporters: [otlp/primary, otlp/archive]\`). The Collector sends a copy of each batch to every exporter listed in the pipeline. This is simpler than using connectors or separate pipelines when the same data and processing apply to both destinations.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q7",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `How does the routing connector differ from the spanmetrics connector in terms of signal handling?`,
    options: [
      { key: "A", text: `The routing connector routes the same signal type to different pipelines, while the spanmetrics connector transforms traces into metrics` },
      { key: "B", text: `The routing connector transforms signals between types, while the spanmetrics connector only routes metrics` },
      { key: "C", text: `Both connectors perform the same function but for different signal types` },
      { key: "D", text: `The routing connector only handles logs, while the spanmetrics connector handles all three signal types` },
    ],
    correctAnswer: "A",
    explanation: `The routing connector performs same-signal routing (traces to traces, metrics to metrics, logs to logs) based on OTTL conditions without modifying the data. The spanmetrics connector performs cross-signal transformation, consuming trace spans and producing metrics (calls, duration, and optionally events). This is a fundamental distinction: routing connectors direct data flow, while transforming connectors create new signal data from existing signals.`,
    difficulty: "recall",
  },
  {
    id: "16-Q8",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An operations team wants to route error logs (severity ERROR and above) to an expensive, fast-query backend and lower-severity logs to cheap long-term storage. Both streams come from the same OTLP receiver. Which approach best accomplishes this?`,
    options: [
      { key: "A", text: `Use a filter processor to drop non-error logs before the expensive exporter` },
      { key: "B", text: `Use two separate Collector instances, one for each log severity level` },
      { key: "C", text: `Use the routing connector with a log context condition based on severity_number to direct logs to different pipelines` },
      { key: "D", text: `Use the transform processor to add a routing attribute and let the exporter decide` },
    ],
    correctAnswer: "C",
    explanation: `The routing connector with \`context: log\` and a severity-based condition (e.g., \`condition: severity_number >= SEVERITY_NUMBER_ERROR\`) routes individual log records to different downstream pipelines. Error logs go to the fast-query pipeline while lower-severity logs go to cheap storage. Unlike a filter processor, the routing connector preserves all data by directing it to appropriate destinations rather than dropping it.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q9",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team configures the routing connector with routing rules for three known tenants but does not configure \`default_pipelines\`. A fourth, unexpected tenant begins sending telemetry. What happens to the fourth tenant's data?`,
    options: [
      { key: "A", text: `It is routed to all three tenant pipelines simultaneously` },
      { key: "B", text: `It is silently dropped because no default pipeline is configured` },
      { key: "C", text: `It causes the Collector to return an error and reject the entire batch` },
      { key: "D", text: `It is held in a buffer until a matching routing rule is added` },
    ],
    correctAnswer: "B",
    explanation: `When the routing connector has no \`default_pipelines\` configured and telemetry does not match any routing rule, the data is silently dropped. This is a common oversight in multi-tenant configurations. Best practice is to always configure \`default_pipelines\` to catch unmatched telemetry and prevent silent data loss.`,
    difficulty: "edge-case",
  },
  {
    id: "16-Q10",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What are the metrics generated by the spanmetrics connector by default (without enabling optional features)?`,
    options: [
      { key: "A", text: `rate, errors, and latency` },
      { key: "B", text: `throughput, failures, and response_time` },
      { key: "C", text: `requests, errors, and p99_duration` },
      { key: "D", text: `calls (a monotonic Sum counting spans) and duration (a Histogram of span durations)` },
    ],
    correctAnswer: "D",
    explanation: `The spanmetrics connector generates two metrics by default: \`calls\` (a monotonic Sum counting the number of spans per unique dimension combination, which includes both successful and errored spans) and \`duration\` (a Histogram of span durations calculated as end_time minus start_time). An optional \`events\` metric can be enabled separately via the \`events.enabled\` configuration. The default metric namespace is \`traces.span.metrics\`.`,
    difficulty: "recall",
  },
  {
    id: "16-Q11",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A development team wants to monitor request rates, error rates, and latency distributions for their microservices without adding metric instrumentation to application code. They already have distributed tracing configured. How should they set up the Collector pipelines?`,
    options: [
      { key: "A", text: `Add the spanmetrics connector as an exporter in the traces pipeline and as a receiver in a metrics pipeline` },
      { key: "B", text: `Add the transform processor to the traces pipeline to convert span attributes into metric data points` },
      { key: "C", text: `Configure the count connector to count spans and calculate durations per service` },
      { key: "D", text: `Use the batch processor with metric aggregation enabled on the traces pipeline` },
    ],
    correctAnswer: "A",
    explanation: `The spanmetrics connector generates RED (Request rate, Error rate, Duration) metrics from trace spans. In the pipeline configuration, it is listed as an exporter in the traces pipeline (e.g., \`exporters: [spanmetrics, otlp/traces]\`) and as a receiver in a metrics pipeline (e.g., \`receivers: [spanmetrics]\`). This requires no application code changes since it derives metrics from existing trace data. The traces can still be exported to a trace backend alongside the connector.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q12",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `How does a connector participate in the Collector's pipeline wiring?`,
    options: [
      { key: "A", text: `It is listed only in the receivers field of the output pipeline` },
      { key: "B", text: `It is listed only in the exporters field of the input pipeline` },
      { key: "C", text: `It is listed as an exporter in the input pipeline and as a receiver in the output pipeline` },
      { key: "D", text: `It is listed in a dedicated connectors subsection within the service.pipelines block` },
    ],
    correctAnswer: "C",
    explanation: `A connector bridges two pipelines by appearing in the \`exporters\` list of the source (input) pipeline and in the \`receivers\` list of the destination (output) pipeline. For example, the spanmetrics connector appears in \`exporters: [spanmetrics]\` in the traces pipeline and \`receivers: [spanmetrics]\` in the metrics pipeline. This dual role as both exporter and receiver is the defining characteristic of connectors.`,
    difficulty: "recall",
  },
  {
    id: "16-Q13",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A fintech company processes both high-priority payment traces and high-volume health check traces. They want to apply tail sampling only to health check traces while forwarding payment traces without sampling. Both types arrive through the same OTLP receiver. How should they structure the pipelines?`,
    options: [
      { key: "A", text: `Use a single pipeline with the tail_sampling processor configured to skip payment-related spans` },
      { key: "B", text: `Use a routing connector to direct traces into two named pipelines: one with sampling for health checks and one without for payments` },
      { key: "C", text: `Use the filter processor to drop health check traces entirely before exporting` },
      { key: "D", text: `Use two separate Collector instances listening on different ports` },
    ],
    correctAnswer: "B",
    explanation: `The routing connector can evaluate resource or span attributes to direct traces into separate named pipelines (e.g., \`traces/payments\` and \`traces/healthchecks\`). The payment pipeline skips sampling, while the health check pipeline includes \`tail_sampling\`. This provides different processing paths while keeping a single Collector deployment. A single pipeline with conditional sampling is less maintainable, and dropping health check traces entirely loses useful data.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q14",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team configures the routing connector with \`context: request\` and uses a \`statement\` field (e.g., \`statement: route() where request["X-Tenant"] == "acme"\`) instead of a \`condition\` field. What happens when the Collector starts?`,
    options: [
      { key: "A", text: `The statement is automatically converted to a condition for the request context` },
      { key: "B", text: `The routing works normally because statement and condition are interchangeable` },
      { key: "C", text: `The routing ignores the statement and routes all data to default_pipelines` },
      { key: "D", text: `The Collector fails to start with a validation error because request context requires a condition, not a statement` },
    ],
    correctAnswer: "D",
    explanation: `The request context in the routing connector only supports \`condition\`, not \`statement\`. This is because the request context uses a limited grammar that only supports simple equality (\`==\`) and inequality (\`!=\`) comparisons on HTTP headers or gRPC metadata. The full OTTL \`route() where ...\` syntax used by \`statement\` is not available in request context. Using \`statement\` with request context produces a validation error at startup.`,
    difficulty: "edge-case",
  },
  {
    id: "16-Q15",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team deploys the spanmetrics connector and adds \`user.id\` and \`request.id\` as custom dimensions. After a few hours in production, the Collector's memory usage grows rapidly and unboundedly. What is the most likely cause, and which configuration option mitigates it?`,
    options: [
      { key: "A", text: `High-cardinality dimensions create millions of unique metric series; setting aggregation_cardinality_limit caps the number of tracked combinations` },
      { key: "B", text: `The metrics_flush_interval is too long, causing excessive metric buffering; reducing it flushes metrics more frequently` },
      { key: "C", text: `The histogram buckets are too granular, creating excessive data points; using exponential histograms reduces memory` },
      { key: "D", text: `The resource_metrics_cache_size is too small, causing cache eviction and reallocation; increasing it stabilizes memory usage` },
    ],
    correctAnswer: "A",
    explanation: `Attributes like \`user.id\` and \`request.id\` are unique per user or per request, producing millions of unique dimension combinations and consuming unbounded memory. The \`aggregation_cardinality_limit\` option caps the maximum number of unique combinations tracked. When the limit is reached, new combinations are dropped and recorded under a special entry with \`otel.metric.overflow="true"\`. Best practice is to avoid high-cardinality dimensions entirely and use low-cardinality attributes like \`user.tier\` or \`region\` instead.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q16",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What is the default OTTL context used by the routing connector when no context field is specified in a routing table entry?`,
    options: [
      { key: "A", text: `log` },
      { key: "B", text: `span` },
      { key: "C", text: `resource` },
      { key: "D", text: `request` },
    ],
    correctAnswer: "C",
    explanation: `The routing connector defaults to \`resource\` context when no \`context\` field is specified. Resource context evaluates conditions against resource attributes and processes data per resource bundle (e.g., all spans or logs from one service instance). This is both the most common and most efficient context for routing decisions based on attributes like \`service.name\` or \`deployment.environment\`, since it evaluates once per resource rather than once per individual signal.`,
    difficulty: "recall",
  },
  {
    id: "16-Q17",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An engineer configures the routing connector with \`error_mode: propagate\` (the default). One of the OTTL conditions references an attribute that does not exist on some incoming telemetry. What happens to telemetry that triggers this OTTL evaluation error?`,
    options: [
      { key: "A", text: `The telemetry is routed to default_pipelines as a fallback` },
      { key: "B", text: `The Collector logs a warning and continues processing the telemetry normally` },
      { key: "C", text: `The telemetry is silently dropped without any log message` },
      { key: "D", text: `The entire payload containing that telemetry is dropped because the connector returns an error to the pipeline` },
    ],
    correctAnswer: "D",
    explanation: `With \`error_mode: propagate\` (the default), OTTL errors cause the connector to return an error, which drops the entire payload from the Collector. This can lead to unexpected data loss when conditions reference attributes that are not consistently present across all telemetry sources. Setting \`error_mode: ignore\` instead logs the error and routes the telemetry to \`default_pipelines\`, which is the recommended setting for production environments.`,
    difficulty: "scenario",
  },
  {
    id: "16-Q18",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team configures the spanmetrics connector with both \`histogram.explicit.buckets\` and \`histogram.exponential.max_size\` in the same configuration block. What happens when the Collector starts?`,
    options: [
      { key: "A", text: `The Collector uses exponential histograms and silently ignores the explicit bucket configuration` },
      { key: "B", text: `The Collector fails to start with a validation error because both histogram types cannot be used simultaneously` },
      { key: "C", text: `The Collector uses explicit histograms for durations under 1 second and exponential histograms for longer durations` },
      { key: "D", text: `The Collector generates two separate duration metrics, one explicit and one exponential` },
    ],
    correctAnswer: "B",
    explanation: `The spanmetrics connector does not allow configuring both explicit and exponential histogram types simultaneously. The Collector validates this at startup and fails with the error "use either 'explicit' or 'exponential' buckets histogram." Teams must choose one based on their use case: explicit histograms for known latency distributions with precise SLO bucket boundaries, or exponential histograms for widely varying latency ranges where bucket distribution is unknown in advance.`,
    difficulty: "edge-case",
  },
  {
    id: "16-Q19",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `In the routing connector's routing table, what is the difference between \`action: copy\` (the default) and \`action: move\`?`,
    options: [
      { key: "A", text: `copy sends data synchronously while move sends data asynchronously to the target pipelines` },
      { key: "B", text: `copy creates a deep clone of the data while move passes a zero-copy reference` },
      { key: "C", text: `copy keeps the data available for subsequent routing rules and default_pipelines, while move removes it from further evaluation` },
      { key: "D", text: `copy sends data to all pipelines listed in the route while move sends to only the first pipeline` },
    ],
    correctAnswer: "C",
    explanation: `With \`action: copy\` (the default), matched data is copied to the target pipelines while remaining available for evaluation against subsequent routing rules and \`default_pipelines\`. With \`action: move\`, matched data is moved to the target pipelines and removed from further processing. It will not be evaluated against subsequent routes or sent to default pipelines. The \`move\` action is useful for creating mutually exclusive routing rules where each piece of telemetry should go to exactly one destination.`,
    difficulty: "edge-case",
  },
  {
    id: "16-Q20",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What are the default dimensions included in all metrics generated by the spanmetrics connector?`,
    options: [
      { key: "A", text: `service.name, span.name, span.kind, and status.code` },
      { key: "B", text: `service.name, http.method, http.status_code, and http.route` },
      { key: "C", text: `service.name, span.name, duration, and error.type` },
      { key: "D", text: `service.name, service.namespace, deployment.environment, and span.name` },
    ],
    correctAnswer: "A",
    explanation: `The spanmetrics connector includes four default dimensions in all generated metrics: \`service.name\` (from resource attributes), \`span.name\` (the span operation name), \`span.kind\` (e.g., SPAN_KIND_SERVER, SPAN_KIND_CLIENT), and \`status.code\` (e.g., STATUS_CODE_OK, STATUS_CODE_ERROR). Additional dimensions can be configured using the \`dimensions\` field. Attributes like \`http.method\` or \`http.status_code\` must be explicitly added as custom dimensions. The \`exclude_dimensions\` field can remove defaults if they are not needed.`,
    difficulty: "recall",
  },
  {
    id: "17-Q1",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What is the default value of \`error_mode\` in the transform processor?`,
    options: [
      { key: "A", text: `ignore` },
      { key: "B", text: `silent` },
      { key: "C", text: `propagate` },
      { key: "D", text: `drop` },
    ],
    correctAnswer: "C",
    explanation: `The transform processor defaults to \`error_mode: propagate\`, which returns errors up the pipeline and causes the entire payload to be dropped when a statement fails. This is why the documentation recommends switching to \`ignore\` for production deployments.`,
    difficulty: "recall",
  },
  {
    id: "17-Q2",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A platform team configures the filter processor with the condition \`name == "health_check"\` under \`traces.span\`. What happens to spans matching this condition?`,
    options: [
      { key: "A", text: `They are dropped from the pipeline` },
      { key: "B", text: `They are kept, and all non-matching spans are dropped` },
      { key: "C", text: `They are tagged with a "filtered" attribute for downstream processing` },
      { key: "D", text: `They are routed to a separate pipeline for health check analysis` },
    ],
    correctAnswer: "A",
    explanation: `In the filter processor, when any OTTL condition evaluates to true, the matching telemetry is dropped. Conditions within the same signal section are OR'd together, meaning any match causes the drop.`,
    difficulty: "scenario",
  },
  {
    id: "17-Q3",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `Which hashing algorithm does the attributes processor use for the \`hash\` action?`,
    options: [
      { key: "A", text: `MD5` },
      { key: "B", text: `SHA-256` },
      { key: "C", text: `SHA-1` },
      { key: "D", text: `CRC-32` },
    ],
    correctAnswer: "B",
    explanation: `The attributes processor uses SHA-256 for the hash action. The hash is computed as a lowercase hexadecimal string and replaces the original attribute value. The result is deterministic, so the same input always produces the same hash, but it cannot be reversed.`,
    difficulty: "recall",
  },
  {
    id: "17-Q4",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An observability team needs to rename a resource attribute from \`k8s-cluster\` to \`k8s.cluster.name\` using the transform processor. Which approach produces the correct result?`,
    options: [
      { key: "A", text: `\`replace_all_patterns(resource.attributes, "key", "k8s-cluster", "k8s.cluster.name")\`` },
      { key: "B", text: `\`rename_key(resource.attributes, "k8s-cluster", "k8s.cluster.name")\`` },
      { key: "C", text: `\`set(resource.attributes["k8s.cluster.name"], resource.attributes["k8s-cluster"])\` alone, without deleting the original` },
      { key: "D", text: `\`set(resource.attributes["k8s.cluster.name"], resource.attributes["k8s-cluster"])\` followed by \`delete_key(resource.attributes, "k8s-cluster")\`` },
    ],
    correctAnswer: "D",
    explanation: `OTTL does not have a rename function. To rename an attribute, you must first copy the value to the new key using \`set\`, then remove the old key using \`delete_key\`. The order matters: deleting before copying would result in a nil value being set. Option A uses \`replace_all_patterns\` which can also work for key renaming, but it operates with regex patterns, not exact key names, and is a less direct approach.`,
    difficulty: "scenario",
  },
  {
    id: "17-Q5",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team configures \`copy_metric(name="requests.total")\` in the transform processor without a \`where\` clause. What happens?`,
    options: [
      { key: "A", text: `The processor enters an infinite loop, continuously creating copies of the metric` },
      { key: "B", text: `A single copy is created and subsequent statements are skipped` },
      { key: "C", text: `The collector rejects the configuration at startup` },
      { key: "D", text: `The original metric is replaced by the copy` },
    ],
    correctAnswer: "A",
    explanation: `The \`copy_metric\` function creates a copy that is appended to the metric slice and then processed by subsequent statements, including the \`copy_metric\` statement itself. Without a restrictive \`where\` clause (e.g., \`where metric.name == "requests"\`) that would not match the new copy, the function recursively creates copies, leading to an infinite loop that consumes excessive CPU and memory.`,
    difficulty: "edge-case",
  },
  {
    id: "17-Q6",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `What is a key difference between the resource processor and the attributes processor?`,
    options: [
      { key: "A", text: `The resource processor supports all seven action types, but the attributes processor supports only four` },
      { key: "B", text: `The resource processor can modify span-level attributes, but the attributes processor cannot` },
      { key: "C", text: `The resource processor does not support include/exclude filtering` },
      { key: "D", text: `The resource processor requires OTTL statements, while the attributes processor uses action-based configuration` },
    ],
    correctAnswer: "C",
    explanation: `Both the resource processor and the attributes processor support the same seven action types (insert, update, upsert, delete, hash, extract, convert) and use action-based configuration. The key difference is that the resource processor operates exclusively on resource attributes and does not support include/exclude filtering, while the attributes processor supports full include/exclude filtering for selective processing.`,
    difficulty: "recall",
  },
  {
    id: "17-Q7",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A fintech company configures the filter processor to drop datapoints where \`attributes["environment"] == "staging"\` from a metric. If all datapoints in a metric are dropped, what happens to the metric itself?`,
    options: [
      { key: "A", text: `The metric remains with zero datapoints` },
      { key: "B", text: `The entire metric is also dropped` },
      { key: "C", text: `The metric keeps its metadata but loses its type information` },
      { key: "D", text: `The collector logs a warning and retains the metric` },
    ],
    correctAnswer: "B",
    explanation: `The filter processor follows hierarchical filtering rules. If all datapoints for a metric are dropped by datapoint-level conditions, the entire metric is also removed from the payload. Similarly, if all telemetry in a resource is dropped, the resource itself is removed.`,
    difficulty: "scenario",
  },
  {
    id: "17-Q8",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `In the tail sampling processor, what happens to a trace when no policy returns a "sample" or "drop" decision?`,
    options: [
      { key: "A", text: `The trace is sampled with a default 10% probability` },
      { key: "B", text: `The trace is held in memory until a matching policy is added` },
      { key: "C", text: `The trace is forwarded to the next processor without a sampling decision` },
      { key: "D", text: `The trace is not sampled` },
    ],
    correctAnswer: "D",
    explanation: `When no tail sampling policy matches a trace, the default behavior is to not sample it. The trace is discarded. This means that if you want a baseline sampling rate for unmatched traces, you must explicitly add an \`always_sample\` or \`probabilistic\` policy as a catch-all.`,
    difficulty: "recall",
  },
  {
    id: "17-Q9",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An SRE team wants to always capture error traces while limiting normal traffic to 1,000 spans per second with allocated budgets: 50% for errors, 30% for slow requests, and the rest for baseline sampling. Which tail sampling policy type supports this?`,
    options: [
      { key: "A", text: `Composite policy with \`max_total_spans_per_second\` and \`rate_allocation\`` },
      { key: "B", text: `An AND policy combining status_code and rate_limiting sub-policies` },
      { key: "C", text: `Multiple independent policies with \`sample_on_first_match: true\`` },
      { key: "D", text: `A probabilistic policy with a high sampling percentage for errors` },
    ],
    correctAnswer: "A",
    explanation: `The composite policy is designed for tiered sampling strategies. It accepts \`max_total_spans_per_second\` to set an overall budget, \`composite_sub_policy\` to define multiple policies (like errors, slow, and baseline), and \`rate_allocation\` to assign percentage budgets to each policy. Unallocated percentage goes to the remaining policies.`,
    difficulty: "scenario",
  },
  {
    id: "17-Q10",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An engineer configures the attributes processor with an extract action using the pattern \`(.*):\\/\\/(.*)\` on an HTTP URL attribute. What is the result?`,
    options: [
      { key: "A", text: `Two new attributes are created with auto-generated names like \`group_1\` and \`group_2\`` },
      { key: "B", text: `The URL is split into protocol and host attributes` },
      { key: "C", text: `No attributes are created because the capture groups are not named` },
      { key: "D", text: `The processor crashes with a regex compilation error` },
    ],
    correctAnswer: "C",
    explanation: `The attributes processor's extract action requires named capture groups using the \`(?P<name>...)\` syntax. Anonymous groups like \`(.*)\` do not produce named attributes. The correct pattern would be \`(?P<http_protocol>.*):\\/\\/(?P<http_domain>.*)\`. Without named groups, the extract action takes no action.`,
    difficulty: "edge-case",
  },
  {
    id: "17-Q11",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `In OTTL, how are editors and converters distinguished by naming convention?`,
    options: [
      { key: "A", text: `Editors use SCREAMING_CASE; converters use snake_case` },
      { key: "B", text: `Editors are lowercase (e.g., \`set\`, \`delete_key\`); converters start with an uppercase letter (e.g., \`Concat\`, \`IsMatch\`)` },
      { key: "C", text: `Both use the same naming convention and are distinguished only by their return type` },
      { key: "D", text: `Editors are prefixed with \`edit_\`; converters are prefixed with \`convert_\`` },
    ],
    correctAnswer: "B",
    explanation: `OTTL uses a naming convention to distinguish function types. Editors, which transform telemetry data in place, use lowercase names like \`set\`, \`delete_key\`, and \`append\`. Converters, which return values for use in expressions, follow PascalCase naming like \`Concat\`, \`Split\`, \`IsMatch\`, and \`ParseJSON\`.`,
    difficulty: "recall",
  },
  {
    id: "17-Q12",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `In the filter processor, OTTL conditions defined within the same signal section (e.g., multiple conditions under \`traces.span\`) are combined using which logical operator?`,
    options: [
      { key: "A", text: `AND (all conditions must be true to drop)` },
      { key: "B", text: `XOR (exactly one condition must be true to drop)` },
      { key: "C", text: `They are evaluated independently, each producing a separate filtering decision` },
      { key: "D", text: `OR (any condition being true causes the telemetry to be dropped)` },
    ],
    correctAnswer: "D",
    explanation: `In the filter processor, when multiple OTTL conditions are listed for the same signal type, they are OR'd together. If any single condition evaluates to true, the telemetry item is dropped. This means each condition independently can trigger a drop, and you do not need all conditions to match.`,
    difficulty: "recall",
  },
  {
    id: "17-Q13",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team writes the following transform processor configuration for metrics, but the collector fails to start:

\`\`\`yaml
metric_statements:
  - convert_sum_to_gauge() where metric.name == "process.count"
  - limit(datapoint.attributes, 50, ["host.name"])
\`\`\`

What is the root cause and how should they fix it?`,
    options: [
      { key: "A", text: `Split the statements into separate groups because \`convert_sum_to_gauge\` requires the metric context while \`limit(datapoint.attributes, ...)\` requires the datapoint context` },
      { key: "B", text: `Add \`error_mode: ignore\` because \`convert_sum_to_gauge\` might fail on non-Sum metrics` },
      { key: "C", text: `Remove the \`where\` clause because it conflicts with the \`limit\` function` },
      { key: "D", text: `Replace \`limit\` with \`keep_keys\` because \`limit\` is not available in the datapoint context` },
    ],
    correctAnswer: "A",
    explanation: `The transform processor uses context inference to determine which OTTL context to use for a statement group. \`convert_sum_to_gauge()\` is only available in the metric context, while \`limit(datapoint.attributes, ...)\` requires the datapoint context to access datapoint paths. These contexts are incompatible in a single statement group. The solution is to split them into separate groups using the advanced configuration format with multiple \`statements\` blocks.`,
    difficulty: "edge-case",
  },
  {
    id: "17-Q14",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `In the tail sampling processor, what is the precedence of the DROP policy relative to other sampling decisions?`,
    options: [
      { key: "A", text: `DROP has the lowest precedence and is only applied if no other policy matches` },
      { key: "B", text: `DROP takes precedence over all other sampling decisions, including "sample"` },
      { key: "C", text: `DROP and sample decisions have equal precedence, resolved by policy order` },
      { key: "D", text: `DROP only applies to traces that have already been marked for sampling` },
    ],
    correctAnswer: "B",
    explanation: `The tail sampling processor follows a strict decision hierarchy: DROP decisions always take precedence. If any policy returns a "drop" decision, the trace is not sampled regardless of whether other policies returned a "sample" decision. After DROP, any "sample" decision results in sampling. If no policy matches, the default is to not sample.`,
    difficulty: "recall",
  },
  {
    id: "17-Q15",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An e-commerce platform needs to remove all span attributes that contain personally identifiable information (PII). The PII attributes follow a pattern like \`user.email\`, \`user.phone\`, and \`user.ssn\`. Using the transform processor, which OTTL approach handles this most efficiently?`,
    options: [
      { key: "A", text: `Three separate \`delete_key\` statements, one for each attribute` },
      { key: "B", text: `\`keep_keys(span.attributes, ["http.method", "http.route", ...])\` listing every non-PII attribute to retain` },
      { key: "C", text: `\`delete_matching_keys(span.attributes, "^user\\\\.")\` to remove all attributes starting with \`user.\`` },
      { key: "D", text: `\`replace_all_patterns(span.attributes, "value", ".*", "[REDACTED]")\` to mask all attribute values` },
    ],
    correctAnswer: "C",
    explanation: `The \`delete_matching_keys\` editor accepts a regex pattern and removes all attributes whose keys match. Using \`"^user\\\\."\` efficiently removes all user-related PII attributes in a single statement. While individual \`delete_key\` calls would work, they do not scale as new PII fields are added. Using \`keep_keys\` requires maintaining an explicit allowlist, which is brittle.`,
    difficulty: "scenario",
  },
  {
    id: "17-Q16",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `An SRE team deploys a transform processor to redact sensitive fields from logs. After deployment, they notice that entire log batches are being dropped whenever a log record arrives with an unexpected field type, causing an OTTL statement to fail. What configuration change prevents this data loss?`,
    options: [
      { key: "A", text: `Add \`where\` clauses to every statement to check field types before processing` },
      { key: "B", text: `Move the transform processor after the batch processor in the pipeline` },
      { key: "C", text: `Enable \`flatten_data\` to isolate individual log records during transformation` },
      { key: "D", text: `Set \`error_mode: ignore\` so statement failures are logged but processing continues` },
    ],
    correctAnswer: "D",
    explanation: `The default \`error_mode\` for the transform processor is \`propagate\`, which causes the entire payload to be dropped when any statement encounters an error. Setting \`error_mode: ignore\` makes the processor log the error and continue to the next statement, preventing data loss from unexpected field types or missing values. The \`silent\` mode would also work but suppresses error logging, making it harder to identify issues.`,
    difficulty: "scenario",
  },
  {
    id: "17-Q17",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A team uses the attributes processor to delete the \`http.route\` attribute from metric datapoints, expecting the processor to re-aggregate the resulting datapoints. Instead, they observe duplicate time series in their backend. What is the cause?`,
    options: [
      { key: "A", text: `The attributes processor does not perform re-aggregation when modifying existing datapoint attributes, causing identity conflicts` },
      { key: "B", text: `The delete action requires a pattern instead of a key for metric attributes` },
      { key: "C", text: `The filter processor should be used instead of the attributes processor for metric modifications` },
      { key: "D", text: `The include filter was missing, so all metrics were processed instead of the target metric` },
    ],
    correctAnswer: "A",
    explanation: `In OpenTelemetry, a metric's identity is defined by its name, type, resource attributes, and datapoint attributes. When the attributes processor modifies or deletes existing datapoint attributes, it does not re-aggregate the resulting datapoints. This can create multiple time series that map to the same identity, causing identity conflicts in downstream backends. Adding new attributes is safe, but modifying existing ones requires re-aggregation through a connector or the metrics transform processor.`,
    difficulty: "edge-case",
  },
  {
    id: "17-Q18",
    domain: "collector",
    domainLabel: "OpenTelemetry Collector",
    question: `A monitoring team wants to reduce metric cardinality by grouping HTTP response status codes into ranges. They want to combine codes 200, 201, and 204 into a single "2xx" value and codes 500, 502, and 503 into "5xx". Which transform processor function achieves this?`,
    options: [
      { key: "A", text: `\`replace_all_patterns(datapoint.attributes, "value", "^2\\\\d{2}\$", "2xx")\`` },
      { key: "B", text: `\`aggregate_on_attribute_value("sum", "http.status_code", ["200", "201", "204"], "2xx")\`` },
      { key: "C", text: `\`set(datapoint.attributes["http.status_code"], "2xx") where datapoint.attributes["http.status_code"] >= 200 and datapoint.attributes["http.status_code"] < 300\`` },
      { key: "D", text: `\`keep_keys(datapoint.attributes, ["http.status_code"])\` followed by manual attribute renaming` },
    ],
    correctAnswer: "B",
    explanation: `The \`aggregate_on_attribute_value\` function is specifically designed for this use case. It aggregates datapoints where an attribute has one of the specified values into a single datapoint with a new attribute value, using a specified aggregation function like \`"sum"\`. Each call handles one group of values (e.g., 2xx, 5xx), and the function performs proper re-aggregation to avoid identity conflicts.`,
    difficulty: "scenario",
  },
  {
    id: "18-Q1",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `What is the primary purpose of the debug exporter in the OpenTelemetry Collector?`,
    options: [
      { key: "A", text: `To export telemetry data to a debug backend for production monitoring` },
      { key: "B", text: `To output telemetry data to the Collector's logs for inspection during development and testing` },
      { key: "C", text: `To generate synthetic telemetry data for pipeline validation` },
      { key: "D", text: `To capture telemetry data in a binary format for offline analysis` },
    ],
    correctAnswer: "B",
    explanation: `The debug exporter writes telemetry data to the Collector's standard output (logs), allowing operators to inspect what the pipeline is processing. Setting \`verbosity: detailed\` shows full attributes on spans, metrics, and logs. It is used for development and testing, not production traffic.`,
    difficulty: "recall",
  },
  {
    id: "18-Q2",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `An operator deploys a new Collector configuration and notices that no trace data is reaching the observability backend. The Collector logs show no errors. What is the most effective first debugging step?`,
    options: [
      { key: "A", text: `Increase the Collector's memory limits to handle more data` },
      { key: "B", text: `Restart the Collector pod and wait for data to appear` },
      { key: "C", text: `Add a debug exporter to the traces pipeline to verify data is flowing through the Collector` },
      { key: "D", text: `Increase the log level to debug and search for TLS handshake errors` },
    ],
    correctAnswer: "C",
    explanation: `Adding a debug exporter (with \`verbosity: detailed\`) to the pipeline confirms whether the Collector is receiving and processing data at all. This isolates the problem: if the debug exporter shows data, the issue is with the backend exporter; if it shows nothing, the issue is upstream (receivers or source applications). Changing log level or memory settings addresses different problems.`,
    difficulty: "scenario",
  },
  {
    id: "18-Q3",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `What is the default endpoint of the health_check extension, and what is its primary use?`,
    options: [
      { key: "A", text: `0.0.0.0:13133, used for orchestrator liveness and readiness probes` },
      { key: "B", text: `0.0.0.0:8888, used for exposing Prometheus metrics` },
      { key: "C", text: `localhost:1777, used for Go runtime profiling` },
      { key: "D", text: `0.0.0.0:4317, used for gRPC health checks between collectors` },
    ],
    correctAnswer: "A",
    explanation: `The health_check extension exposes an HTTP endpoint on port 13133 by default. It is primarily used for Kubernetes liveness and readiness probes, allowing the orchestrator to determine whether the Collector is healthy and ready to receive traffic. The metrics endpoint (8888) and pprof endpoint (1777) serve different purposes.`,
    difficulty: "recall",
  },
  {
    id: "18-Q4",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `Which command validates that a Collector configuration file is syntactically correct and that all referenced components are available in the binary?`,
    options: [
      { key: "A", text: `\`otelcol check --config config.yaml\`` },
      { key: "B", text: `\`otelcol --config config.yaml --dry-run\`` },
      { key: "C", text: `\`otelcol lint --config config.yaml\`` },
      { key: "D", text: `\`otelcol validate --config config.yaml\`` },
    ],
    correctAnswer: "D",
    explanation: `The \`validate\` command checks that the YAML is syntactically valid, that the component names match components available in the Collector binary, and that the configuration structure matches each component's specification. This catches configuration errors before deployment without starting the Collector.`,
    difficulty: "recall",
  },
  {
    id: "18-Q5",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A team configures their Collector to scrape its own Prometheus metrics endpoint using a Prometheus receiver pointed at localhost:8888. Why is this considered an anti-pattern?`,
    options: [
      { key: "A", text: `The Prometheus receiver cannot scrape localhost endpoints` },
      { key: "B", text: `If the Collector crashes or becomes overloaded, its diagnostic metrics are lost because the monitoring path depends on the component being monitored` },
      { key: "C", text: `Scraping its own metrics doubles the Collector's memory consumption` },
      { key: "D", text: `The Collector does not expose Prometheus-format metrics` },
    ],
    correctAnswer: "B",
    explanation: `When a Collector scrapes its own metrics, it creates a circular dependency. If the Collector is under stress (for example, queues are full), it cannot reliably export its own diagnostic data through the same stressed pipeline. The recommended architecture uses a separate, lightweight Collector layer to receive telemetry from the primary Collector, ensuring diagnostic data reaches the backend through an independent path.`,
    difficulty: "scenario",
  },
  {
    id: "18-Q6",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A Collector is configured with \`limit_mib: 4000\` in the memory_limiter processor, but the container's RSS consistently shows around 4300 MiB. Is this a bug?`,
    options: [
      { key: "A", text: `No, this is expected because the memory_limiter tracks Go heap allocation, not total process memory, and the Go runtime adds overhead` },
      { key: "B", text: `Yes, the memory_limiter should prevent total process memory from exceeding 4000 MiB` },
      { key: "C", text: `No, the spike_limit_mib adds extra memory on top of limit_mib` },
      { key: "D", text: `Yes, the check_interval is too long and memory spikes between checks` },
    ],
    correctAnswer: "A",
    explanation: `The memory_limiter processor monitors Go heap allocation (\`runtime.MemStats.Alloc\`), not process RSS. The Go runtime, stack allocations, CGO memory, and OS page caches contribute additional overhead of roughly 50-100 MiB beyond heap allocation. Container memory limits should be set 10-15% higher than \`limit_mib\` to account for this difference.`,
    difficulty: "edge-case",
  },
  {
    id: "18-Q7",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `After enabling TLS on a Collector pipeline, the exporter shows repeated connection failures. The certificates were generated correctly. Which misconfiguration is most likely?`,
    options: [
      { key: "A", text: `The batch processor is interfering with the TLS handshake` },
      { key: "B", text: `The Collector binary does not support TLS without a rebuild` },
      { key: "C", text: `The exporter's \`ca_file\` does not point to the Certificate Authority that signed the receiver's server certificate` },
      { key: "D", text: `TLS requires the health_check extension to be enabled` },
    ],
    correctAnswer: "C",
    explanation: `The most common TLS misconfiguration is a trust chain mismatch. The exporter (client side) must have a \`ca_file\` pointing to the Certificate Authority that signed the receiver's (server side) certificate. If the CA does not match, the client cannot verify the server's identity and the TLS handshake fails. This is independent of batch processing or health checks.`,
    difficulty: "scenario",
  },
  {
    id: "18-Q8",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `What does the pprof extension provide in the OpenTelemetry Collector?`,
    options: [
      { key: "A", text: `A dashboard showing real-time pipeline throughput statistics` },
      { key: "B", text: `A log aggregation interface for filtering Collector errors` },
      { key: "C", text: `A structured health report for Kubernetes readiness probes` },
      { key: "D", text: `An HTTP endpoint for Go runtime profiling, enabling CPU and memory analysis` },
    ],
    correctAnswer: "D",
    explanation: `The pprof extension exposes Go's built-in profiling endpoints (by default on localhost:1777). This allows operators to collect CPU profiles, memory allocation profiles, goroutine dumps, and other runtime diagnostics. Combined with a continuous profiling backend like Pyroscope, it enables ongoing performance analysis of the Collector process.`,
    difficulty: "recall",
  },
  {
    id: "18-Q9",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `An SRE team suspects that their Collector is dropping telemetry data under high load. Which internal metric should they monitor to confirm data loss at the processor level?`,
    options: [
      { key: "A", text: `\`otelcol_receiver_accepted_spans\`` },
      { key: "B", text: `\`otelcol_exporter_sent_spans\`` },
      { key: "C", text: `\`otelcol_processor_refused_spans\`` },
      { key: "D", text: `\`otelcol_exporter_queue_capacity\`` },
    ],
    correctAnswer: "C",
    explanation: `When the memory_limiter processor refuses data due to memory pressure, it increments \`otelcol_processor_refused_spans\` (and the equivalent counters for metrics and logs). A high refusal rate indicates the Collector is undersized for its workload or experiencing traffic spikes. Monitoring the refused-to-accepted ratio helps operators decide when to scale up or optimize the pipeline.`,
    difficulty: "scenario",
  },
  {
    id: "18-Q10",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A Collector configuration uses \`\${BACKEND_ENDPOINT}\` for the exporter endpoint, but the environment variable is not set. What happens when the Collector starts?`,
    options: [
      { key: "A", text: `The variable resolves to an empty string and the Collector logs a WARNING about the unset variable` },
      { key: "B", text: `The Collector refuses to start and exits with a validation error` },
      { key: "C", text: `The variable is ignored and the exporter uses its default endpoint` },
      { key: "D", text: `The Collector substitutes the literal string "\${BACKEND_ENDPOINT}"` },
    ],
    correctAnswer: "A",
    explanation: `The Collector's environment variable provider returns an empty string for unset variables and logs a WARNING: "Configuration references unset environment variable." It does not fail startup. This can lead to subtle configuration errors where an exporter silently targets an empty endpoint. To enforce required variables, validate them externally before starting the Collector.`,
    difficulty: "edge-case",
  },
  {
    id: "18-Q11",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A platform team operating 200 Collector instances notices one instance consuming significantly more memory than others. They want to identify which functions are allocating the most memory. Which approach should they use?`,
    options: [
      { key: "A", text: `Enable the debug exporter with detailed verbosity` },
      { key: "B", text: `Increase the memory_limiter's check_interval to capture more data` },
      { key: "C", text: `Set the Collector log level to debug and grep for "alloc"` },
      { key: "D", text: `Use the pprof extension to capture a heap profile and analyze it with a profiling tool` },
    ],
    correctAnswer: "D",
    explanation: `The pprof extension exposes Go profiling endpoints that allow operators to capture heap allocation profiles. These profiles show exactly which functions are allocating memory and how much. Combined with a continuous profiling backend or the \`go tool pprof\` CLI, this provides function-level visibility into memory consumption. The debug exporter shows telemetry data, not runtime diagnostics; log-level changes do not provide allocation breakdowns.`,
    difficulty: "scenario",
  },
  {
    id: "18-Q12",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `What are the available log levels for the Collector's internal telemetry, ordered from most to least verbose?`,
    options: [
      { key: "A", text: `trace, debug, info, warn, error, fatal` },
      { key: "B", text: `verbose, normal, quiet, silent` },
      { key: "C", text: `debug, info, warn, error` },
      { key: "D", text: `all, standard, minimal, none` },
    ],
    correctAnswer: "C",
    explanation: `The Collector supports four log levels: debug (most verbose, includes memory usage details from the memory_limiter), info (default, general operational messages), warn (potential issues), and error (failures only). Setting the level to debug is useful for diagnosing pipeline issues but produces significant log volume in production.`,
    difficulty: "recall",
  },
  {
    id: "18-Q13",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A Collector is configured with \`limit_mib: 4000\` and \`spike_limit_mib: 800\`. At what memory usage does the Collector begin refusing incoming data?`,
    options: [
      { key: "A", text: `At 4000 MiB (the hard limit)` },
      { key: "B", text: `At 3200 MiB (the soft limit, calculated as limit_mib minus spike_limit_mib)` },
      { key: "C", text: `At 800 MiB (the spike limit)` },
      { key: "D", text: `At 4800 MiB (limit_mib plus spike_limit_mib)` },
    ],
    correctAnswer: "B",
    explanation: `The memory_limiter uses two thresholds. The soft limit equals \`limit_mib - spike_limit_mib\` (4000 - 800 = 3200 MiB). When heap allocation exceeds this soft limit, the processor begins refusing data and may trigger garbage collection. The hard limit (4000 MiB) triggers more aggressive garbage collection. The spike_limit_mib represents the buffer between soft and hard limits to absorb memory spikes between checks.`,
    difficulty: "recall",
  },
  {
    id: "18-Q14",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `An engineer wants to test whether a new OTTL transformation correctly modifies span attributes before deploying it to production. What is the recommended approach?`,
    options: [
      { key: "A", text: `Create a minimal Collector config with the transform processor and a debug exporter, then use telemetrygen to send synthetic spans and inspect the output` },
      { key: "B", text: `Deploy the transformation to a production Collector with the log level set to debug` },
      { key: "C", text: `Write a unit test in Go that imports the transform processor package` },
      { key: "D", text: `Use the \`otelcol validate\` command, which tests OTTL logic in addition to syntax` },
    ],
    correctAnswer: "A",
    explanation: `The recommended functional testing approach uses a local integration test loop: configure a minimal pipeline with an OTLP receiver, the processor under test, and a debug exporter with detailed verbosity. Then use telemetrygen to send synthetic data with attributes matching the test conditions. The debug exporter output shows whether the transformation was applied correctly. The validate command only checks syntax and schema, not functional logic.`,
    difficulty: "scenario",
  },
  {
    id: "18-Q15",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A Collector configuration defines a \`transform\` processor but does not include it in any pipeline's processors list. What happens at startup?`,
    options: [
      { key: "A", text: `The Collector starts normally with no warnings` },
      { key: "B", text: `The Collector refuses to start and exits with a configuration validation error` },
      { key: "C", text: `The Collector starts but logs a warning that the component is defined but unused` },
      { key: "D", text: `The transform processor runs on all pipelines by default` },
    ],
    correctAnswer: "B",
    explanation: `The Collector validates that all components defined in the configuration are referenced by at least one pipeline. If a component is defined but not used in any pipeline, the Collector treats this as a configuration error and refuses to start. This strict validation helps operators catch configuration mistakes where they define a processor but forget to add it to the pipeline's processor list, preventing silent misconfigurations in production.`,
    difficulty: "edge-case",
  },
  {
    id: "18-Q16",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A Collector's internal metrics show that \`otelcol_exporter_send_failed_spans\` is steadily increasing while \`otelcol_exporter_sent_spans\` remains at zero. What does this indicate?`,
    options: [
      { key: "A", text: `The Collector's receivers are not accepting incoming data` },
      { key: "B", text: `The memory_limiter processor is refusing all data` },
      { key: "C", text: `The batch processor is dropping spans due to size limits` },
      { key: "D", text: `The exporter is unable to reach the backend, and all export attempts are failing` },
    ],
    correctAnswer: "D",
    explanation: `When \`send_failed_spans\` increases but \`sent_spans\` stays at zero, the exporter is consistently failing to deliver data to the backend. Common causes include an incorrect backend endpoint, network connectivity issues, TLS misconfiguration, or authentication failures. The receiver metrics would show whether data is being accepted into the pipeline, and the processor metrics would show refusals, but the exporter failure pattern points specifically to an export-side problem.`,
    difficulty: "scenario",
  },
  {
    id: "19-Q1",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `What retry strategy does the OpenTelemetry Collector's exporter helper use when a retryable error occurs?`,
    options: [
      { key: "A", text: `Exponential backoff with jitter` },
      { key: "B", text: `Fixed interval retry every 5 seconds` },
      { key: "C", text: `Linear backoff doubling the interval each attempt` },
      { key: "D", text: `Immediate retry up to three times, then drop the data` },
    ],
    correctAnswer: "A",
    explanation: `The exporter helper uses exponential backoff with a randomization factor (jitter) to avoid thundering herd problems. The default configuration starts at a 5-second initial interval, multiplies by 1.5 each attempt, caps at a 30-second maximum interval, and gives up after 5 minutes of elapsed time. The randomization factor (default 0.5) adds jitter to spread retries across time.`,
    difficulty: "recall",
  },
  {
    id: "19-Q2",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `An operations team notices that their Collector keeps retrying exports for malformed telemetry data, consuming queue capacity and delaying valid data. The backend consistently returns HTTP 400 for these requests. How should the exporter classify this error?`,
    options: [
      { key: "A", text: `Return a generic error so the retry mechanism handles it automatically` },
      { key: "B", text: `Increase the MaxElapsedTime to give the backend more time to accept the data` },
      { key: "C", text: `Wrap the error with consumererror.NewPermanent() so the retry mechanism skips it` },
      { key: "D", text: `Disable retries entirely for this exporter to prevent queue buildup` },
    ],
    correctAnswer: "C",
    explanation: `HTTP 400 (Bad Request) indicates a client-side error that will not succeed on retry. The exporter should wrap such errors with \`consumererror.NewPermanent()\` to tell the retry mechanism not to retry them. This preserves queue capacity for genuinely retryable failures like HTTP 503 (Service Unavailable) or network errors. Disabling retries entirely would also prevent recovery from transient failures.`,
    difficulty: "scenario",
  },
  {
    id: "19-Q3",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `What does the \`QueueSize\` parameter in the Collector's sending queue configuration represent?`,
    options: [
      { key: "A", text: `The maximum number of individual spans, metric points, or log records buffered` },
      { key: "B", text: `The total memory in megabytes allocated for the queue` },
      { key: "C", text: `The maximum number of parallel export goroutines` },
      { key: "D", text: `The maximum number of batches held in the queue` },
    ],
    correctAnswer: "D",
    explanation: `QueueSize is measured in batches, not individual telemetry items or bytes. The actual memory or disk consumption depends on both the queue size and the size of each batch. This is an important distinction for capacity planning, because a queue of 1000 batches with 500 items per batch holds significantly more data than 1000 individual items. The number of parallel export goroutines is controlled by the separate NumConsumers parameter.`,
    difficulty: "recall",
  },
  {
    id: "19-Q4",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A team runs a two-layer Collector architecture on Kubernetes. During a 30-minute backend outage, they discover that the default in-memory sending queue fills up after about 5 minutes and starts dropping data. What should they add to survive longer outages without data loss?`,
    options: [
      { key: "A", text: `Increase the number of NumConsumers to process the queue faster` },
      { key: "B", text: `Enable the file storage extension and configure the sending queue to use persistent storage` },
      { key: "C", text: `Add a second batch processor to compress data and reduce queue usage` },
      { key: "D", text: `Switch the sending queue to use limit_percentage instead of a fixed QueueSize` },
    ],
    correctAnswer: "B",
    explanation: `The file storage extension provides disk-backed persistent queues (Write-Ahead Log approach). When the in-memory queue is insufficient for the expected outage duration, configuring a file storage extension and referencing it in the exporter's sending queue via the \`storage\` parameter allows the Collector to buffer data to disk. If the Collector itself restarts, it reads the on-disk queue and resumes sending. Increasing NumConsumers helps with throughput but not with buffer capacity during a backend outage.`,
    difficulty: "scenario",
  },
  {
    id: "19-Q5",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A Collector is configured with \`limit_mib: 4000\`, but monitoring shows the process RSS is consistently around 4200 MiB. Is this a problem?`,
    options: [
      { key: "A", text: `No, this is expected; limit_mib tracks Go heap allocation, and process RSS includes additional overhead from the Go runtime, stack memory, and OS-level allocations` },
      { key: "B", text: `Yes, the memory limiter is not working correctly and should be reconfigured with a lower limit_mib` },
      { key: "C", text: `No, the memory limiter only activates when RSS exceeds 150% of limit_mib` },
      { key: "D", text: `Yes, the spike_limit_mib is too high, allowing memory to exceed the hard limit` },
    ],
    correctAnswer: "A",
    explanation: `The memory limiter tracks Go heap allocation (\`runtime.MemStats.Alloc\`), not process RSS or total memory usage. Process total memory is typically 50-100 MiB higher than heap allocation due to Go runtime overhead, stack memory, CGO allocations, and OS page caches. Container memory limits should be set 10-15% higher than limit_mib to account for this difference.`,
    difficulty: "edge-case",
  },
  {
    id: "19-Q6",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A Collector is configured with \`limit_mib: 4000\` and \`spike_limit_mib: 800\`. At what heap usage level does the memory limiter begin refusing data?`,
    options: [
      { key: "A", text: `4000 MiB, the hard limit` },
      { key: "B", text: `4800 MiB, the sum of both values` },
      { key: "C", text: `3200 MiB, the soft limit` },
      { key: "D", text: `2000 MiB, half of the hard limit` },
    ],
    correctAnswer: "C",
    explanation: `The memory limiter uses two thresholds. The soft limit is calculated as \`limit_mib - spike_limit_mib\` (4000 - 800 = 3200 MiB). When heap usage exceeds the soft limit, the processor begins refusing data and may trigger garbage collection. The hard limit (4000 MiB) triggers more aggressive garbage collection. The spike_limit_mib provides a buffer zone to account for memory increases that can occur between check intervals.`,
    difficulty: "recall",
  },
  {
    id: "19-Q7",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `An engineer configures the memory limiter with \`min_gc_interval_when_soft_limited: 5s\` and \`min_gc_interval_when_hard_limited: 10s\`. What happens when the Collector starts?`,
    options: [
      { key: "A", text: `The Collector starts normally and uses the configured intervals` },
      { key: "B", text: `The Collector starts but ignores min_gc_interval_when_hard_limited` },
      { key: "C", text: `The Collector logs a warning and uses default values instead` },
      { key: "D", text: `The Collector fails validation because min_gc_interval_when_soft_limited must be greater than or equal to min_gc_interval_when_hard_limited` },
    ],
    correctAnswer: "D",
    explanation: `The memory limiter validates that \`min_gc_interval_when_soft_limited\` is greater than or equal to \`min_gc_interval_when_hard_limited\`. This makes sense because the hard limit represents a more critical memory situation, so garbage collection should be allowed to run more frequently (shorter or zero interval) when memory is critically high. Setting a longer interval for the hard limit than for the soft limit would be counterproductive.`,
    difficulty: "edge-case",
  },
  {
    id: "19-Q8",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `During a traffic spike, a team observes that their OTLP receivers are intermittently returning errors to instrumented applications. The Collector's memory usage is near its configured limit. What is happening, and is data being permanently lost?`,
    options: [
      { key: "A", text: `The Collector is dropping data permanently; the team must increase the memory limit immediately` },
      { key: "B", text: `The memory limiter is returning non-permanent errors to receivers, which signal instrumented applications to retry with backoff; data is not permanently lost if clients implement retry logic` },
      { key: "C", text: `The receivers are shutting down their listening ports to prevent new connections until memory recovers` },
      { key: "D", text: `The batch processor is rejecting data because it cannot allocate buffers for new batches` },
    ],
    correctAnswer: "B",
    explanation: `When the memory limiter's thresholds are exceeded, it returns non-permanent errors (ErrDataRefused) to preceding components in the pipeline. Well-behaved receivers and OTLP clients treat these as retryable errors and apply backoff. This backpressure mechanism is intentional: it slows down data ingestion rather than crashing the Collector. Data is preserved as long as clients buffer and retry. The memory limiter automatically resumes normal operation when heap usage drops below the soft limit.`,
    difficulty: "scenario",
  },
  {
    id: "19-Q9",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `If spike_limit_mib is not explicitly set in the memory limiter configuration, what default value does the Collector use?`,
    options: [
      { key: "A", text: `20% of limit_mib` },
      { key: "B", text: `512 MiB regardless of limit_mib` },
      { key: "C", text: `50% of limit_mib` },
      { key: "D", text: `0 MiB, meaning only the hard limit applies` },
    ],
    correctAnswer: "A",
    explanation: `When spike_limit_mib is not explicitly configured, it defaults to 20% of limit_mib. For example, with \`limit_mib: 4000\`, the default spike_limit_mib would be 800 MiB, resulting in a soft limit of 3200 MiB. This default provides a reasonable buffer for most workloads. For spiky traffic, a higher percentage (such as 30%) may be appropriate.`,
    difficulty: "recall",
  },
  {
    id: "19-Q10",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `An SRE team is designing a telemetry pipeline that must survive backend outages of varying duration. They need to handle brief network blips, multi-hour provider outages, and even full-day incidents. Which approach correctly matches resilience strategies to outage duration?`,
    options: [
      { key: "A", text: `Kafka for brief outages, persistent storage for multi-hour outages, in-memory queues for full-day incidents` },
      { key: "B", text: `Persistent storage for all scenarios, since it covers both short and long outages` },
      { key: "C", text: `In-memory queues for brief outages, persistent storage (WAL) for multi-hour outages, and distributed messaging such as Kafka for full-day incidents` },
      { key: "D", text: `In-memory queues for brief outages, and Kafka for everything longer, since persistent storage is deprecated` },
    ],
    correctAnswer: "C",
    explanation: `The three resilience strategies form a progression. In-memory queues handle short disruptions (seconds to minutes) with minimal complexity. Persistent storage using the file storage extension (WAL) survives longer outages and Collector restarts, limited by available disk space. Distributed messaging systems like Kafka provide the highest resilience for extended outages, offering distributed storage, consumer scaling to drain backlogs, and operational familiarity for platform teams. Each level adds complexity and operational cost.`,
    difficulty: "scenario",
  },
  {
    id: "19-Q11",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `What file must exist at the root of every Weaver registry?`,
    options: [
      { key: "A", text: `schema.yaml` },
      { key: "B", text: `weaver.config.yaml` },
      { key: "C", text: `conventions.yaml` },
      { key: "D", text: `registry_manifest.yaml` },
    ],
    correctAnswer: "D",
    explanation: `Every Weaver registry requires a \`registry_manifest.yaml\` file at its root. This file contains metadata including the registry name, the semantic conventions version (semconv_version), and optionally a list of dependencies on other registries. Without this manifest, Weaver cannot identify or process the registry.`,
    difficulty: "recall",
  },
  {
    id: "19-Q12",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A large organization uses a three-tier convention structure: OpenTelemetry semantic conventions at the base, a corporate registry extending them, and individual application registries extending the corporate one. What is the maximum depth of dependency chains that Weaver supports?`,
    options: [
      { key: "A", text: `3 levels` },
      { key: "B", text: `10 levels` },
      { key: "C", text: `Unlimited depth` },
      { key: "D", text: `5 levels` },
    ],
    correctAnswer: "B",
    explanation: `Weaver supports registry dependency chains (A depends on B depends on C) with a maximum depth of 10 levels. This allows layered convention structures where application registries reference corporate registries, which in turn reference the official OpenTelemetry semantic conventions. While 10 levels is the technical limit, Weaver's documentation recommends using dependency chains sparingly to avoid complexity.`,
    difficulty: "edge-case",
  },
  {
    id: "19-Q13",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `An observability platform team has defined their telemetry conventions in a Weaver registry. Before promoting a new service to production, they want to verify that its telemetry output conforms to the defined schema. Which Weaver capability enables this validation?`,
    options: [
      { key: "A", text: `The live check feature, which accepts OTLP data and validates it against registry definitions in real time` },
      { key: "B", text: `The emit command, which generates expected telemetry for comparison` },
      { key: "C", text: `The infer command, which deduces the schema from existing telemetry` },
      { key: "D", text: `The policy enforcement engine, which only runs during CI/CD builds` },
    ],
    correctAnswer: "A",
    explanation: `Weaver's live check feature accepts OTLP telemetry data and validates it against the definitions in a registry, reporting violations, warnings, or informational findings. This can be used in staging environments or pre-production gates to verify that instrumentation conforms to defined conventions before going live. The emit command generates fake data from definitions (useful for dashboards), while the infer command reverse-engineers definitions from existing telemetry. Policy enforcement with Rego can run in CI/CD but is separate from live OTLP validation.`,
    difficulty: "scenario",
  },
  {
    id: "19-Q14",
    domain: "debugging",
    domainLabel: "Maintaining and Debugging",
    question: `A dashboard engineering team needs to build visualizations for a new microservice, but the development team has not finished instrumenting the application yet. Which Weaver feature allows the dashboard team to work in parallel without waiting for real telemetry?`,
    options: [
      { key: "A", text: `The infer command, which generates definitions from the dashboard requirements` },
      { key: "B", text: `The policy enforcement engine, which simulates telemetry based on Rego rules` },
      { key: "C", text: `The emit command, which generates fake telemetry data from schema definitions` },
      { key: "D", text: `The live check feature, which replays historical telemetry from other services` },
    ],
    correctAnswer: "C",
    explanation: `Weaver's emit command generates synthetic telemetry data from registry definitions. Dashboard teams can use this fake but schema-compliant data to build and test visualizations while developers instrument the application in parallel. This reduces time-to-delivery by decoupling dashboard creation from instrumentation work. The infer command works in the opposite direction, deriving definitions from existing telemetry rather than generating telemetry from definitions.`,
    difficulty: "scenario",
  },
];
